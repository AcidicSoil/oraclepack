{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Initialize Go Module and Core Error Handling",
        "description": "Set up the Go module structure and implement the foundational error handling package with typed errors and exit codes.",
        "details": "Initialize the `go.mod` file with `go mod init`. Create the directory structure `internal/errors`. Define custom error types representing different failure domains (e.g., `ErrInvalidPack`, `ErrExecutionFailed`). Implement an `ExitCode(err error) int` function to map these errors to specific integer exit codes for the CLI. Add unit tests to ensure correct mapping.",
        "testStrategy": "Unit tests verifying that specific error types map to the expected integer exit codes.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Initialize Go Module",
            "description": "Initialize the Go module for the project using the standard Go toolchain.",
            "dependencies": [],
            "details": "Run `go mod init <module-name>` (likely `oraclepack` or similar based on context) in the project root. This creates the `go.mod` file which tracks dependencies. Ensure the Go version is set to a recent stable version (e.g., 1.21+).",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-30T21:37:57.613Z"
          },
          {
            "id": 2,
            "title": "Create Error Handling Package Structure",
            "description": "Set up the directory structure for the custom error handling package.",
            "dependencies": [
              1
            ],
            "details": "Create the directory `internal/errors`. Inside this directory, create the initial Go file, likely `errors.go`, which will house the custom error definitions and logic. This establishes the package `errors` (or `apierrors` to avoid conflict with stdlib).",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-30T21:38:15.207Z"
          },
          {
            "id": 3,
            "title": "Define Domain-Specific Error Types",
            "description": "Implement custom error types and sentinel errors representing specific failure domains.",
            "dependencies": [
              2
            ],
            "details": "In `internal/errors`, define error variables or structs for expected failure modes. Examples include `ErrInvalidPack` (parsing errors), `ErrExecutionFailed` (runtime errors), `ErrConfigInvalid`. Ensure these implement the standard `error` interface.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-30T21:38:15.209Z"
          },
          {
            "id": 4,
            "title": "Implement Exit Code Mapping Logic",
            "description": "Create a function to map specific errors to integer exit codes for CLI termination.",
            "dependencies": [
              3
            ],
            "details": "Implement a function signature like `func ExitCode(err error) int`. Use `errors.Is` or type switching to determine the error type and return a specific integer code (e.g., 1 for generic, 2 for config, 3 for parsing). Default to 1 for unknown errors and 0 for nil.",
            "status": "done",
            "testStrategy": "Unit tests passing various error types into ExitCode and asserting the returned integer matches the specification.",
            "parentId": "undefined",
            "updatedAt": "2025-12-30T21:38:15.210Z"
          },
          {
            "id": 5,
            "title": "Add Unit Tests for Error Handling",
            "description": "Write comprehensive unit tests to verify the error types and exit code mapping.",
            "dependencies": [
              4
            ],
            "details": "Create `internal/errors/errors_test.go`. Test that wrapped errors are correctly identified by the `ExitCode` function using `fmt.Errorf(\"%w\", err)`. Verify that the correct exit codes are returned for `ErrInvalidPack`, `ErrExecutionFailed`, etc.",
            "status": "done",
            "testStrategy": "Run `go test ./internal/errors` and ensure 100% pass rate.",
            "parentId": "undefined",
            "updatedAt": "2025-12-30T21:38:31.645Z"
          }
        ],
        "updatedAt": "2025-12-30T21:38:31.645Z"
      },
      {
        "id": 2,
        "title": "Implement Pack Parsing and Validation",
        "description": "Develop the Markdown parser to extract bash blocks, parse steps/prelude, and validate structure.",
        "details": "Implement `internal/pack`. Use a library like `github.com/yuin/goldmark` or standard regex to locate the first bash fence. Parse the content to separate the 'prelude' (before first `# NN)`) from 'steps'. Create structs `Pack`, `Step`, and `Prelude`. Implement `Validate(p Pack)` to enforce rules (1+ steps, 2-digit numbering, no duplicates). Implement `DeriveMetadata` to extract `out_dir` and `--write-output` using regex. Handle edge cases like missing fences.",
        "testStrategy": "Table-driven unit tests with various Markdown inputs (valid, missing fence, malformed headers, duplicate steps) to verify parsing accuracy and error reporting.",
        "priority": "high",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Pack, Step, and Prelude Structures",
            "description": "Create the core data structures in `internal/pack` to represent the parsed Markdown content.",
            "dependencies": [],
            "details": "Create `internal/pack/types.go`. Define structs `Pack` (containing Prelude, Steps, Source path), `Step` (ID, Number, Code, OriginalLine), and `Prelude` (Code, Metadata). Include fields for derived metadata like `OutDir` and `WriteOutput`.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2025-12-30T21:39:47.853Z"
          },
          {
            "id": 2,
            "title": "Implement Markdown Parsing Logic",
            "description": "Develop the parser to read a Markdown file and extract the first bash code block.",
            "dependencies": [
              1
            ],
            "details": "Implement `Parse(content []byte) (*Pack, error)` in `internal/pack/parser.go`. Use a library like `goldmark` or regex to identify the first ````bash` fence. Extract the content within the fence for further processing. Handle errors for missing fences.",
            "status": "done",
            "testStrategy": "Unit tests with sample Markdown files containing valid and invalid bash blocks.",
            "parentId": "undefined",
            "updatedAt": "2025-12-30T21:39:47.855Z"
          },
          {
            "id": 3,
            "title": "Implement Step and Prelude Separation",
            "description": "Logic to split the extracted bash block into a prelude and individual numbered steps.",
            "dependencies": [
              2
            ],
            "details": "In `internal/pack/parser.go`, implement logic to iterate through lines of the extracted bash block. Identify steps using the `# NN)` pattern. Everything before the first step is `Prelude`. Populate the `Step` slice in the `Pack` struct.",
            "status": "done",
            "testStrategy": "Unit tests with various internal structures: prelude only, steps only, mixed, and malformed step headers.",
            "parentId": "undefined",
            "updatedAt": "2025-12-30T21:39:47.857Z"
          },
          {
            "id": 4,
            "title": "Implement Metadata Extraction and Derivation",
            "description": "Extract specific configuration values like output directory from the parsed content.",
            "dependencies": [
              3
            ],
            "details": "Implement `DeriveMetadata` method on the `Pack` or `Prelude` struct. Use regex to scan the prelude or specific comments for `out_dir` assignments and `--write-output` flags. Populate the corresponding fields in the `Pack` struct.",
            "status": "done",
            "testStrategy": "Unit tests verifying that metadata variables are correctly extracted from shell assignment syntax.",
            "parentId": "undefined",
            "updatedAt": "2025-12-30T21:39:47.860Z"
          },
          {
            "id": 5,
            "title": "Implement Pack Validation Rules",
            "description": "Enforce structural rules on the parsed Pack object to ensure integrity.",
            "dependencies": [
              3
            ],
            "details": "Implement `Validate() error` method for `Pack`. Check requirements: at least one step exists, step numbers are sequential and valid 2-digit format, and no duplicate step numbers. Return specific errors for violations.",
            "status": "done",
            "testStrategy": "Table-driven tests covering edge cases: missing steps, non-sequential numbers, duplicate IDs, and invalid numbering formats.",
            "parentId": "undefined",
            "updatedAt": "2025-12-30T21:39:47.862Z"
          }
        ],
        "updatedAt": "2025-12-30T21:39:47.862Z"
      },
      {
        "id": 3,
        "title": "Implement State Persistence and Reporting Models",
        "description": "Create the data models for run state and reporting, including atomic JSON persistence.",
        "details": "Implement `internal/state` and `internal/report`. Define `RunState` struct with schema version, pack hash, and step statuses. Implement `SaveStateAtomic(path, state)` using a temp-file-rename strategy to prevent corruption. Define `ReportV1` struct for the machine-readable summary. Include JSON tags. Use `encoding/json` for serialization.",
        "testStrategy": "Unit tests checking serialization/deserialization cycles and atomic write behavior (simulating write failures if possible).",
        "priority": "medium",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define RunState Structure",
            "description": "Create the data model for the execution state in `internal/state` package, including schema versioning and step tracking.",
            "dependencies": [],
            "details": "Define a `RunState` struct in `internal/state/types.go`. It should include fields for `SchemaVersion` (string or int), `PackHash` (string), `StartTime` (time.Time), and `StepStatuses` (map[string]StepStatus or a slice). Define `StepStatus` struct to hold individual step outcomes (Pending, Running, Success, Failed, Skipped), exit codes, and timestamps. Use proper JSON tags.",
            "status": "done",
            "testStrategy": "Unit tests verifying struct initialization and JSON marshaling/unmarshaling.",
            "parentId": "undefined",
            "updatedAt": "2025-12-30T21:42:14.950Z"
          },
          {
            "id": 2,
            "title": "Implement Atomic State Persistence",
            "description": "Develop functionality to save the run state to a file atomically to prevent data corruption.",
            "dependencies": [
              1
            ],
            "details": "Implement `SaveStateAtomic(path string, state *RunState) error` in `internal/state/persist.go`. Use `encoding/json` to marshal the state. Write to a temporary file first (e.g., using `os.CreateTemp` or appending `.tmp` to the path), ensure the write is flushed (`file.Sync`), and then use `os.Rename` to replace the target file atomically. Handle file permission setup.",
            "status": "done",
            "testStrategy": "Unit tests creating a state, saving it, and verifying the file exists. Simulate concurrent writes or interruptions if possible (though difficult in pure unit tests), or verify temp file cleanup.",
            "parentId": "undefined",
            "updatedAt": "2025-12-30T21:42:14.952Z"
          },
          {
            "id": 3,
            "title": "Implement State Loading Logic",
            "description": "Create functionality to load and validate existing run state from disk.",
            "dependencies": [
              1
            ],
            "details": "Implement `LoadState(path string) (*RunState, error)` in `internal/state/persist.go`. Read the file using `os.ReadFile` or `os.Open`. Decode using `encoding/json`. Validate the `SchemaVersion` to ensure compatibility. If the file doesn't exist, return a specific error or a default empty state depending on the design decision (usually specific error `ErrStateNotFound`).",
            "status": "done",
            "testStrategy": "Unit tests loading valid JSON files, corrupted JSON files, and non-existent files.",
            "parentId": "undefined",
            "updatedAt": "2025-12-30T21:42:14.954Z"
          },
          {
            "id": 4,
            "title": "Define ReportV1 Data Model",
            "description": "Create the machine-readable reporting data model in `internal/report`.",
            "dependencies": [],
            "details": "Define `ReportV1` struct in `internal/report/types.go`. This should be distinct from `RunState` but potentially overlapping. It is intended for final output. Fields should include `Summary` (total steps, success count, duration), `PackInfo` (name, hash), and `Steps` (a simplified list of executed steps and their results). Ensure strict JSON tagging for external consumption.",
            "status": "done",
            "testStrategy": "Unit test checking the JSON structure matches the expected schema output.",
            "parentId": "undefined",
            "updatedAt": "2025-12-30T21:42:14.956Z"
          },
          {
            "id": 5,
            "title": "Implement Report Generation Logic",
            "description": "Implement the logic to generate a ReportV1 object from a RunState.",
            "dependencies": [
              1,
              4
            ],
            "details": "Implement a function `GenerateReport(state *state.RunState, packInfo ...interface{}) *ReportV1` in `internal/report/generate.go`. This function maps the internal `RunState` data to the public `ReportV1` format, calculating derived metrics like total duration or success rates if they aren't explicitly stored in RunState.",
            "status": "done",
            "testStrategy": "Unit tests passing a populated RunState and asserting the fields in the generated ReportV1 are correct.",
            "parentId": "undefined",
            "updatedAt": "2025-12-30T21:42:14.958Z"
          }
        ],
        "updatedAt": "2025-12-30T21:42:14.958Z"
      },
      {
        "id": 4,
        "title": "Implement Shell Execution Engine",
        "description": "Build the execution runner to run bash scripts, stream output, and manage log files.",
        "details": "Implement `internal/exec`. Create `Runner` struct with fields for Shell, WorkDir, and Env. Implement `RunPrelude` and `RunStep`. Use `os/exec` to invoke `bash -lc`. Pipe `Cmd.Stdout` and `Cmd.Stderr`. Implement a streaming mechanism (e.g., `io.MultiWriter` to file and a callback function) to capture logs in real-time. Ensure process termination handling.",
        "testStrategy": "Integration tests executing simple shell scripts (e.g., echo, exit 1) and verifying that output is captured in the log file and the callback receives lines.",
        "priority": "high",
        "dependencies": [
          "2"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Runner Interface and Structs",
            "description": "Create the basic Runner structure and interface in `internal/exec` to define how the shell execution will be structured.",
            "dependencies": [],
            "details": "Create `internal/exec/runner.go`. Define `Runner` struct with fields: `Shell` (defaulting to /bin/bash), `WorkDir` (string), and `Env` (map[string]string or []string). Define `RunnerOptions` for configuration. Create a `NewRunner` factory function. Define the basic method signatures `RunPrelude` and `RunStep` without full implementation yet. Ensure compatibility with the `Pack` and `Step` structs from `internal/pack`.",
            "status": "done",
            "testStrategy": "Unit tests ensuring the Runner is initialized with correct default values and options.",
            "parentId": "undefined",
            "updatedAt": "2025-12-30T21:41:02.561Z"
          },
          {
            "id": 2,
            "title": "Implement Output Streaming Mechanism",
            "description": "Develop a helper to capture stdout/stderr from a command, streaming it simultaneously to a writer (file) and a callback function.",
            "dependencies": [
              1
            ],
            "details": "In `internal/exec/stream.go`, implement a custom `io.Writer` or use `io.MultiWriter` logic. Create a function `StreamOutput(reader io.Reader, writers ...io.Writer) error` or similar. Implement a line-scanning mechanism (using `bufio.Scanner`) that allows passing a callback function `OnLine(line string)` for real-time log processing (needed for the TUI later). Ensure it handles concurrency properly if reading stdout and stderr simultaneously.",
            "status": "done",
            "testStrategy": "Unit tests writing to a pipe and asserting that data appears in both the destination buffer and the callback function.",
            "parentId": "undefined",
            "updatedAt": "2025-12-30T21:41:02.564Z"
          },
          {
            "id": 3,
            "title": "Implement Basic Command Execution Logic",
            "description": "Implement the core logic to invoke a shell command using `os/exec` within the Runner context.",
            "dependencies": [
              2
            ],
            "details": "In `internal/exec/exec.go` (or `runner.go`), implement a private method `execCommand(script string, env []string) error`. This should construct `exec.Command` using `bash -lc` (or configured shell). It must attach the `WorkDir` and merge the process environment with the Runner's `Env`. Wire up the `Stdout` and `Stderr` pipes to the streaming mechanism defined in the previous subtask. Handle process start and wait.",
            "status": "done",
            "testStrategy": "Integration test: Run a simple `echo hello` command and verify successful exit code and captured output.",
            "parentId": "undefined",
            "updatedAt": "2025-12-30T21:41:02.565Z"
          },
          {
            "id": 4,
            "title": "Implement RunPrelude and RunStep Methods",
            "description": "Flesh out the public methods to execute specific parts of a Pack.",
            "dependencies": [
              3
            ],
            "details": "Implement `RunPrelude(p *pack.Prelude, logWriter io.Writer)` and `RunStep(s *pack.Step, logWriter io.Writer)`. These methods should construct the shell script string from the provided model (concatenating commands if necessary, or passing the raw script block). They should invoke the internal command execution logic, passing the appropriate log writers. Ensure `RunStep` handles errors by returning a specific error type that indicates failure.",
            "status": "done",
            "testStrategy": "Mock the `pack` structs and verify that `RunStep` executes the script content contained in the step.",
            "parentId": "undefined",
            "updatedAt": "2025-12-30T21:41:02.567Z"
          },
          {
            "id": 5,
            "title": "Implement Process Termination and Context Handling",
            "description": "Add support for context-based cancellation to handle interrupts and timeouts gracefully.",
            "dependencies": [
              4
            ],
            "details": "Update the Runner methods to accept `context.Context`. Modify the `os/exec` command creation to use `exec.CommandContext`. Ensure that if the context is cancelled, the underlying process is killed (using `cmd.Wait` and potentially process group killing if the shell spawns children). This is crucial for stopping a run gracefully via Ctrl+C.",
            "status": "done",
            "testStrategy": "Test starting a long-running command (e.g., `sleep 10`) and cancelling the context immediately, verifying the process terminates quickly.",
            "parentId": "undefined",
            "updatedAt": "2025-12-30T21:41:02.568Z"
          }
        ],
        "updatedAt": "2025-12-30T21:41:02.568Z"
      },
      {
        "id": 5,
        "title": "Implement Flag Injection Logic",
        "description": "Add logic to safely inject user-provided flags into `oracle` command invocations within steps.",
        "details": "Extend `internal/exec` or `internal/pack` with a transformation function. It should scan step body lines; if a line matches `^\\s*oracle\\s+`, append the extra flags. Ensure it handles whitespace correctly and does not modify non-oracle lines. This must be conservative to avoid breaking scripts.",
        "testStrategy": "Unit tests with various step bodies, asserting that only 'oracle' lines are modified and others remain untouched.",
        "priority": "medium",
        "dependencies": [
          "4"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Analyze and Define Regex Strategy for Oracle Command Detection",
            "description": "Create a robust regular expression strategy to correctly identify `oracle` command invocations within a shell script body.",
            "dependencies": [],
            "details": "Analyze various shell command patterns (e.g., `oracle ...`, `  oracle ...`, `path/to/oracle ...`). Define a regex pattern, likely `^\\s*oracle(\\s+|$)`, to safely target lines calling the oracle binary without matching false positives like comments or strings. Document edge cases.",
            "status": "done",
            "testStrategy": "Create a list of test strings (positive and negative matches) to verify regex behavior before implementation.",
            "parentId": "undefined",
            "updatedAt": "2025-12-30T21:43:01.568Z"
          },
          {
            "id": 2,
            "title": "Implement Script Transformation Function in `internal/exec`",
            "description": "Develop the core function `InjectFlags(script string, flags []string) string` to process script bodies.",
            "dependencies": [
              1
            ],
            "details": "In `internal/exec`, add a new file or utility function. The function should accept the raw script content and a list of flags. It must iterate line-by-line, detect lines matching the defined regex, and append the provided flags to the end of the command line, preserving existing whitespace and arguments.",
            "status": "done",
            "testStrategy": "Unit test the function with string inputs representing shell scripts, ensuring output strings contain injected flags on correct lines.",
            "parentId": "undefined",
            "updatedAt": "2025-12-30T21:43:01.570Z"
          },
          {
            "id": 3,
            "title": "Integrate Flag Injection into Runner Execution Flow",
            "description": "Modify the `Runner` struct and `RunStep` method to accept and apply extra flags.",
            "dependencies": [
              2
            ],
            "details": "Update the `Runner` struct in `internal/exec/runner.go` (or equivalent) to hold an `OracleFlags` field (slice of strings). Update the `RunStep` method to call the transformation function on the step's script body before passing it to `os/exec` or writing it to a temporary file.",
            "status": "done",
            "testStrategy": "Integration test: Instantiate a Runner with specific flags, run a mock step containing `oracle`, and verify the executed command includes the flags (e.g., via echoed output or log inspection).",
            "parentId": "undefined",
            "updatedAt": "2025-12-30T21:43:01.572Z"
          },
          {
            "id": 4,
            "title": "Add Configuration Support for Oracle Flags",
            "description": "Ensure the application configuration can pass these flags down to the execution engine.",
            "dependencies": [
              3
            ],
            "details": "Update the main application config structure (likely in `internal/app` or `internal/config`) to include a field for user-provided flags (e.g., from CLI args). Ensure this configuration is correctly propagated when initializing the `Runner` instance.",
            "status": "done",
            "testStrategy": "Verify that a configured list of flags in the App struct results in a populated `OracleFlags` field in the Runner.",
            "parentId": "undefined",
            "updatedAt": "2025-12-30T21:43:01.577Z"
          },
          {
            "id": 5,
            "title": "Verify Injection Safety and Side Effects",
            "description": "Conduct comprehensive testing to ensure non-oracle lines and complex scripts remain unbroken.",
            "dependencies": [
              2,
              3
            ],
            "details": "Write a suite of test cases covering edge cases: multi-line scripts, indentation, other commands starting with 'o', and scripts with no oracle calls. Ensure the transformation is idempotent or harmless if run multiple times (though it should only run once per execution).",
            "status": "done",
            "testStrategy": "Run a regression test suite with complex step bodies to ensure the script syntax remains valid and only target lines are modified.",
            "parentId": "undefined",
            "updatedAt": "2025-12-30T21:43:01.574Z"
          }
        ],
        "updatedAt": "2025-12-30T21:43:01.577Z"
      },
      {
        "id": 6,
        "title": "Wire Application Core (Plain Mode)",
        "description": "Connect parser, executor, and state modules to create a functional non-TUI application runner.",
        "details": "Implement `internal/app`. Create `RunPlain` method. Orchestrate the flow: Load Pack -> Load State (if resume) -> Iterate Steps -> (Prompt/Confirm if interactive) -> Execute -> Update State -> Write Report. Handle `stop-on-fail` logic. This forms the business logic layer independent of the UI.",
        "testStrategy": "Integration tests running the full flow with a mock executor or simple scripts, verifying state updates and report generation.",
        "priority": "high",
        "dependencies": [
          "3",
          "5"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Application Context and Config Structures",
            "description": "Create the core App struct in internal/app containing configuration for execution mode, paths, and dependencies.",
            "dependencies": [],
            "details": "Create `internal/app/app.go`. Define a `Config` struct holding flags like `Verbose`, `DryRun`, `StopOnFail`, and `Resume`. Define the `App` struct that aggregates `parser.Pack`, `state.Manager`, and `exec.Runner`. Include a constructor `New(config Config)` that initializes these components.",
            "status": "done",
            "testStrategy": "Unit tests ensuring configuration options are correctly applied to the App struct.",
            "parentId": "undefined",
            "updatedAt": "2025-12-30T21:44:25.768Z"
          },
          {
            "id": 2,
            "title": "Implement Workflow Orchestration Logic",
            "description": "Develop the RunPlain method to iterate through pack steps and manage the execution flow.",
            "dependencies": [
              1
            ],
            "details": "In `internal/app/run.go`, implement `RunPlain() error`. This method should load the pack using `internal/parser`, initialize or load the state using `internal/state`, and loop through `Pack.Steps`. Inside the loop, check if the step is already completed (if resuming) before proceeding.",
            "status": "done",
            "testStrategy": "Mock dependencies (Parser, State) to test flow logic: skipping completed steps on resume, and stopping on error.",
            "parentId": "undefined",
            "updatedAt": "2025-12-30T21:44:25.770Z"
          },
          {
            "id": 3,
            "title": "Integrate Step Execution and Logging",
            "description": "Connect the orchestration loop to the executor engine to actually run step commands.",
            "dependencies": [
              2
            ],
            "details": "Within the `RunPlain` loop, invoke `exec.Runner.RunStep`. Configure the runner to stream output to stdout (for plain mode) and write logs to the file defined in the state/config. Handle `exec` errors by wrapping them in domain specific errors defined in `internal/errors`.",
            "status": "done",
            "testStrategy": "Integration test with a mock script to verify that the executor is called and logs are directed to the correct writer.",
            "parentId": "undefined",
            "updatedAt": "2025-12-30T21:44:25.772Z"
          },
          {
            "id": 4,
            "title": "Implement State Transitions and Persistence",
            "description": "Update the state manager after each step execution to record success or failure.",
            "dependencies": [
              3
            ],
            "details": "After `exec.Runner.RunStep` returns, update the current step's status in `internal/state`. Call `state.Save()` to persist progress to disk immediately. Ensure that the 'running' status is set before execution and 'completed'/'failed' after execution to support crash recovery.",
            "status": "done",
            "testStrategy": "Verify that the state file on disk reflects the progress after each step in a multi-step workflow.",
            "parentId": "undefined",
            "updatedAt": "2025-12-30T21:44:25.774Z"
          },
          {
            "id": 5,
            "title": "Generate Final Report and Cleanup",
            "description": "Implement the logic to finalize the run and generate a summary report.",
            "dependencies": [
              4
            ],
            "details": "At the end of `RunPlain`, regardless of success or failure (use `defer` or final block), generate a run report. This should summarize total time, steps completed, and any errors. Use the report format defined in `internal/parser` or `internal/state` if available, or create a simple text summary.",
            "status": "done",
            "testStrategy": "Run a full flow and assert that the report file/output exists and contains accurate summary data.",
            "parentId": "undefined",
            "updatedAt": "2025-12-30T21:44:25.776Z"
          }
        ],
        "updatedAt": "2025-12-30T21:44:25.776Z"
      },
      {
        "id": 7,
        "title": "Implement CLI Entrypoint and Subcommands",
        "description": "Set up the CLI framework using Cobra or standard flag parsing to expose functionality.",
        "details": "Implement `internal/cli`. Use `github.com/spf13/cobra` for robust command handling. Create `root` command and subcommands `run`, `validate`, `list`. specific flags: `--yes`, `--resume`, `--no-tui`, `--oracle-bin`. Map these flags to the `app` configuration. Ensure `help` text is auto-generated.",
        "testStrategy": "CLI tests parsing arguments and flags, asserting that the correct app configuration object is constructed.",
        "priority": "medium",
        "dependencies": [
          "6"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Initialize Cobra CLI Structure and Root Command",
            "description": "Set up the internal/cli package, create the root command, and define the global flags structure.",
            "dependencies": [],
            "details": "Create `internal/cli/root.go`. Initialize the root `cobra.Command` representing the entry point. Define a `Config` struct (or use an existing one from `internal/app` if available) to hold flag values. Define global flags like `--no-tui` and `--oracle-bin` here if they apply globally, or prepare them for specific subcommands. Ensure `Execute()` function is exported for `main.go`.",
            "status": "done",
            "testStrategy": "Unit test verifying the root command executes and displays help text without errors.",
            "parentId": "undefined",
            "updatedAt": "2025-12-30T21:45:35.697Z"
          },
          {
            "id": 2,
            "title": "Implement 'run' Subcommand with Flags",
            "description": "Create the 'run' subcommand to execute the pack, binding specific flags like --yes and --resume.",
            "dependencies": [
              1
            ],
            "details": "Create `internal/cli/run.go`. Define the `runCmd` struct. Register flags `--yes` (bool), `--resume` (bool), `--oracle-bin` (string), and `--no-tui` (bool) specifically for this command if they are not global. In the `RunE` handler, validate arguments (path to markdown file). This handler will eventually call the application logic.",
            "status": "done",
            "testStrategy": "Unit test invoking the run command with various flag combinations and asserting the config struct is populated correctly.",
            "parentId": "undefined",
            "updatedAt": "2025-12-30T21:45:35.699Z"
          },
          {
            "id": 3,
            "title": "Implement 'validate' and 'list' Subcommands",
            "description": "Add the 'validate' and 'list' subcommands to check pack integrity and list steps without execution.",
            "dependencies": [
              1
            ],
            "details": "Create `internal/cli/validate.go` and `internal/cli/list.go`. `validate` should accept a file path and call the parser's validation logic. `list` should parse the file and print steps. Ensure both commands inherit necessary global settings but generally require fewer flags than `run`.",
            "status": "done",
            "testStrategy": "Unit tests ensuring commands parse arguments correctly and fail on missing file arguments.",
            "parentId": "undefined",
            "updatedAt": "2025-12-30T21:45:35.702Z"
          },
          {
            "id": 4,
            "title": "Connect CLI to Application Logic",
            "description": "Map the parsed CLI flags and arguments to the main application controller or configuration object.",
            "dependencies": [
              2,
              3
            ],
            "details": "In each command's `RunE` method, instantiate the necessary components (e.g., `PackParser`, `Runner`, `App`). Pass the flag values (e.g., `resume`, `oracleBinary`) into an options struct used to initialize the application. Ensure the `run` command conditionally initializes the TUI based on the `--no-tui` flag.",
            "status": "done",
            "testStrategy": "Integration test mocking the internal app controller and verifying it receives the correct parameters from the CLI execution.",
            "parentId": "undefined",
            "updatedAt": "2025-12-30T21:45:35.703Z"
          },
          {
            "id": 5,
            "title": "Create Main Entrypoint",
            "description": "Implement the main.go file to invoke the CLI entry point.",
            "dependencies": [
              1
            ],
            "details": "Create `cmd/oraclepack/main.go`. This file should be minimal, importing `internal/cli` and calling `cli.Execute()`. It should handle the top-level error exit code (e.g., `os.Exit(1)` if `Execute` returns an error).",
            "status": "done",
            "testStrategy": "Build test (go build ./cmd/oraclepack) to ensure the binary compiles and basic manual execution test.",
            "parentId": "undefined",
            "updatedAt": "2025-12-30T21:45:35.705Z"
          }
        ],
        "updatedAt": "2025-12-30T21:45:35.705Z"
      },
      {
        "id": 8,
        "title": "Develop Markdown Rendering for Terminal",
        "description": "Implement a renderer to display Markdown content (previews) as ANSI-styled text.",
        "details": "Implement `internal/render`. integrate `github.com/charmbracelet/glamour`. Create a function `RenderMarkdown(text string) string` that returns ANSI strings. Configure a style (dark/light mode aware if possible, or fixed high-contrast).",
        "testStrategy": "Golden file tests comparing raw Markdown input against expected ANSI output strings.",
        "priority": "low",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Add Glamour Dependency and Create Render Package",
            "description": "Initialize the internal/render package and add the github.com/charmbracelet/glamour dependency to the project.",
            "dependencies": [],
            "details": "Run `go get github.com/charmbracelet/glamour` to update go.mod. Create the directory `internal/render`. Create a `render.go` file within this package. This sets up the foundational environment for markdown rendering.",
            "status": "done",
            "testStrategy": "Verify go.mod contains the dependency and the package compiles.",
            "parentId": "undefined",
            "updatedAt": "2025-12-30T21:46:38.916Z"
          },
          {
            "id": 2,
            "title": "Define RenderMarkdown Interface and Setup Glamour Renderer",
            "description": "Implement the core RenderMarkdown function and initialize the glamour renderer with a specific style.",
            "dependencies": [
              1
            ],
            "details": "In `internal/render/render.go`, define `func RenderMarkdown(text string) (string, error)`. Inside, initialize a glamour renderer using `glamour.NewTermRenderer`. Configure it to use a standard style like 'dark' or 'notty' initially. This function should take raw markdown text and return the ANSI string.",
            "status": "done",
            "testStrategy": "Unit test passing a simple markdown string (**bold**) and asserting the output contains ANSI escape codes.",
            "parentId": "undefined",
            "updatedAt": "2025-12-30T21:46:38.918Z"
          },
          {
            "id": 3,
            "title": "Implement Custom Style Configuration",
            "description": "Configure the glamour renderer to attempt auto-detection of the terminal background or fallback to a high-contrast style.",
            "dependencies": [
              2
            ],
            "details": "Enhance the renderer initialization logic. Use `glamour.WithAutoStyle()` if appropriate, or allow passing a style preference (Light/Dark) via a configuration struct/parameter. Ensure the style ensures readability in standard terminal environments. Handle initialization errors gracefully.",
            "status": "done",
            "testStrategy": "Manual verification in a terminal or unit tests checking that different styles produce different ANSI codes.",
            "parentId": "undefined",
            "updatedAt": "2025-12-30T21:46:38.920Z"
          },
          {
            "id": 4,
            "title": "Integrate Rendering with Pack Structures",
            "description": "Create a helper to render the description or content of a Pack Step directly.",
            "dependencies": [
              2
            ],
            "details": "Import `internal/pack`. Add a function `RenderStepDescription(step pack.Step) (string, error)` (or similar) in `internal/render`. This function should extract the relevant text field from the Step struct (e.g., the markdown body or prelude) and pass it to `RenderMarkdown`. This bridges the domain model with the view logic.",
            "status": "done",
            "testStrategy": "Unit test creating a mock Step object and verifying its markdown content is correctly rendered to ANSI.",
            "parentId": "undefined",
            "updatedAt": "2025-12-30T21:46:38.922Z"
          },
          {
            "id": 5,
            "title": "Create Golden File Tests for Markdown Rendering",
            "description": "Establish a robust testing suite using golden files to ensure rendering output remains consistent.",
            "dependencies": [
              2,
              3
            ],
            "details": "Create `internal/render/render_test.go`. Setup a test data directory `testdata/`. Add `.md` files with various markdown features (lists, code blocks, headers) and corresponding `.golden` files containing the expected ANSI output. Write a test function that renders the input and compares it to the golden file.",
            "status": "done",
            "testStrategy": "Run `go test ./internal/render` and verify that changes to styles break tests (prompting golden file updates) and output matches expectations.",
            "parentId": "undefined",
            "updatedAt": "2025-12-30T21:46:38.923Z"
          }
        ],
        "updatedAt": "2025-12-30T21:46:38.923Z"
      },
      {
        "id": 9,
        "title": "Build TUI with Bubble Tea",
        "description": "Create the interactive terminal UI using Bubble Tea, featuring step lists and log streaming.",
        "details": "Implement `internal/tui`. Define the Bubble Tea `Model`. Use `github.com/charmbracelet/bubbles/list` for the step list and `github.com/charmbracelet/bubbles/viewport` for logs. Implement `Update` loop to handle messages (KeyMsg, WindowSizeMsg, custom StatusMsg). Bind the `app` logic to trigger execution commands that return `Cmd`s. Show step status (spinner for running, check for success). Integrate `internal/render` for previews.",
        "testStrategy": "Manual testing of UI flows. Unit tests for Update function state transitions based on messages.",
        "priority": "high",
        "dependencies": [
          "6",
          "8"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define TUI Model and Initialize Layout",
            "description": "Create the base Bubble Tea model structure and initialize the list and viewport components.",
            "dependencies": [],
            "details": "Create `internal/tui/model.go`. Define a `Model` struct containing state fields: `pack` (from internal/pack), `steps` (list.Model), `logs` (viewport.Model), `spinner` (spinner.Model), `currentStepIdx` int, and `running` bool. Implement `Init()`, returning a batch command to start the spinner. In `NewModel(pack)`, initialize the `list.Model` with step items derived from the pack and the `viewport.Model` for log output. Configure default styles for the list and viewport.",
            "status": "done",
            "testStrategy": "Unit test `NewModel` to verify initial state configuration.",
            "parentId": "undefined",
            "updatedAt": "2025-12-30T21:48:37.631Z"
          },
          {
            "id": 2,
            "title": "Implement Update Loop and Key Bindings",
            "description": "Implement the Update method to handle key presses and window resizing.",
            "dependencies": [
              1
            ],
            "details": "In `internal/tui/update.go`, implement `Update(msg tea.Msg) (tea.Model, tea.Cmd)`. Handle `tea.KeyMsg`: 'q'/Ctrl+C to quit, 'enter' to run selected step (if not running), up/down to navigate list. Handle `tea.WindowSizeMsg` to resize the list and viewport dynamically (e.g., list takes 1/3 width, viewport 2/3). Handle `spinner.TickMsg` to update the spinner if a step is running. Ensure navigation is disabled while a step is executing.",
            "status": "done",
            "testStrategy": "Unit tests simulating KeyMsg inputs to verify model state changes (e.g., selection index updates).",
            "parentId": "undefined",
            "updatedAt": "2025-12-30T21:48:37.633Z"
          },
          {
            "id": 3,
            "title": "Integrate Step Execution and Command Triggering",
            "description": "Bind the execution logic to the TUI model using Bubble Tea Commands.",
            "dependencies": [
              2
            ],
            "details": "Define custom messages: `StepStartedMsg`, `LogLineMsg`, `StepFinishedMsg`. Create a `runStepCmd(runner, step)` function that returns a `tea.Cmd`. This command should invoke `internal/exec` logic. Since `exec` streams logs, wrap the execution in a way that it sends `LogLineMsg` back to the Update loop via a channel or callback, finally returning `StepFinishedMsg`. Update the Model to handle `StepStartedMsg` (set running=true), `StepFinishedMsg` (set running=false, update status icon), and append content to viewport on `LogLineMsg`.",
            "status": "done",
            "testStrategy": "Integration test mocking the executor to verify messages flow correctly through the Update loop.",
            "parentId": "undefined",
            "updatedAt": "2025-12-30T21:48:37.635Z"
          },
          {
            "id": 4,
            "title": "Implement View Rendering with Styles",
            "description": "Construct the UI layout string in the View method using Lip Gloss for styling.",
            "dependencies": [
              1
            ],
            "details": "In `internal/tui/view.go`, implement `View() string`. Use `lipgloss` to create a split view: left pane for the step list, right pane for the log viewport. Apply borders and colors. Render the current step's status (pending, running, success, fail) using icons next to list items. Display a help footer (e.g., 'Enter: Run, q: Quit'). Ensure the view handles the current terminal dimensions correctly based on previous WindowSizeMsg.",
            "status": "done",
            "testStrategy": "Visual inspection via manual run; Snapshot testing of View output string if feasible.",
            "parentId": "undefined",
            "updatedAt": "2025-12-30T21:48:37.637Z"
          },
          {
            "id": 5,
            "title": "Connect TUI to Main Application Entrypoint",
            "description": "Wire the TUI into the main CLI command to launch the interface.",
            "dependencies": [
              3,
              4
            ],
            "details": "Update `cmd/oraclepack/main.go` (or wherever the root command runs). Add a flag or command to start interactive mode. Load the `Pack` using `internal/pack`. Initialize `internal/exec.Runner`. Create the TUI model via `tui.NewModel`. Start the program with `tea.NewProgram(model).Run()`. Ensure graceful shutdown and error reporting if TUI fails to initialize.",
            "status": "done",
            "testStrategy": "Manual verification: Run the binary with a sample pack and check if the TUI launches and interacts correctly.",
            "parentId": "undefined",
            "updatedAt": "2025-12-30T21:48:37.639Z"
          }
        ],
        "updatedAt": "2025-12-30T21:48:37.639Z"
      },
      {
        "id": 10,
        "title": "Setup Release Automation",
        "description": "Configure GoReleaser to automate builds and packaging for distribution.",
        "details": "Create `.goreleaser.yaml`. Configure builds for Linux (amd64/arm64), macOS (amd64/arm64), and Windows. Set up archive formats (tar.gz, zip). Configure Homebrew tap integration. Ensure binary is stripped and optimized.",
        "testStrategy": "Run `goreleaser release --snapshot --clean` locally to verify artifact generation.",
        "priority": "low",
        "dependencies": [
          "7",
          "9"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Initialize GoReleaser Configuration",
            "description": "Generate and configure the initial .goreleaser.yaml file for the project.",
            "dependencies": [],
            "details": "Run `goreleaser init` or manually create `.goreleaser.yaml` at the project root. define the `project_name` as 'oraclepack'. Configure the `before` hooks to run `go mod tidy`. Set up the basic `builds` section pointing to the main package entry point (e.g., `cmd/oraclepack` or `.`).",
            "status": "done",
            "testStrategy": "Run `goreleaser check` to validate the syntax of the configuration file.",
            "updatedAt": "2025-12-30T21:48:59.868Z",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Configure Cross-Platform Builds",
            "description": "Define build targets for Linux, macOS, and Windows with appropriate architectures.",
            "dependencies": [
              1
            ],
            "details": "Update the `builds` section in `.goreleaser.yaml`. Enable `env` with `CGO_ENABLED=0` for static binaries. specific `goos` as [linux, darwin, windows] and `goarch` as [amd64, arm64]. Ensure binary names follow the standard convention (e.g., `oraclepack`).",
            "status": "done",
            "testStrategy": "Run `goreleaser build --snapshot --clean` and inspect the `dist/` folder to verify binaries are created for all targets.",
            "parentId": "undefined",
            "updatedAt": "2025-12-30T21:48:59.870Z"
          },
          {
            "id": 3,
            "title": "Configure Archives and Checksums",
            "description": "Set up archive formats (tar.gz, zip) and name templates for distribution.",
            "dependencies": [
              2
            ],
            "details": "Configure the `archives` section. Use `tar.gz` for Linux/macOS and `zip` for Windows. Define `name_template` to include project name, version, OS, and architecture. Add files like `README.md` and `LICENSE` to the archive. Enable `checksum` generation.",
            "status": "done",
            "testStrategy": "Run `goreleaser release --snapshot --clean` and verify the structure and contents of the generated archives in `dist/`.",
            "parentId": "undefined",
            "updatedAt": "2025-12-30T21:48:59.872Z"
          },
          {
            "id": 4,
            "title": "Enable Binary Optimization and Stripping",
            "description": "Configure linker flags to strip debug information and reduce binary size.",
            "dependencies": [
              2
            ],
            "details": "In the `builds` section, add `ldflags` to strip the binary (e.g., `-s -w`). Also, inject version information (version, commit, date) into the main package variables if applicable (e.g., `-X main.version={{.Version}}`).",
            "status": "done",
            "testStrategy": "Build a snapshot and check binary size comparison with/without flags. Use `go tool objdump` or `file` to confirm symbols are stripped.",
            "parentId": "undefined",
            "updatedAt": "2025-12-30T21:48:59.874Z"
          },
          {
            "id": 5,
            "title": "Configure Homebrew Tap Integration",
            "description": "Add Homebrew formula generation to the release process.",
            "dependencies": [
              3
            ],
            "details": "Configure the `brews` section in `.goreleaser.yaml`. Define the tap repository (e.g., `github.com/user/homebrew-tap`). Set the formula description and homepage. ensure the install instructions copy the binary properly. Note: This requires a GitHub token which should be configured in the CI environment, not hardcoded.",
            "status": "done",
            "testStrategy": "Dry-run the release locally using `goreleaser release --snapshot --skip-publish` to verify the formula is generated correctly in `dist/`.",
            "parentId": "undefined",
            "updatedAt": "2025-12-30T21:48:59.876Z"
          }
        ],
        "updatedAt": "2025-12-30T21:48:59.876Z"
      },
      {
        "id": 11,
        "title": "Implement 'Run All' Sequential Execution in TUI",
        "description": "Add capability to execute all steps sequentially in TUI mode, triggered by CLI flag or keybinding.",
        "details": "",
        "testStrategy": "",
        "status": "done",
        "dependencies": [],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Analyze and Refactor TUI Model for Sequential Execution State",
            "description": "Examine the current Bubble Tea model in `internal/tui` and add state fields necessary to track a 'Run All' process.",
            "dependencies": [],
            "details": "Update the TUI model struct to include fields like `runningAll bool`, `pendingSteps []int` (indices or IDs), and `stopOnFailure bool`. This state will track whether the TUI is currently in the auto-advance mode. Review `internal/tui/model.go` (or equivalent) to ensure the `Update` loop can distinguish between a single manual run and a sequential run.",
            "status": "done",
            "testStrategy": "Unit tests for the model's initial state and state transitions when `runningAll` flags are toggled.",
            "parentId": "undefined",
            "updatedAt": "2025-12-31T03:49:55.837Z"
          },
          {
            "id": 2,
            "title": "Implement CLI Flag for Auto-Start and Pass to TUI",
            "description": "Add a `--run-all` or similar flag to the CLI and propagate this configuration to the TUI initialization.",
            "dependencies": [
              1
            ],
            "details": "Modify `internal/cli` to accept a new boolean flag (e.g., `--run-all` or `--auto`). Pass this boolean value into the function that constructs the TUI model. Ensure that if this flag is true, the TUI initializes in the `runningAll` state immediately upon startup.",
            "status": "done",
            "testStrategy": "Integration test invoking the binary with `--run-all`, verifying via mock or logs that the TUI enters the auto-run state immediately.",
            "parentId": "undefined",
            "updatedAt": "2025-12-31T03:49:55.839Z"
          },
          {
            "id": 3,
            "title": "Implement 'Run All' Keybinding in TUI",
            "description": "Register a new keybinding in the Bubble Tea application to trigger the sequential execution mode manually.",
            "dependencies": [
              1
            ],
            "details": "In `internal/tui/update.go` (or wherever key messages are handled), add a case for the chosen key (e.g., 'a' or 'r'). When pressed, set `model.runningAll = true`, identify the first pending step, and trigger the `Cmd` to run that step. Add visual feedback (e.g., a status line update) indicating 'Running All Steps'.",
            "status": "done",
            "testStrategy": "Manual verification or UI test harness (if available) pressing the key and observing the state change.",
            "parentId": "undefined",
            "updatedAt": "2025-12-31T03:49:55.841Z"
          },
          {
            "id": 4,
            "title": "Implement Sequential Step Chaining Logic",
            "description": "Modify the TUI Update loop to automatically trigger the next step upon the successful completion of the current step when in 'Run All' mode.",
            "dependencies": [
              1,
              3
            ],
            "details": "In the `Update` function, locate the message handling for a step completion (e.g., `StepFinishedMsg`). If `model.runningAll` is true and the step succeeded, locate the next step index. Return a command to run the next step immediately. If the step failed or was the last one, set `runningAll = false` and update the status message.",
            "status": "done",
            "testStrategy": "Unit tests for the `Update` function: simulate a `StepFinishedMsg` (success) while `runningAll` is true, asserting that the returned command is the execution of the next step.",
            "parentId": "undefined",
            "updatedAt": "2025-12-31T03:49:55.844Z"
          },
          {
            "id": 5,
            "title": "Implement User Interruption Handling",
            "description": "Allow the user to interrupt the sequential execution flow using a keybinding (e.g., Esc or Ctrl+C).",
            "dependencies": [
              4
            ],
            "details": "In the `Update` loop, ensure that key presses like `Esc` or `q` are checked during the 'Run All' sequence. If pressed, set `runningAll = false` to stop the chain after the current step finishes (or attempt to cancel the current running step if the execution engine supports cancellation context).",
            "status": "done",
            "testStrategy": "Unit test simulating an interrupt key press during `runningAll` state, verifying that the state resets to `runningAll = false` and no further steps are queued.",
            "parentId": "undefined",
            "updatedAt": "2025-12-31T03:49:55.846Z"
          }
        ],
        "updatedAt": "2025-12-31T03:49:55.846Z"
      },
      {
        "id": 12,
        "title": "Fix Step Header Parsing for Em-Dash Support",
        "description": "Update the Markdown parser to correctly handle step headers formatted with an em-dash (e.g., '# 01  Step Title') in addition to the existing parenthesis format.",
        "details": "The current `ExtractSteps` function in `internal/parser/parser.go` uses a regular expression that strictly expects a closing parenthesis after the step number (e.g., `^#\\s+(\\d+)\\)\\s+(.*)`). This causes validation failures for files using the em-dash format commonly found in some documentation styles (e.g., `^#\\s+(\\d+)\\s+[-]\\s+(.*)`).\n\nImplementation Steps:\n1.  Locate `stepRegex` in `internal/parser/parser.go`.\n2.  Modify the regular expression to allow for alternate separators. A proposed pattern is `^#\\s+(\\d+)(?:\\)|\\.|\\s+[-])\\s+(.*)`, covering `1)`, `1.`, and `1 ` formats.\n3.  Update the logic that parses the captured groups to ensure the step number and title are correctly extracted regardless of the separator used.\n4.  Ensure that the change does not break existing support for parenthesis-based headers.\n5.  (Optional) If strict formatting is desired, normalize the stored title, but the primary goal is parsing flexibility.",
        "testStrategy": "1. Add a new test case in `internal/parser/parser_test.go` specifically using a sample Markdown content with em-dash headers (e.g., `# 01  Install Dependencies`).\n2. Verify that the parser correctly extracts the ID `1` and the title `Install Dependencies`.\n3. Run existing tests to ensure no regression for `# 01) Title` formats.\n4. specific CLI test: Run `oraclepack validate` against a file with em-dash headers to confirm it passes.",
        "status": "done",
        "dependencies": [
          "7"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-12-31T20:08:52.250Z"
      },
      {
        "id": 13,
        "title": "Implement ROI Extraction and Execution Filtering",
        "description": "Parse ROI values from step headers and add CLI flags to filter step execution based on an ROI threshold.",
        "details": "Update the core parser and execution engine to support value-based step filtering.\n\n1. **Parser Update (`internal/parser`)**: Modify `ExtractSteps` and the `Step` struct. Update the header regex (building on Task 12's em-dash support) to capture an optional `ROI=(\\d+(\\.\\d+)?)` pattern. Store this as a `float64` in the `Step` struct. Ideally, strip the 'ROI=...' substring from the final `Step.Title` for cleaner UI display.\n\n2. **CLI Flags (`internal/cli`)**: Add new flags to the `run` command: `--roi-threshold` (float, default 0) and `--roi-mode` (string, options: 'over', 'under', default 'over'). Bind these to the application configuration.\n\n3. **Filtering Logic (`internal/app`)**: In the `RunPlain` loop (and TUI equivalent), inject a filtering check before execution. \n   - If `mode` is 'over', skip steps where `step.ROI < threshold`.\n   - If `mode` is 'under', skip steps where `step.ROI > threshold`.\n   - Treat steps without an explicit ROI tag as having an ROI of 0.0.",
        "testStrategy": "1. **Unit Tests**: Extend `internal/parser/parser_test.go` with table-driven tests containing headers like `# 01) Task ROI=5.5` and `# 02  Low Value ROI=0.1`. Assert that `ROI` fields are populated correctly.\n2. **Integration Tests**: Create a dummy pack with varying ROI values. Execute the CLI with `--roi-threshold=1.0 --roi-mode=over` and verify via logs or report output that only high-ROI steps were executed.",
        "status": "done",
        "dependencies": [
          "6",
          "7",
          "12"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Update Step Struct and Header Regex for ROI",
            "description": "Modify the Step struct to include an ROI float64 field and update the header parsing regex to capture 'ROI=value' patterns.",
            "dependencies": [],
            "details": "In `internal/parser`, add `ROI float64` to the `Step` struct. Update the regex in `ExtractSteps` (or helper function) to identify `ROI=(\\d+(\\.\\d+)?)` within the header line. Ensure the parsing logic converts the captured string to a float using `strconv.ParseFloat`. The regex must support existing formats (parenthesis, em-dash) while optionally capturing this new tag.",
            "status": "pending",
            "testStrategy": "Unit tests in `internal/parser` checking that `Step.ROI` is correctly populated for headers like `# 01) Title ROI=5.5` and defaults to 0.0 otherwise.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement ROI Tag Stripping from Step Titles",
            "description": "Clean up the step title by removing the raw ROI tag string after extraction to ensure clean UI output.",
            "dependencies": [
              1
            ],
            "details": "After extracting the ROI value in `internal/parser`, implement string manipulation to remove the substring `ROI=...` from the `Step.Title`. Use `strings.Replace` or regex replacement to trim this tag and any surrounding extra whitespace so the user sees `# 01) Task Name` instead of `# 01) Task Name ROI=5.5`.",
            "status": "pending",
            "testStrategy": "Extend parser unit tests to assert `Step.Title` does not contain the 'ROI=' substring after parsing.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Add ROI Flags to CLI Configuration",
            "description": "Define `--roi-threshold` and `--roi-mode` flags in the CLI and bind them to the application configuration.",
            "dependencies": [
              1
            ],
            "details": "In `internal/cli`, specifically within the `run` command setup, add two new flags: `--roi-threshold` (float, default 0.0) and `--roi-mode` (string, default 'over', validation options: 'over', 'under'). Ensure these values are propagated to the `app.Config` or `RunConfig` struct passed to the execution engine.",
            "status": "pending",
            "testStrategy": "CLI tests invoking the run command with these flags and asserting the config object reflects the provided values.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement Execution Filtering Logic in Run Loop",
            "description": "Inject logic into the main execution loop to skip steps based on the configured ROI threshold and mode.",
            "dependencies": [
              1,
              3
            ],
            "details": "In `internal/app` (within `RunPlain` and potentially TUI logic), add a conditional check before executing a step. If `roi-mode` is 'over', continue only if `step.ROI >= threshold`. If 'under', continue only if `step.ROI <= threshold`. Log a specific message when a step is skipped due to ROI filtering.",
            "status": "pending",
            "testStrategy": "Integration test with a mock pack containing high and low ROI steps. Verify via logs that steps failing the threshold check are skipped.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Verify ROI Filtering in TUI/Interactive Mode",
            "description": "Ensure the ROI filtering logic is correctly applied or visualized in the TUI mode if applicable.",
            "dependencies": [
              4
            ],
            "details": "Review the TUI implementation in `internal/tui` or the shared execution logic in `internal/app` to ensure the filtering condition applies consistently across both plain and TUI modes. If the TUI displays a list of steps beforehand, consider visually dimming or marking skipped steps, or simply enforcing the skip during execution.",
            "status": "pending",
            "testStrategy": "Manual verification or integration test running in TUI mode (if testable) to ensure consistency with the plain runner.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-31T20:45:25.293Z"
      },
      {
        "id": 14,
        "title": "Restore CLI Run-All Flag and TUI Auto-Execution Logic",
        "description": "Re-implement the `--run-all` CLI flag and connect it to the TUI's initialization sequence to enable immediate, sequential execution of all steps upon startup, fixing a regression from recent commits.",
        "details": "This task focuses on rewiring the bridge between the CLI entry point and the TUI's state machine to support unattended (visual) execution.\n\n1. **CLI Layer (`internal/cli/run.go`)**: \n   - Re-introduce the `--run-all` (bool) flag in the `run` command configuration.\n   - Update the `runAction` function to capture this boolean and pass it into the `tui.NewModel` factory function.\n\n2. **TUI Initialization (`internal/tui/tui.go` & `model.go`)**:\n   - Update `NewModel` signature to accept `autoRun bool`.\n   - Store this boolean in the `Model` struct.\n   - Define a `StartAutoRunMsg` struct (empty struct).\n   - In the `Init()` method of the Bubble Tea model: if `autoRun` is true, return a `Cmd` that emits `StartAutoRunMsg` immediately. This ensures the TUI renders at least one frame before execution starts.\n\n3. **TUI Update Loop (`internal/tui/update.go`)**:\n   - Handle `StartAutoRunMsg`: Set the internal `runningAll` state to true (bridging the `autoAdvance` logic) and trigger the execution of the step at `currentIdx`.\n   - Verify the `Update` loop handles the completion of a step (`StepSuccessMsg`) by checking if `runningAll` is active. If so, increment `currentIdx` and immediately dispatch the command to run the next step, ensuring the sequential flow continues without user input.",
        "testStrategy": "1. **Unit Test**: In `internal/tui/tui_test.go`, initialize a model with `autoRun=true` and verify that `Init()` returns the `StartAutoRunMsg` command.\n2. **Manual Integration**: Run `oraclepack run --run-all --pack ./examples/simple` and verify that the UI opens and immediately begins executing steps 1..N without requiring the user to press 'r' or 'enter'.\n3. **Regression Check**: Ensure running without the flag still requires manual initiation.",
        "status": "done",
        "dependencies": [
          "7",
          "9",
          "11",
          "13"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Add Run-All Flag to CLI Configuration",
            "description": "Re-introduce the --run-all boolean flag to the run command and propagate it to the action handler.",
            "dependencies": [],
            "details": "In `internal/cli/run.go`, define a `runAll` boolean variable. Register it in the `init()` function for the `runCmd` using `runCmd.Flags().BoolVar`. Ensure the description clearly indicates it triggers immediate sequential execution. This flag will be used to signal the TUI to start automatically.",
            "status": "pending",
            "testStrategy": "Verify via `oraclepack run --help` that the flag exists and parses correctly in a unit test for the CLI command.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Update TUI Model and Factory for Auto-Run State",
            "description": "Modify the TUI Model struct and NewModel factory to accept and store the auto-run configuration.",
            "dependencies": [
              1
            ],
            "details": "In `internal/tui/model.go` (and `tui.go` if the factory is there), add an `autoRun bool` field to the `Model` struct. Update the `NewModel` function signature to accept this boolean argument. Update the call site in `internal/cli/run.go` to pass the captured `runAll` flag value.",
            "status": "pending",
            "testStrategy": "Unit test `NewModel` to ensure the returned model has the `autoRun` field set correctly based on the input.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement StartAutoRunMsg and Init Trigger",
            "description": "Create a message type for auto-start and trigger it in the Bubble Tea Init method if configured.",
            "dependencies": [
              2
            ],
            "details": "In `internal/tui/model.go`, define `type StartAutoRunMsg struct{}`. In the `Init()` method of the `Model`, check if `m.autoRun` is true. If so, return a command that sends `StartAutoRunMsg` (e.g., `func() tea.Msg { return StartAutoRunMsg{} }`). If false, return `nil` or existing init commands.",
            "status": "pending",
            "testStrategy": "Unit test the `Init()` method of the model. When `autoRun` is true, assert that the returned `tea.Cmd` produces a `StartAutoRunMsg`.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Handle Auto-Run Message in Update Loop",
            "description": "Implement the logic to catch StartAutoRunMsg and transition the state to running all steps.",
            "dependencies": [
              3
            ],
            "details": "In `internal/tui/update.go`, add a case for `StartAutoRunMsg` in the `Update` function. This handler should set `m.runningAll = true` (or equivalent state tracking variable) and return the command to execute the current step (likely `m.executeStep(m.currentIdx)`). Ensure this sets the UI into the correct 'running' mode visually.",
            "status": "pending",
            "testStrategy": "Unit test the `Update` loop. Pass `StartAutoRunMsg` to a model and assert that the resulting model state indicates 'running' and returns an execution command.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Ensure Sequential Execution in Step Completion",
            "description": "Verify and fix the step completion logic to continue execution if the run-all mode is active.",
            "dependencies": [
              4
            ],
            "details": "In `internal/tui/update.go`, within the `StepSuccessMsg` (or equivalent) handler, check if `m.runningAll` is true. If so, and if there are more steps, increment the index and return the command to execute the next step immediately. This ensures the loop continues without user intervention until all steps are done or failure occurs.",
            "status": "pending",
            "testStrategy": "Integration test: Initialize model with `autoRun=true` and mock step execution. Verify that multiple steps execute in sequence without intermediate input.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2026-01-01T00:21:59.762Z"
      },
      {
        "id": 15,
        "title": "Implement Interactive ROI Filtering in TUI",
        "description": "Enable real-time filtering of the step list within the TUI using a new keybinding to set ROI thresholds, updating the view dynamically.",
        "details": "Extend the Bubble Tea model in `internal/tui` to support dynamic list filtering based on the ROI data structure from Task 13.\n\n1. **Model Extension**: Update `internal/tui/model.go`. Add fields to preserve the full list of steps (`allSteps []list.Item`) separate from the displayed list. Add state for `roiThreshold` (float64), `roiFilterActive` (bool), and an input bubble (`textinput.Model`) to capture user input.\n\n2. **Input Handling**: In `internal/tui/update.go`, bind the key 'f'. When pressed, switch the view mode to show the text input. On 'Enter' in the input, parse the value as a float, update `roiThreshold`, and trigger a list rebuild. On 'Esc', cancel the filter input.\n\n3. **Filtering Logic**: Create a method `filterList()` that iterates over `allSteps`. Type-assert items to the Step concrete type (from `internal/pack` or `internal/tui` wrapper). If `step.ROI >= roiThreshold` (or logic matches the existing CLI filter logic), keep the item. Call `m.list.SetItems(filtered)` to update the view immediately.\n\n4. **Visual Feedback**: Modify `internal/tui/view.go`. Add a status indicator (e.g., using `lipgloss` styles) in the header or footer showing the active filter state (e.g., \"Filter: ROI >= 5.0\") so the user knows why steps might be hidden.",
        "testStrategy": "1. **Unit Tests**: Create `internal/tui/filter_test.go`. Initialize the model with a mocked list of steps having known ROI values (e.g., 1.0, 5.5, 10.0). Invoke the filtering function with various thresholds and assert that the `list.Model` items slice contains the correct subset of steps.\n2. **Interactive Verification**: Run the TUI with a sample pack. Press 'f', enter a value, and verify the list shrinks. Verify that navigating the filtered list and selecting a step works correctly (indices map to the correct underlying step).",
        "status": "done",
        "dependencies": [
          "9",
          "13"
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Extend TUI Model with ROI Filtering State",
            "description": "Modify the Bubble Tea model to hold filtering state including all steps, threshold value, and input field.",
            "dependencies": [],
            "details": "Update `internal/tui/model.go`. Add `allSteps []list.Item` to store the original unfiltered list. Add `roiThreshold` (float64, default 0), `isFiltering` (bool), and `filterInput` (textinput.Model). Initialize the text input with standard styles in `NewModel`.",
            "status": "pending",
            "testStrategy": "Unit test ensuring `NewModel` initializes the new fields correctly and the text input model is ready.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Filter Input Keybinding and State Switching",
            "description": "Add key handling to toggle the filter input mode when 'f' is pressed.",
            "dependencies": [
              1
            ],
            "details": "In `internal/tui/update.go`, inside the `Update` loop, check for key 'f'. If not filtering, set `isFiltering = true`, focus `filterInput`, and prevent list navigation. If 'Esc' is pressed while filtering, clear input, reset `isFiltering = false`, and blur input.",
            "status": "pending",
            "testStrategy": "Unit test verifying that sending KeyMsg('f') changes the model state to `isFiltering=true`.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement List Filtering Logic",
            "description": "Create the logic to filter the displayed list based on the ROI threshold.",
            "dependencies": [
              1
            ],
            "details": "Create a method `applyFilter()` in `internal/tui/model.go` (or a helper). It should iterate `m.allSteps`. Type assert items to `StepItem` (or the specific item wrapper). Check if `item.Step.ROI >= m.roiThreshold`. Collect matching items and update `m.list.SetItems(filtered)`. Handle invalid float input gracefully.",
            "status": "pending",
            "testStrategy": "Unit test creating a model with known steps (ROI 1.0, 5.0, 10.0), calling `applyFilter` with threshold 5.0, and asserting list contains 2 items.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Connect Filter Input to Update Loop",
            "description": "Process the text input submission to trigger the filter logic.",
            "dependencies": [
              2,
              3
            ],
            "details": "In `internal/tui/update.go`, when `isFiltering` is true, pass messages to `m.filterInput.Update(msg)`. If 'Enter' is pressed, parse `m.filterInput.Value()` as float64, update `m.roiThreshold`, call `m.applyFilter()`, and set `isFiltering = false`. If parsing fails, perhaps show an error or ignore.",
            "status": "pending",
            "testStrategy": "Integration test: Simulate 'f', type '2.5', press 'Enter', verify `roiThreshold` is 2.5 and list items are updated.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Update TUI View for Filter Status",
            "description": "Render the filter input box or the active filter status in the UI.",
            "dependencies": [
              1
            ],
            "details": "Modify `internal/tui/view.go`. If `isFiltering`, render `m.filterInput.View()`. If `!isFiltering` and `roiThreshold > 0`, render a status line (e.g., using `lipgloss`) indicating 'ROI Filter: >= X.X'. Ensure this fits within the layout without breaking the list view.",
            "status": "pending",
            "testStrategy": "Visual verification or string containment test on `View()` output to ensure the input field or status text appears when appropriate.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2026-01-01T00:30:34.168Z"
      },
      {
        "id": 16,
        "title": "Implement Robust Output Directory Handling and CLI Override",
        "description": "Implement logic to parse `out_dir` from the pack prelude, support a CLI override flag, and propagate the resolved path to the execution environment with a safe default.",
        "details": "1. **CLI Extension (`internal/cli`)**: Add a global `--out-dir` string flag to the root or `run` command using Cobra. Map this flag to the application configuration.\n2. **Configuration Resolution (`internal/app`)**: In the application entry point (wired in Task 6), implement precedence logic: `CLI Flag` > `Prelude Metadata (out_dir)` > `Default (.)`. Ensure that if no value is provided, it defaults to the current working directory (`.`) rather than an empty string (which could imply root).\n3. **Directory Provisioning**: Before starting the `internal/exec.Runner`, ensure the target directory exists using `os.MkdirAll(path, 0755)`. Handle permission errors gracefully.\n4. **Execution Context (`internal/exec`)**: Pass the resolved path to the `Runner` struct. Ensure the `os/exec.Cmd` uses this path as its `Dir` field so that scripts run relative to the output directory. Alternatively, export it as an environment variable (e.g., `ORACLE_OUT_DIR`).\n5. **Parser Integration**: Verify `internal/pack.DeriveMetadata` correctly extracts the `out_dir` pattern from the Markdown prelude as implemented in Task 2.",
        "testStrategy": "1. **Unit Tests (`internal/app`)**: Create a test case validating the precedence logic (e.g., define `out_dir` in a mock pack, provide a different CLI flag, assertion should match CLI flag). Test the default case ensuring it resolves to `.`.\n2. **Integration Tests**: Create a temporary test pack with `out_dir: ./subdir`. Run it and verify `subdir` is created and output files appear there. Repeat with the `--out-dir` flag overriding it to `./override`.",
        "status": "done",
        "dependencies": [
          "2",
          "4",
          "6",
          "7"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Add Output Directory Flag to CLI",
            "description": "Extend the root command in internal/cli/root.go to include a global string flag '--out-dir' (shorthand '-o') and bind it to the configuration.",
            "dependencies": [],
            "details": "In 'internal/cli/root.go', define a 'outDir' variable. Add 'rootCmd.PersistentFlags().StringVarP(&outDir, \"out-dir\", \"o\", \"\", ...)' in the init function. Ensure this value is accessible when commands execute, potentially by binding it to viper or passing it to the config struct.",
            "status": "pending",
            "testStrategy": "Unit test in 'internal/cli/root_test.go' verifying that parsing arguments with '--out-dir' correctly sets the variable.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Verify and Expose Pack Metadata Extraction",
            "description": "Review internal/pack to ensure the Pack struct includes an OutDir field populated from the prelude, and ensure it is accessible to the app layer.",
            "dependencies": [
              1
            ],
            "details": "Check 'internal/pack/parser.go' and the 'Pack' struct definition. Verify 'DeriveMetadata' regex matches 'out_dir: <path>'. If missing, implement the regex extraction logic. Ensure the 'Pack' struct has a public 'OutDir' field (string) that holds this value.",
            "status": "pending",
            "testStrategy": "Unit test in 'internal/pack/parser_test.go' with a sample markdown containing 'out_dir: ./build' in the prelude and asserting the parsed Pack struct reflects this.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement Configuration Resolution Logic",
            "description": "Create a resolution function in internal/app that determines the final output directory based on precedence rules.",
            "dependencies": [
              2
            ],
            "details": "In 'internal/app/config.go' (or similar), implement 'ResolveOutDir(cliFlag string, packOutDir string) string'. Logic: If cliFlag != \"\", use cliFlag. Else if packOutDir != \"\", use packOutDir. Else return \".\" (current directory). Ensure absolute path resolution is considered if needed.",
            "status": "pending",
            "testStrategy": "Table-driven unit tests in 'internal/app/config_test.go' covering all permutation cases (CLI only, Pack only, Both, None).",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Provision Output Directory",
            "description": "Add logic to the application flow to create the resolved output directory before execution begins.",
            "dependencies": [
              3
            ],
            "details": "In the main execution flow (likely 'internal/app/app.go' or 'internal/cli/run.go'), call 'ResolveOutDir'. Then use 'os.MkdirAll(resolvedPath, 0755)' to ensure the directory exists. Return a user-friendly error if creation fails (e.g., permissions issues).",
            "status": "pending",
            "testStrategy": "Integration test creating a temporary directory structure, running the provisioning logic, and verifying the directory is created on the filesystem.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Propagate Directory to Execution Runner",
            "description": "Update the Runner initialization in internal/exec to accept the resolved working directory and use it during command execution.",
            "dependencies": [
              4
            ],
            "details": "Modify 'internal/exec/runner.go': Update the 'Runner' struct to include a 'WorkDir' field. Update 'NewRunner' signature. In the 'Run' or 'execCmd' method, set 'cmd.Dir = r.WorkDir'. Optionally set an 'ORACLE_OUT_DIR' environment variable in 'cmd.Env'. Update call sites in 'internal/app' to pass the resolved path.",
            "status": "pending",
            "testStrategy": "Integration test in 'internal/exec/runner_test.go': Configure a Runner with a specific temporary WorkDir, execute 'pwd', and assert the output matches the WorkDir.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2026-01-01T01:17:49.560Z"
      },
      {
        "id": 17,
        "title": "Implement TUI Post-Run Navigation and State Reset",
        "description": "Implement a state machine within the TUI to manage transitions between list, running, and summary views, including logic for resetting the run state.",
        "details": "Extend the Bubble Tea model in `internal/tui` to support distinct application states: `ViewSteps`, `ViewRunning`, and `ViewDone`.\n\n1. **State Machine (`internal/tui/model.go`)**: Introduce an enumerated type for the view state. Refactor the `Update` method to process key messages differently based on the current state.\n2. **Done Screen**: Create a new view function `viewDone()` that displays a summary of the execution (e.g., 'All steps completed', 'Failed at step X').\n3. **Navigation Logic (`internal/tui/update.go`)**:\n   - **'b' (Back)**: Transition from `ViewDone` to `ViewSteps` without resetting state, allowing the user to review the list.\n   - **'n' (New Run)**: Trigger a `ResetState()` function. This function must traverse the `internal/state.RunState` and mark all steps as `StatusPending`, clear the execution logs in the UI, reset the list cursor to index 0, and transition to `ViewSteps`.\n   - **'r' (Rerun)**: If in `ViewDone`, trigger the execution logic for the specific failed step or restart the sequence (depending on context), transitioning back to `ViewRunning`.\n   - **'q' (Quit)**: Terminate the application.\n4. **Integration**: Ensure that the completion of the `Run All` command (Task 11) automatically transitions the state to `ViewDone`.",
        "testStrategy": "1. **Unit Tests (`internal/tui/model_test.go`)**: Create a test initializing the model in `ViewDone`. Send the 'n' KeyMsg and assert that the view state changes to `ViewSteps` and the step status map is cleared. Send 'b' and assert the view changes but state remains.\n2. **Manual Integration**: Run the full tool. Execute all steps. Verify the UI automatically switches to the Done screen. Press 'n' and verify all green checkmarks reset to empty/pending. Press 'q' to ensure clean exit.",
        "status": "done",
        "dependencies": [
          "3",
          "11",
          "15"
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Define TUI View States and Refactor Model",
            "description": "Introduce an enumerated type for application states and update the Model struct to track the current view state.",
            "dependencies": [],
            "details": "In `internal/tui/model.go`, define an enum (e.g., `ViewState`) with values `ViewSteps`, `ViewRunning`, and `ViewDone`. Add a `state ViewState` field to the `Model` struct. Refactor `NewModel` to initialize the state to `ViewSteps`.",
            "status": "pending",
            "testStrategy": "Unit test verifying that NewModel initializes with the correct default ViewState.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Update Loop State Routing",
            "description": "Refactor the Bubble Tea Update method to delegate logic based on the current ViewState.",
            "dependencies": [
              1
            ],
            "details": "In `internal/tui/update.go`, modify the `Update` function to switch on `m.state`. Create separate handler methods (e.g., `updateSteps`, `updateRunning`, `updateDone`) for handling key messages specific to each view. Ensure global keys like Ctrl+C still work universally.",
            "status": "pending",
            "testStrategy": "Unit tests mocking key presses in different states to ensure only relevant handlers are triggered.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement ViewDone UI Rendering",
            "description": "Create the view logic for the summary screen displayed after execution completes.",
            "dependencies": [
              1
            ],
            "details": "In `internal/tui/view.go`, implement a `viewDone()` function. It should render a summary string based on the `RunState` (e.g., success message or error details). Update the main `View()` method to call `viewDone()` when the state is `ViewDone`.",
            "status": "pending",
            "testStrategy": "Manual verification by forcing the state to ViewDone and checking the rendered output for correct summary information.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement State Transitions and Reset Logic",
            "description": "Add logic to handle transitions between views, including a complete state reset for new runs.",
            "dependencies": [
              2
            ],
            "details": "In `internal/tui/update.go`, implement the 'n' key logic in `ViewDone` to call a new method `resetState()`. This method must iterate through `m.store.Steps`, resetting their status to `StatusPending` and clearing logs. It should then switch `m.state` back to `ViewSteps`. Implement 'b' to switch to `ViewSteps` without resetting.",
            "status": "pending",
            "testStrategy": "Unit test initializing model in ViewDone with executed steps, sending 'n', and verifying steps are pending and state is ViewSteps.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Integrate Run Completion with ViewDone Transition",
            "description": "Ensure the TUI automatically transitions to the Done view when the runner finishes execution.",
            "dependencies": [
              2
            ],
            "details": "Modify the `Update` method in `internal/tui/update.go`. Listen for a specific completion message (e.g., `RunFinishedMsg`) generated by the execution command. Upon receipt, update `m.state` to `ViewDone`. Ensure this transition preserves the final execution status for display.",
            "status": "pending",
            "testStrategy": "Integration test simulating a run completion message and asserting the model state transitions to ViewDone.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2026-01-01T01:21:52.861Z"
      },
      {
        "id": 19,
        "title": "Fix Regression in Relative Path Handling and Output Directory Context",
        "description": "Modify the application core to prevent the runner from changing the working directory to out_dir, ensuring relative paths resolve from the project root while preserving output directory availability via environment variables.",
        "details": "This task addresses a regression where setting an output directory changed the execution working directory, breaking scripts that rely on relative paths for sourcing files or reading project assets.\n\n1. **Modify App Wiring (`internal/app/app.go`)**: Locate the `RunPlain` method and any TUI initialization logic where the `exec.Runner` is constructed. Identify the line where `Runner.WorkDir` is currently set to `config.OutDir`.\n2. **Restore Working Directory**: Change the `WorkDir` assignment to use the project root (current working directory `.`) regardless of whether `out_dir` is set. This ensures paths like `./internal/...` resolve correctly.\n3. **Environment Variable Injection**: Ensure that `config.OutDir` is passed into the runner's `Env` map (e.g., as `ORACLE_OUT_DIR`). This allows scripts to explicitly target the output directory for artifacts (e.g., `cp result.json $ORACLE_OUT_DIR/`).\n4. **Directory Creation**: Retain the `os.MkdirAll(config.OutDir, 0755)` call to ensure the target directory exists before execution begins.",
        "testStrategy": "Create a regression test pack `examples/test_paths_regression`.\n1. **Setup**: The pack should contain a step that attempts to read a known file from the project root using a relative path (e.g., `cat ./README.md`).\n2. **Output Verification**: The step should also write a file to the configured output path variable: `echo 'test' > \"$ORACLE_OUT_DIR/verification.txt\"`.\n3. **Execution**: Run this pack with the CLI flag `--out-dir ./temp_output`.\n4. **Assertions**: Verify the command succeeds (exit code 0), proving `README.md` was found. Verify `./temp_output/verification.txt` exists, proving the environment variable was passed correctly.",
        "status": "done",
        "dependencies": [
          "4",
          "6",
          "16"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2026-01-01T18:50:16.938Z"
      }
    ],
    "metadata": {
      "version": "1.0.0",
      "lastModified": "2026-01-01T18:50:16.939Z",
      "taskCount": 19,
      "completedCount": 18,
      "tags": [
        "master"
      ],
      "created": "2026-01-01T19:43:53.899Z",
      "description": "Tasks for master context",
      "updated": "2026-01-01T19:43:53.899Z"
    }
  },
  "bugfix-tui": {
    "tasks": [
      {
        "id": 1,
        "title": "Define Runtime Overrides Data Model & Step-Aware Merge Logic",
        "description": "Create the core data structures to hold runtime overrides and implement the logic to merge them with baseline flags on a per-step basis.",
        "details": "Create a new package `internal/overrides`. Define `RuntimeOverrides` struct containing `AddedFlags` ([]string), `RemovedFlags` ([]string), `ChatGPTURL` (string), and `ApplyToSteps` (map[string]bool). Implement `EffectiveFlags(stepID string, baseline []string) []string` which returns the final flag set for a given step. If the step is not in `ApplyToSteps`, return baseline. Otherwise, append added flags, remove removed flags, and inject `--chatgpt-url` if set. Add unit tests covering various combinations (add/remove intersection, empty sets).",
        "testStrategy": "Unit tests in `internal/overrides/merge_test.go` verifying that `EffectiveFlags` correctly computes the final slice for targeted and non-targeted steps.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [],
        "complexity": 3,
        "recommendedSubtasks": 0,
        "expansionPrompt": "No expansion needed.",
        "updatedAt": "2026-01-02T16:21:30.521Z"
      },
      {
        "id": 2,
        "title": "Implement Oracle Invocation Extraction (Scanning)",
        "description": "Develop a robust scanner to identify and extract `oracle` command invocations from bash scripts, handling multi-line continuations.",
        "details": "Create `internal/exec/oracle_scan.go`. Implement `ExtractOracleInvocations(script string) []OracleInvocation`. It must detect lines starting with `oracle` (allowing for indentation) and capture the full command including multi-line continuations (lines ending with `\\`). The struct `OracleInvocation` should hold the `Raw` command string and a `Display` version for UI. Use regex or a simple line-by-line state machine. Ensure it handles edge cases like trailing spaces before backslashes.\n\n[2026-01-02] Fixed scanner compile/test issues: corrected backslash suffix literal in internal/overrides/scanner.go; converted multiline expected command to raw string in internal/overrides/scanner_test.go. go test ./internal/overrides now passes.\n\n[2026-01-02] Implemented ExtractOracleInvocations in internal/exec/oracle_scan.go, moved scanner from overrides to exec package, updated tests, and removed overrides OracleInvocation type. go test ./internal/exec ./internal/overrides passes.",
        "testStrategy": "Unit tests in `oracle_scan_test.go` with table-driven tests containing various shell script snippets (single line, multiline, indented) to assert correct extraction.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Core State Machine for Line Parsing",
            "description": "Develop the low-level state machine logic to identify the start of 'oracle' commands and track multi-line continuations.",
            "dependencies": [],
            "details": "Implement a state tracker (e.g., boolean flag `inCommand`) in `internal/exec/oracle_scan.go`. The logic must handle indentation (leading whitespace), identify lines starting with the token `oracle`, and detect the line continuation character `\\` at the end of the line (ignoring trailing whitespace) to determine if the next line belongs to the same command.",
            "status": "pending",
            "testStrategy": "Unit tests verifying state transitions on specific string inputs, ensuring indentation and trailing backslashes trigger the correct state changes.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement ExtractOracleInvocations Function",
            "description": "Develop the public extractor function that processes full scripts to return structured OracleInvocation objects.",
            "dependencies": [
              1
            ],
            "details": "Implement `ExtractOracleInvocations(script string) []OracleInvocation` in `internal/exec/oracle_scan.go`. This function should split the script by lines, utilize the core state machine to aggregate lines into full commands, filter out comments (lines starting with `#`), and populate the `Raw` and `Display` fields of the `OracleInvocation` struct.",
            "status": "pending",
            "testStrategy": "Table-driven tests in `oracle_scan_test.go` using realistic bash script fixtures containing mixed commands, comments, and multi-line oracle invocations.",
            "parentId": "undefined"
          }
        ],
        "complexity": 6,
        "recommendedSubtasks": 2,
        "expansionPrompt": "Break down the scanner implementation into: 1. A core line-parsing state machine that handles indentation, command detection, and line continuation logic (handling backslashes). 2. A higher-level extractor that processes whole scripts and returns structured `OracleInvocation` objects, including edge case handling for comments or quoting.",
        "updatedAt": "2026-01-02T18:14:10.000Z"
      },
      {
        "id": 3,
        "title": "Upgrade Flag Injection to Support Multi-line Commands",
        "description": "Refactor the existing flag injection logic to safely insert flags into multi-line `oracle` commands without breaking shell syntax.",
        "details": "Modify `internal/exec/inject.go` (or create a new version if preferred). The `InjectFlags` function should use the logic from the scanner to locate the insertion point. For multi-line commands (ending with `\\`), inject flags before the backslash on the first line or append to the argument list in a way that preserves valid shell syntax. Ensure it still handles single-line commands correctly. This replaces the naive line-replacement approach.\n\n[2026-01-02] Implemented multi-line flag injection: insert flags after oracle token, preserve indentation, handle continuation backslashes, and skip commented lines. Updated inject tests for multiline and comment cases. go test ./internal/exec passes.",
        "testStrategy": "Enhance `inject_test.go`. specific cases: `oracle \\` -> `oracle --flag \\`, `  oracle arg \\` -> `  oracle --flag arg \\`. Verify syntax validity.",
        "priority": "high",
        "dependencies": [
          "2"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Establish Injection Test Suite",
            "description": "Create a comprehensive test suite in `inject_test.go` covering target scenarios.",
            "dependencies": [],
            "details": "Define a table-driven test structure with cases for: simple single-line commands, multi-line commands with backslash continuations, indented commands, and commands with existing arguments. These tests should currently fail or define the expected behavior for the TDD process.",
            "status": "pending",
            "testStrategy": "Create `inject_test.go` ensuring it compiles and fails on the new logic requirements.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Insertion Point Logic",
            "description": "Develop the logic to locate the precise insertion index within the command string.",
            "dependencies": [
              1
            ],
            "details": "Modify `internal/exec/inject.go` to analyze the command string. It must identify the end of the `oracle` binary token while skipping leading whitespace. It should distinguish between immediate arguments and line continuation characters (\\) to determine where flags can be safely added.",
            "status": "pending",
            "testStrategy": "Unit tests verifying that the correct index is returned for various command string formats.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement Single-Line Injection",
            "description": "Refactor `InjectFlags` to handle standard single-line command strings safely.",
            "dependencies": [
              2
            ],
            "details": "Implement the string manipulation logic for cases without newlines. Ensure that the generated string inserts the provided flags at the determined index with exactly one space of padding on either side, preventing double-spacing issues or concatenated words.",
            "status": "pending",
            "testStrategy": "Run `inject_test.go` focusing on single-line cases like `oracle arg` -> `oracle --flag arg`.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement Multi-Line Injection",
            "description": "Extend `InjectFlags` to safely handle commands using backslash line continuations.",
            "dependencies": [
              3
            ],
            "details": "Add logic to detect if the command line ends with a backslash `\\`. The injection routine must insert the flags *before* the backslash on that specific line, ensuring the shell interprets the flags as part of the initial command call without breaking the continuation syntax.",
            "status": "pending",
            "testStrategy": "Run `inject_test.go` focusing on multi-line cases like `oracle \\` -> `oracle --flag \\`.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Finalize Formatting and Edge Cases",
            "description": "Polish the injection logic to handle indentation and irregular whitespace.",
            "dependencies": [
              4
            ],
            "details": "Ensure that if the original command was indented (e.g., inside a shell function), the injection process preserves this indentation. Handle edge cases where multiple spaces exist between `oracle` and the backslash, ensuring the resulting command is syntactically valid bash.",
            "status": "pending",
            "testStrategy": "Pass all tests in `inject_test.go` including indentation and whitespace variation cases.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 0,
        "expansionPrompt": "No expansion needed.",
        "updatedAt": "2026-01-02T18:03:59.000Z"
      },
      {
        "id": 4,
        "title": "Implement Mode 2 Validation Runner",
        "description": "Create a validation routine that runs `oracle --dry-run summary` to verify that overrides result in valid commands.",
        "details": "Create `internal/exec/oracle_validate.go`. Implement `ValidateOverrides(ctx, ...)` which iterates through targeted steps, extracts oracle invocations using the scanner from Task 2, applies overrides using the model from Task 1, and executes the resulting command with `--dry-run summary`. It should capture specific error output if the command fails. This ensures the user's configuration is valid before execution.\n\n[2026-01-02] Added validation runner in internal/exec/oracle_validate.go with ValidationError struct, dry-run execution, PATH export, and tests in internal/exec/oracle_validate_test.go. go test ./internal/exec ./internal/overrides passes.",
        "testStrategy": "Integration tests mocking the `oracle` binary (e.g., using a helper script in `testdata` that checks for `--dry-run` arg). Verify that it returns specific errors for invalid flags.",
        "priority": "high",
        "dependencies": [
          "1",
          "2"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Scaffold Validation Interface and Types",
            "description": "Define the function signature and result structures for the validation runner in a new file.",
            "dependencies": [],
            "details": "Create `internal/exec/oracle_validate.go`. Define the public function `ValidateOverrides(ctx context.Context, steps []model.Step, overrides model.Overrides) ([]ValidationError, error)`. Define the `ValidationError` struct to hold StepID, Command, and ErrorMessage. This establishes the contract for the validation logic.",
            "status": "pending",
            "testStrategy": "None (structural setup only)",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Step Iteration and Command Preparation",
            "description": "Implement the logic to loop through targeted steps and prepare the oracle commands for validation.",
            "dependencies": [
              1
            ],
            "details": "In `ValidateOverrides`, iterate through the provided steps. call `oracle_scan.ExtractOracleInvocations` (from Task 2) to get the raw command. Apply the overrides (from the model) to construct the temporary command string that includes the new flags. This step ensures we have the exact string that needs testing.",
            "status": "pending",
            "testStrategy": "Unit test verifying that the correct number of commands are prepared from a given set of steps and overrides.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement Dry-Run Execution Logic",
            "description": "Create the helper function to execute the constructed command with the dry-run flag.",
            "dependencies": [
              1
            ],
            "details": "Implement a private function, e.g., `execDryRun(ctx, command string) (string, error)`. This should parse the command string, append the `--dry-run summary` argument, and execute it using `os/exec`. It must capture both Stdout and Stderr to return distinct failure reasons if the process exits with a non-zero code.",
            "status": "pending",
            "testStrategy": "Unit test with a simple echo command to verify arguments are appended and output is captured.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement Error Parsing and Result Aggregation",
            "description": "Connect the execution logic to the iteration loop and handle failure scenarios.",
            "dependencies": [
              2,
              3
            ],
            "details": "Call `execDryRun` within the iteration loop from Subtask 2. If the command fails, extract the relevant error message from Stderr, populate a `ValidationError` struct with the Step ID, and append it to the results slice. Ensure the loop continues to validate remaining steps even if one fails.",
            "status": "pending",
            "testStrategy": "Unit test simulating mixed results (some pass, some fail) to ensure aggregation works correctly.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Create Integration Tests with Mock Oracle",
            "description": "Verify the full validation flow using a mock script to simulate the oracle binary.",
            "dependencies": [
              4
            ],
            "details": "Create `internal/exec/oracle_validate_test.go`. Use `GO_EXEC_TEST` pattern or a shell script in `testdata` that acts as the `oracle` binary. It should return exit code 0 when `--dry-run` is present and valid flags are used, and exit code 1 with an error message for specific 'invalid' flags. Verify `ValidateOverrides` correctly identifies these failures.",
            "status": "pending",
            "testStrategy": "Integration test using a mock executable to simulate external process behavior.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 0,
        "expansionPrompt": "No expansion needed.",
        "updatedAt": "2026-01-02T18:18:21.000Z"
      },
      {
        "id": 5,
        "title": "Implement TUI State Machine for Overrides Flow",
        "description": "Scaffold the Bubble Tea model for the overrides wizard, managing transitions between main screen and overrides sub-screens.",
        "details": "In `internal/tui/overrides_flow.go`, define a `Model` that wraps the specific sub-models (picker, url, confirm). Define messages `OverridesStartedMsg`, `OverridesAppliedMsg`, `OverridesCancelledMsg`. Implement the `Update` loop to handle switching between steps: Flags -> Steps -> URL -> Confirm. Integrate this as a new state in the main `internal/tui/tui.go` model.\n\n[2026-01-02] Added overrides flow scaffold (internal/tui/overrides_flow.go) with steps, messages, and basic navigation. Integrated into main TUI with ViewOverrides state and key 'o' to start; handles apply/cancel transitions. go test ./internal/tui passes.",
        "testStrategy": "Unit/Model tests verifying state transitions (e.g., sending `NextMsg` moves from Flags to Steps).",
        "priority": "medium",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Overrides Flow Model and Message Types",
            "description": "Scaffold the basic structures and message types required for the overrides wizard in the TUI.",
            "dependencies": [],
            "details": "Create `internal/tui/overrides_flow.go`. Define a `Model` struct that includes fields to track the current wizard step (e.g., Flags, Steps, URL, Confirm) and placeholders for sub-models. Define the Bubble Tea messages: `OverridesStartedMsg`, `OverridesAppliedMsg` (which should include the final configuration), and `OverridesCancelledMsg`.",
            "status": "pending",
            "testStrategy": "Unit tests ensuring the Model initializes correctly and messages are defined with appropriate payloads.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Navigation Logic and Update Loop",
            "description": "Develop the state machine logic to handle transitions between the different steps of the overrides wizard.",
            "dependencies": [
              1
            ],
            "details": "In `internal/tui/overrides_flow.go`, implement the `Update` method. Handle specific key bindings (e.g., Enter/Tab for Next, Esc for Back) to modify the state variable. Ensure the `View` method switches rendering based on the active step (Flags -> Steps -> URL -> Confirm).",
            "status": "pending",
            "testStrategy": "Write model tests that simulate key presses and assert that the model state transitions from one step to the next correctly.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Integrate Overrides Flow into Main Application",
            "description": "Connect the independent overrides flow into the main application loop to enable user access.",
            "dependencies": [
              2
            ],
            "details": "Update `internal/tui/tui.go` to include the `OverridesFlow` model. Modify the root `Update` function to listen for `OverridesStartedMsg` to switch the view to the wizard, and handle `OverridesAppliedMsg` or `OverridesCancelledMsg` to return focus to the main dashboard.",
            "status": "pending",
            "testStrategy": "Manual verification or integration tests to confirm that triggering the flow switches the UI context and completing it returns to the main screen.",
            "parentId": "undefined"
          }
        ],
        "complexity": 7,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Split this into: 1. Defining the message types and the top-level parent model for the overrides flow. 2. Implementing the navigation logic (Next/Back transitions) and state management between sub-screens. 3. Integrating this new flow into the existing `internal/tui/tui.go` main loop without regressing current functionality.",
        "updatedAt": "2026-01-02T18:20:00.000Z"
      },
      {
        "id": 6,
        "title": "Develop Multi-select Flags Picker UI",
        "description": "Build a TUI component to allow users to select/deselect common oracle flags.",
        "details": "Create `internal/tui/overrides_flags.go`. Use `github.com/charmbracelet/bubbles/list` or a custom component. It should display a curated list of flags (e.g., `--debug`, `--json`). Allow multi-selection. Visually distinguish between baseline flags (pre-selected/immutable context) and override flags. Store selection in the `RuntimeOverrides` model.\n\n[2026-01-02] Added flags picker component in internal/tui/overrides_flags.go with multi-select, baseline handling, and custom delegate. Integrated into overrides flow flags step view. go test ./internal/tui passes.",
        "testStrategy": "Manual visual testing + Unit tests for the list selection logic (toggling items updates the model correctly).",
        "priority": "medium",
        "dependencies": [
          "5"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Scaffold Flags Picker Component",
            "description": "Initialize the Bubble Tea component for the flags picker using the bubbles/list library.",
            "dependencies": [],
            "details": "Create `internal/tui/overrides_flags.go`. Define a `FlagsPickerModel` struct that embeds `list.Model`. Implement the standard `Init`, `Update`, and `View` methods. Initialize the list with a default width and height in a constructor function `NewFlagsPicker`. Ensure imports for `github.com/charmbracelet/bubbles/list` are correct.",
            "status": "pending",
            "testStrategy": "Verify the component compiles and renders an empty list in a standalone main harness.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Define Flag Items and Data Source",
            "description": "Create the data structures for list items and populate the curated list of oracle flags.",
            "dependencies": [
              1
            ],
            "details": "Define a `FlagItem` struct that implements `list.Item` (FilterValue, Title, Description). Include fields for `Flag` (string), `Description` (string), and `IsBaseline` (bool). Populate a slice of these items with common flags like `--debug`, `--json`, `--no-color`. The constructor should accept a `baseline []string` argument to mark matching flags as `IsBaseline = true`.",
            "status": "pending",
            "testStrategy": "Unit test to ensure baseline flags passed to the constructor are correctly marked as `IsBaseline` in the generated items list.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement Multi-selection Toggling Logic",
            "description": "Add logic to handle key events for selecting and deselecting flags within the list.",
            "dependencies": [
              2
            ],
            "details": "In the `Update` method, intercept the `Space` key event. If the currently selected item is NOT a baseline flag (immutable), toggle a boolean state indicating selection. Maintain a map or slice within the model to track which non-baseline flags are currently active.",
            "status": "pending",
            "testStrategy": "Unit test the Update loop: send a KeyMsg(Space) and assert the item's selection state changes appropriately.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Customize Delegate for Visual Status",
            "description": "Create a custom list delegate to visually distinguish selected, unselected, and baseline flags.",
            "dependencies": [
              3
            ],
            "details": "Implement a custom `list.ItemDelegate`. The `Render` method should display a checkbox `[x]` for selected items and `[ ]` for unselected ones. For baseline flags, use a distinct visual indicator (e.g., a lock icon or dimmed color) to show they are active but immutable.",
            "status": "pending",
            "testStrategy": "Manual visual verification to ensure selected items and locked baseline items are rendered with distinct styles.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Connect to RuntimeOverrides Model",
            "description": "Implement the state export logic to update the shared RuntimeOverrides struct.",
            "dependencies": [
              3
            ],
            "details": "Add a method `GetSelectedFlags() []string` or a message handler that triggers on confirmation (Enter key). This logic should filter the list for all selected override flags (excluding baseline ones, or managing them based on the `RuntimeOverrides` requirements) and prepare them for the shared state model.",
            "status": "pending",
            "testStrategy": "Unit test calling the export method after modifying selection state to verify it returns the correct list of strings.",
            "parentId": "undefined"
          }
        ],
        "complexity": 4,
        "recommendedSubtasks": 0,
        "expansionPrompt": "No expansion needed.",
        "updatedAt": "2026-01-02T18:22:11.000Z"
      },
      {
        "id": 7,
        "title": "Develop Step Targeting and URL Input UI",
        "description": "Build TUI components for selecting target steps and entering the ChatGPT Project URL.",
        "details": "Create `internal/tui/overrides_steps.go` (list of steps from the pack, multi-select) and `internal/tui/overrides_url.go` (text input using `bubbles/textinput`). The step picker defaults to 'all selected'. The URL input validates simple URL format. Both update the shared `RuntimeOverrides` state.\n\n[2026-01-02] Added steps picker and URL input components (internal/tui/overrides_steps.go, internal/tui/overrides_url.go) and integrated into overrides flow with basic validation and navigation. Updated flow constructor to accept pack steps. go test ./internal/tui passes.",
        "testStrategy": "Unit tests for the component models. Verify text input validation and list selection state persistence.",
        "priority": "medium",
        "dependencies": [
          "5"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define TUI Model State for URL and Step Inputs",
            "description": "Extend the main TUI model to include state fields for the URL input and step selection components.",
            "dependencies": [],
            "details": "Modify the TUI model struct (likely in `internal/tui/model.go`) to hold a `bubbles/textinput` model for the URL and a list model for steps. Ensure there are methods or fields to pass the available steps from the execution pack into the TUI initialization phase.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement URL Input Component with Validation",
            "description": "Create the text input component for entering the ChatGPT Project URL, including basic format validation.",
            "dependencies": [
              1
            ],
            "details": "Create `internal/tui/overrides_url.go`. Initialize a `textinput.Model` with appropriate placeholder text. Implement a validation function that ensures the input starts with 'http://' or 'https://'. Expose an Update/View method compatible with the Bubble Tea runtime.",
            "status": "pending",
            "testStrategy": "Unit test the validation logic in `overrides_url_test.go` to ensure malformed URLs are flagged.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement Step Selection List Component",
            "description": "Build the multi-select list component to toggle which steps the overrides apply to.",
            "dependencies": [
              1
            ],
            "details": "Create `internal/tui/overrides_steps.go`. Use a custom model or `bubbles/list` to display steps. Maintain a `map[string]bool` or slice for selection state. Implement logic to default all steps to 'selected' upon initialization. Include visual indicators (e.g., [x] vs [ ]) for selection status.",
            "status": "pending",
            "testStrategy": "Unit test the toggle logic ensuring the underlying selection map updates correctly when items are selected/deselected.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Integrate Components into Main TUI Update Loop",
            "description": "Wire the new components into the main application loop to handle focus and state updates.",
            "dependencies": [
              2,
              3
            ],
            "details": "Update the main `Update` function to route key messages to the URL input or Step list when they are focused. Implement navigation logic (e.g., Tab to switch between inputs). Ensure that changes in these components propagate to the shared `RuntimeOverrides` data structure defined in Task 1.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Finalize View Rendering and Visual Testing",
            "description": "Combine the component views into the main UI layout and verify visual correctness.",
            "dependencies": [
              4
            ],
            "details": "Update the main `View` function to render the URL input and Step list alongside existing components. Apply styles to indicate focus. Perform manual verification to ensure the URL input accepts text and the step list toggles visually respond to user input.",
            "status": "pending",
            "testStrategy": "Manual verification by running the TUI and cycling through inputs.",
            "parentId": "undefined"
          }
        ],
        "complexity": 4,
        "recommendedSubtasks": 0,
        "expansionPrompt": "No expansion needed.",
        "updatedAt": "2026-01-02T18:24:41.000Z"
      },
      {
        "id": 8,
        "title": "Implement Confirmation Screen with Validation Trigger",
        "description": "Build the final summary screen that shows the diff and runs the validation logic before applying.",
        "details": "Create `internal/tui/overrides_confirm.go`. Display a summary of Added/Removed flags, URL, and count of targeted steps. On confirm key press (e.g., 'Enter'), trigger the validation runner (Task 4) via a `Cmd`. If validation fails, display the error (Step ID + Output) and allow retry. If success, return `OverridesAppliedMsg`.\n\n[2026-01-02] Added confirmation view and validation flow (internal/tui/overrides_confirm.go) with validation cmd, error handling, and OverridesAppliedMsg payload. Integrated confirm step into overrides flow and stored applied overrides in main model. go test ./internal/tui passes.",
        "testStrategy": "Mock the validation runner in tests to simulate pass/fail scenarios and verify the UI response (error message vs state transition).",
        "priority": "high",
        "dependencies": [
          "4",
          "5",
          "6",
          "7"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Overrides Confirmation View",
            "description": "Implement the read-only UI component that summarizes the proposed changes before execution.",
            "dependencies": [],
            "details": "Create `internal/tui/overrides_confirm.go` and define the model struct. Implement the `View()` method to render a clear summary of the overrides: list added/removed flags (using color coding for diffs), show the target URL, and display the count of targeted steps. Ensure the layout clearly separates configuration data from the 'Press Enter to Confirm' prompt.",
            "status": "pending",
            "testStrategy": "Visual verification or snapshot testing of the View output using a Bubble Tea test harness with mock override data.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Validation Command and State Management",
            "description": "Add the asynchronous logic to trigger validation on confirmation and handle the resulting states.",
            "dependencies": [
              1
            ],
            "details": "Update `internal/tui/overrides_confirm.go` to handle the 'Enter' key event. Return a `tea.Cmd` that executes the validation runner (Task 4). Implement a loading state (e.g., a spinner) in the view while validation is pending. Process the resulting message: if validation fails, display the specific error (Step ID + Output) and allow the user to retry; if successful, return `OverridesAppliedMsg` to proceed.",
            "status": "pending",
            "testStrategy": "Unit tests for the `Update` function, mocking the validation command response to verify transitions between loading, error, and success states.",
            "parentId": "undefined"
          }
        ],
        "complexity": 6,
        "recommendedSubtasks": 2,
        "expansionPrompt": "Divide into: 1. Building the read-only summary view (Diff UI) that renders the proposed changes clearly. 2. Implementing the async command triggering for validation, handling the loading state (spinner), and processing the success/failure messages to either block or allow the final apply action.",
        "updatedAt": "2026-01-02T18:27:40.000Z"
      },
      {
        "id": 9,
        "title": "Integrate Overrides into Execution Engine",
        "description": "Wire the runtime overrides into the actual step execution logic in the runner.",
        "details": "Modify `internal/app/app.go` or `internal/runner/runner.go` (where `RunStep` is called). The runner needs access to the `RuntimeOverrides` object. Inside the execution loop, before running a step script, call `overrides.EffectiveFlags(step.ID, baseline)`. Pass these effective flags to `InjectFlags` (Task 3). Ensure non-targeted steps run as normal.\n\n[2026-01-02] Integrated RuntimeOverrides into exec.Runner and RunStep (effective flags per step). Wired OverridesAppliedMsg to set runner overrides in TUI. go test ./internal/exec ./internal/tui passes.",
        "testStrategy": "E2E/Integration test: Run a dummy pack with overrides enabled. Verify via logs or output that the `oracle` command received the extra flags only on targeted steps.",
        "priority": "high",
        "dependencies": [
          "3",
          "8"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Update Runner Configuration to Support Runtime Overrides",
            "description": "Refactor the runner struct and initialization functions to accept and store the RuntimeOverrides configuration object.",
            "dependencies": [],
            "details": "Modify `internal/runner/runner.go` to add a field `Overrides *overrides.RuntimeOverrides` to the Runner struct. Update the `NewRunner` constructor and the call site in `internal/app/app.go` to pass this configuration object down from the application context. Ensure the runner handles nil overrides gracefully.",
            "status": "pending",
            "testStrategy": "Unit test ensuring the runner initializes correctly with and without overrides provided.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Effective Flag Resolution in Execution Loop",
            "description": "Update the step iteration loop to calculate effective flags for the current step based on the overrides configuration.",
            "dependencies": [
              1
            ],
            "details": "Inside the main execution loop in `internal/runner/runner.go`, locate the point before a step is executed. Insert logic to call `r.Overrides.EffectiveFlags(step.ID, baselineFlags)` using the logic from Task 1. This prepares the specific flags needed for the current step ID.",
            "status": "pending",
            "testStrategy": "Unit test mocking the Overrides object to verify `EffectiveFlags` is called with the correct Step ID during iteration.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Apply Flag Injection and Execute Modified Command",
            "description": "Integrate the flag injection utility to modify the command string with the resolved effective flags and ensure correct execution.",
            "dependencies": [
              2
            ],
            "details": "Take the resolved flags from the previous subtask and pass them to `exec.InjectFlags` (from Task 3) along with the original step script. Replace the command execution payload with this modified string. Add logging to indicate when a command has been altered by overrides.",
            "status": "pending",
            "testStrategy": "Integration test using a mock executor to verify that the final command string passed to the shell contains the injected flags.",
            "parentId": "undefined"
          }
        ],
        "complexity": 8,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down into: 1. Refactoring the Runner signature/struct to accept the `RuntimeOverrides` configuration. 2. Modifying the step execution loop to resolve `EffectiveFlags` dynamically for every step. 3. Integrating the injection logic (Task 3) safely into the command preparation phase, ensuring logging and error handling cover the modified commands.",
        "updatedAt": "2026-01-02T18:29:02.000Z"
      },
      {
        "id": 10,
        "title": "Add Main Screen Indicators and Final Polish",
        "description": "Update the main run screen to indicate when overrides are active to prevent user confusion.",
        "details": "In `internal/tui/tui.go`, checking `OverridesAppliedMsg` to update the main model state. Render a small status bar or badge (e.g., \"Overrides Active: +2 flags\") on the main view. Ensure the keybinding to open the overrides flow is visible in the help footer.\n\n[2026-01-02] Added overrides status indicator in main view, updated help text, and cleared overrides on cancel/reset. go test ./internal/tui passes.",
        "testStrategy": "Visual verification. Ensure the indicator appears after applying overrides and disappears if they are cleared/reset.",
        "priority": "low",
        "dependencies": [
          "5",
          "9"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Update Main Model to Store Override State",
            "description": "Modify the main application model to persist the configuration of applied overrides received from the wizard flow.",
            "dependencies": [],
            "details": "In `internal/tui/tui.go`, update the main `Model` struct to include fields for active overrides (e.g., `overrideSummary string` or `activeOverrideCount int`). Implement the `Update` method case for `OverridesAppliedMsg` to populate these fields based on the message payload.",
            "status": "pending",
            "testStrategy": "Unit test the `Update` function to ensure state changes correctly upon receiving `OverridesAppliedMsg`.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Overrides Active Badge Component",
            "description": "Create a styled UI component that renders a visual indicator when overrides are active to alert the user.",
            "dependencies": [
              1
            ],
            "details": "Define a `lipgloss` style for a status badge (e.g., yellow text or background). In the main `View` function, conditionally render this badge (e.g., \"Overrides Active: +2 flags\") only if the model's override state is non-empty/non-zero.",
            "status": "pending",
            "testStrategy": "Visual verification using a Golden file test or manual inspection to ensure the badge appears only when state is set.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Update Keybindings and Help Footer",
            "description": "Ensure the keybinding to open the overrides configuration is registered and visible in the help bubble.",
            "dependencies": [],
            "details": "Update the `KeyMap` in `internal/tui/keys.go` (or equivalent) to include the Overrides trigger (e.g., 'o'). Add this key to the `ShortHelp` and `FullHelp` return values so the Bubble Tea help component renders it in the footer.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Handle Overrides Reset and Cancellation",
            "description": "Implement logic to clear the visual indicator if the user cancels or resets the overrides during the session.",
            "dependencies": [
              1
            ],
            "details": "In `internal/tui/tui.go`, handle `OverridesCancelledMsg` or a specific clear signal. Ensure the model resets the override summary/count fields to zero/nil, causing the UI badge to disappear in the next render cycle.",
            "status": "pending",
            "testStrategy": "Unit test sending a cancellation/reset message and asserting the model state returns to empty.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Apply Final Visual Polish and Layout Integration",
            "description": "Fine-tune the layout and colors of the indicator to ensure it fits the theme and does not break existing UI elements.",
            "dependencies": [
              2,
              3
            ],
            "details": "Adjust margins and padding for the new badge component. Ensure it displays correctly in the header or status bar area without overlapping with the progress bar or log output view. Apply theme-consistent colors.",
            "status": "pending",
            "testStrategy": "Manual visual regression testing with various window sizes.",
            "parentId": "undefined"
          }
        ],
        "complexity": 2,
        "recommendedSubtasks": 0,
        "expansionPrompt": "No expansion needed.",
        "updatedAt": "2026-01-02T18:31:12.000Z"
      },
      {
        "id": 11,
        "title": "Post-implementation improvements",
        "description": "Capture follow-up improvements identified after overrides wizard work.",
        "details": "Improvements list:\\n- Add persistent help bar on main screen (outside viewport) for key shortcuts.\\n- Overrides confirm: show effective oracle flags + summarize validation errors (count/list).\\n- Clipboard fallback: write temp file on copy failure and notify.\\n- ROI filter persistence: save ROI mode/threshold on toggle + load on start.\\n- Add tests for URL picker, step preview, ROI persistence.\\n- Any other UI polish discovered during scan.",
        "testStrategy": "",
        "status": "done",
        "dependencies": [],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2026-01-02T22:30:18.523Z"
      },
      {
        "id": 12,
        "title": "Integrate Glow Markdown Rendering for TUI Viewport",
        "description": "Implement a width-aware Markdown rendering utility using Glamour and integrate it into the Bubble Tea viewport to display formatted content.",
        "details": "1. Create a new package `internal/render` and implement `RenderMarkdown(text string, width int, style string) (string, error)`. Use `github.com/charmbracelet/glamour` as the underlying engine. \n2. Configure Glamour to use a standard terminal style (e.g., 'dark' or 'light' based on TUI defaults) while ensuring the `glamour.WithWordWrap(width)` option is utilized to handle the viewport's dynamic sizing. \n3. In `internal/tui/tui.go`, modify the `Update` function to trigger a re-render of the current selection's description whenever a `tea.WindowSizeMsg` is received or the cursor selection changes. \n4. Update the viewport content using `viewport.SetContent()` with the output from the `render` package. \n5. Ensure the renderer is strictly a library call; do not invoke the Glow binary or TUI program directly. \n6. Handle potential rendering errors by falling back to raw text display to ensure the TUI remains functional.",
        "testStrategy": "1. Unit Tests: Create `internal/render/markdown_test.go` to verify that `RenderMarkdown` correctly wraps text at specific widths and handles various Markdown elements (headers, lists, code blocks). \n2. Manual TUI Verification: Run the application and navigate between steps. Verify that Markdown styling is applied. \n3. Resize Testing: Drag the terminal window to different sizes and ensure the text in the viewport re-wraps and re-renders correctly without clipping. \n4. Edge Case: Provide empty strings or malformed Markdown to the renderer and verify the system handles it gracefully.",
        "status": "done",
        "dependencies": [
          "5",
          "10",
          "11"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2026-01-04T09:14:17.791Z"
      }
    ],
    "metadata": {
      "version": "1.0.0",
      "lastModified": "2026-01-04T09:14:17.792Z",
      "taskCount": 12,
      "completedCount": 12,
      "tags": [
        "bugfix-tui"
      ],
      "created": "2026-01-08T01:17:51.784Z",
      "description": "Tasks for bugfix-tui context",
      "updated": "2026-01-08T01:17:51.784Z"
    }
  },
  "feat-mcp": {
    "tasks": [
      {
        "id": 1,
        "title": "Initialize Project Structure and Environment Configuration",
        "description": "Set up the oraclepack-mcp-server directory structure and implement the configuration loader for environment variables.",
        "details": "Create the project structure: `oraclepack_mcp_server/` package, `requirements.txt`, and `README.md`. Implement `config.py` using `pydantic-settings` or a similar pattern to load: `ORACLEPACK_BIN`, `ORACLEPACK_ALLOWED_ROOTS` (comma-separated list), `ORACLEPACK_ENABLE_EXEC` (bool), `ORACLEPACK_MAX_READ_BYTES` (default 64KB), and `ORACLEPACK_MAX_OUTPUT_CHARS` (default 32000). Ensure the config loader handles defaults according to the PRD.",
        "testStrategy": "Unit test the configuration loader to verify environment variables are correctly mapped to internal settings and defaults are applied when variables are missing.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Scaffold Project Directory and Metadata Files",
            "description": "Initialize the base project structure including the main package directory and initial README.",
            "dependencies": [],
            "details": "Create the root directory 'oraclepack-mcp-server', the python package directory 'oraclepack_mcp_server/', an empty '__init__.py', and a 'README.md' documenting the purpose of the server.",
            "status": "done",
            "testStrategy": "Verify directory structure exists using 'ls -R' and ensure README.md is present.",
            "updatedAt": "2026-01-08T01:25:12.977Z",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Define Project Dependencies and Requirements",
            "description": "Create the requirements.txt file with all necessary libraries for the MCP server and configuration management.",
            "dependencies": [
              1
            ],
            "details": "Include 'pydantic-settings', 'mcp', and 'pytest' in 'requirements.txt'. Ensure versions are pinned or compatible with the target environment.",
            "status": "done",
            "testStrategy": "Run 'pip install -r requirements.txt' in a virtual environment to ensure all dependencies resolve without conflict.",
            "parentId": "undefined",
            "updatedAt": "2026-01-08T01:25:19.087Z"
          },
          {
            "id": 3,
            "title": "Implement Base Configuration Model with Pydantic",
            "description": "Create oraclepack_mcp_server/config.py using pydantic-settings to manage environment variables.",
            "dependencies": [
              2
            ],
            "details": "Define a 'Settings' class that inherits from 'BaseSettings'. Map the variables: ORACLEPACK_BIN, ORACLEPACK_ALLOWED_ROOTS, ORACLEPACK_ENABLE_EXEC, ORACLEPACK_MAX_READ_BYTES, and ORACLEPACK_MAX_OUTPUT_CHARS.",
            "status": "done",
            "testStrategy": "Instantiate the Settings class in a script and print the default values to ensure they match the PRD requirements.",
            "parentId": "undefined",
            "updatedAt": "2026-01-08T01:25:36.703Z"
          },
          {
            "id": 4,
            "title": "Configure Type Parsing and Default Values",
            "description": "Implement specific parsing logic for list and boolean environment variables in config.py.",
            "dependencies": [
              3
            ],
            "details": "Ensure ORACLEPACK_ALLOWED_ROOTS handles comma-separated strings into a list of strings/paths. Set defaults: MAX_READ_BYTES to 65536 and MAX_OUTPUT_CHARS to 32000. Set 'env_prefix' to '' if needed.",
            "status": "done",
            "testStrategy": "Manually set environment variables like 'ORACLEPACK_ALLOWED_ROOTS=/tmp,/var' and verify the loaded model contains a list of two items.",
            "parentId": "undefined",
            "updatedAt": "2026-01-08T01:25:36.708Z"
          },
          {
            "id": 5,
            "title": "Develop Configuration Unit Test Suite",
            "description": "Create a test suite to validate the environment configuration loader.",
            "dependencies": [
              4
            ],
            "details": "Create 'tests/test_config.py'. Write tests to check default value assignment and ensure that providing environment variables correctly overrides the defaults. Test the parsing of the comma-separated roots.",
            "status": "done",
            "testStrategy": "Run 'pytest tests/test_config.py' and ensure all assertions for value mapping and type conversion pass.",
            "parentId": "undefined",
            "updatedAt": "2026-01-08T01:26:18.788Z"
          }
        ],
        "updatedAt": "2026-01-08T01:26:18.788Z"
      },
      {
        "id": 2,
        "title": "Implement Security and Path Resolution Service",
        "description": "Develop the security module to handle path validation and execution gating.",
        "details": "Implement `security.py`. Create a `validate_path` function that resolves paths and ensures they reside within `ORACLEPACK_ALLOWED_ROOTS` using `pathlib.Path.resolve()` and `os.path.commonpath`. Implement an `is_exec_enabled` check that strictly evaluates `ORACLEPACK_ENABLE_EXEC`. Logic must reject any path escaping the allowed roots with a clear error.",
        "testStrategy": "Test `validate_path` with relative paths, symlinks (if applicable), and paths outside the allowed roots. Verify that `is_exec_enabled` reflects the boolean state of the environment flag.",
        "priority": "high",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Initialize security module and configuration loading",
            "description": "Create the security.py module and implement configuration retrieval for security settings.",
            "dependencies": [],
            "details": "Create 'src/oraclepack/security.py'. Implement logic to parse 'ORACLEPACK_ALLOWED_ROOTS' from the environment as a list of absolute paths. Ensure 'ORACLEPACK_ENABLE_EXEC' is correctly interpreted as a boolean value.",
            "status": "done",
            "testStrategy": "Verify that environment variables are correctly parsed into internal constants within the security module.",
            "updatedAt": "2026-01-08T01:27:27.798Z",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement execution gating logic",
            "description": "Develop the is_exec_enabled function to control access to execution features.",
            "dependencies": [
              1
            ],
            "details": "Implement 'is_exec_enabled()' in 'security.py'. This function should return the boolean state of 'ORACLEPACK_ENABLE_EXEC'. It must strictly evaluate the setting to prevent unauthorized subprocess execution.",
            "status": "done",
            "testStrategy": "Unit test is_exec_enabled with various environment configurations (True, False, unset).",
            "parentId": "undefined",
            "updatedAt": "2026-01-08T01:27:27.803Z"
          },
          {
            "id": 3,
            "title": "Implement secure path resolution and validation",
            "description": "Develop the validate_path function using pathlib and os.path.commonpath for root enforcement.",
            "dependencies": [
              1
            ],
            "details": "Implement 'validate_path(user_path: str) -> Path'. Use 'pathlib.Path(user_path).resolve()' to obtain the absolute path. Compare this resolved path against the list of 'ORACLEPACK_ALLOWED_ROOTS' using 'os.path.commonpath' to ensure no path traversal or root escapes occur.",
            "status": "done",
            "testStrategy": "Test with valid internal paths, relative paths, and paths attempting traversal (e.g., ../../etc/passwd).",
            "parentId": "undefined",
            "updatedAt": "2026-01-08T01:27:27.808Z"
          },
          {
            "id": 4,
            "title": "Implement security exception handling",
            "description": "Define and integrate custom security exceptions for path and execution violations.",
            "dependencies": [
              3
            ],
            "details": "Define a 'SecurityError' exception class. Update 'validate_path' to raise this exception with a clear, descriptive message when a path is found outside the allowed roots. Ensure messages do not leak sensitive system structure.",
            "status": "done",
            "testStrategy": "Assert that SecurityError is raised when providing paths outside the defined ORACLEPACK_ALLOWED_ROOTS.",
            "parentId": "undefined",
            "updatedAt": "2026-01-08T01:27:27.812Z"
          },
          {
            "id": 5,
            "title": "Create comprehensive security test suite",
            "description": "Develop a dedicated test file to verify all security constraints and edge cases.",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Create 'tests/test_security.py'. Include test cases for symlink resolution, multiple allowed roots, empty root lists, and the interaction between relative paths and the current working directory in 'validate_path'.",
            "status": "done",
            "testStrategy": "Execute pytest on the new test file and ensure 100% coverage of security logic branches.",
            "parentId": "undefined",
            "updatedAt": "2026-01-08T01:27:27.816Z"
          }
        ],
        "updatedAt": "2026-01-08T01:27:27.816Z"
      },
      {
        "id": 3,
        "title": "Develop Oraclepack CLI Async Subprocess Wrapper",
        "description": "Create a robust wrapper for calling the oraclepack CLI with timeout and truncation handling.",
        "details": "Implement `oraclepack_cli.py`. Use `asyncio.create_subprocess_exec` to run the `oraclepack` binary. Capture stdout/stderr. Implement character-based truncation logic for tool outputs based on `ORACLEPACK_MAX_OUTPUT_CHARS`. Handle timeouts by returning exit code 124. The runner should return a structured object: `{ok, exit_code, duration_s, stdout, stderr, stdout_truncated, stderr_truncated}`.",
        "testStrategy": "Mock the subprocess call to test truncation logic, timeout handling (using `asyncio.wait_for`), and successful capture of exit codes/output.",
        "priority": "high",
        "dependencies": [
          "1",
          "2"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define OraclepackResult Dataclass and Config Constants",
            "description": "Create the data structure for the CLI output and initialize configuration from environment variables.",
            "dependencies": [],
            "details": "In oraclepack_cli.py, define a dataclass 'OraclepackResult' with fields: ok (bool), exit_code (int), duration_s (float), stdout (str), stderr (str), stdout_truncated (bool), and stderr_truncated (bool). Load 'ORACLEPACK_MAX_OUTPUT_CHARS' from the environment with a sensible default.",
            "status": "done",
            "testStrategy": "Verify that the dataclass can be instantiated and environment variables are correctly read using unit tests.",
            "updatedAt": "2026-01-08T01:28:32.443Z",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Output Truncation Logic",
            "description": "Develop a helper function to truncate stdout/stderr based on character limits.",
            "dependencies": [
              1
            ],
            "details": "Implement a helper function that takes a string and the max character limit. It should return a tuple of (truncated_string, was_truncated_bool). This logic will be applied to both stdout and stderr independently to ensure compliance with ORACLEPACK_MAX_OUTPUT_CHARS.",
            "status": "done",
            "testStrategy": "Unit test the truncation function with strings smaller than, equal to, and larger than the character limit.",
            "parentId": "undefined",
            "updatedAt": "2026-01-08T01:28:32.448Z"
          },
          {
            "id": 3,
            "title": "Implement Async Subprocess Execution Core",
            "description": "Create the core runner using asyncio.create_subprocess_exec.",
            "dependencies": [
              1,
              2
            ],
            "details": "Implement the async function 'run_oraclepack_cli' that accepts a list of arguments. Use 'asyncio.create_subprocess_exec' with 'stdout=asyncio.subprocess.PIPE' and 'stderr=asyncio.subprocess.PIPE'. Capture the raw bytes and decode them using UTF-8 (handling errors gracefully).",
            "status": "done",
            "testStrategy": "Execute a simple command like 'echo' and verify that output is captured and decoded correctly.",
            "parentId": "undefined",
            "updatedAt": "2026-01-08T01:28:32.452Z"
          },
          {
            "id": 4,
            "title": "Add Timeout Handling and Execution Timing",
            "description": "Integrate duration tracking and timeout logic using asyncio.wait_for.",
            "dependencies": [
              3
            ],
            "details": "Wrap the subprocess execution in 'asyncio.wait_for' using a configurable timeout. Use 'time.perf_counter()' to calculate 'duration_s'. If a 'TimeoutError' occurs, ensure the process is terminated and return an 'OraclepackResult' with 'exit_code' 124 and 'ok' set to False.",
            "status": "done",
            "testStrategy": "Test with a mock subprocess that sleeps longer than the timeout to verify 'exit_code' 124 and proper cleanup.",
            "parentId": "undefined",
            "updatedAt": "2026-01-08T01:28:32.456Z"
          },
          {
            "id": 5,
            "title": "Integrate Result Construction and Error Mapping",
            "description": "Finalize the 'run_oraclepack_cli' function to return the structured result object.",
            "dependencies": [
              4
            ],
            "details": "Combine the captured exit code, timed duration, and truncated outputs into the 'OraclepackResult' object. Ensure 'ok' is True only if 'exit_code' is 0. Ensure all exceptions (like FileNotFoundError if the binary is missing) are caught and mapped to a failed 'OraclepackResult'.",
            "status": "done",
            "testStrategy": "Comprehensive unit tests using 'unittest.mock' to simulate various exit codes and output sizes.",
            "parentId": "undefined",
            "updatedAt": "2026-01-08T01:28:32.459Z"
          }
        ],
        "updatedAt": "2026-01-08T01:28:32.459Z"
      },
      {
        "id": 4,
        "title": "Implement Action Pack Validation and Detection Logic",
        "description": "Implement the logic for Stage-2 validation and directory/file detection for Action Packs.",
        "details": "Develop validation logic to enforce exactly one file per prefix `01..20`. Implement detection for 'auto', explicit directory, and explicit file modes. Validation must check for the 'single bash fence' constraint and valid step headers in markdown files. If invalid, return a dictionary containing `missing` and `ambiguous` sets for the prefixes.",
        "testStrategy": "Create a test suite with various directory structures: missing prefixes, multiple files with same prefix, invalid bash fences, and correct structures to verify deterministic results.",
        "priority": "medium",
        "dependencies": [
          "3"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Validation Result Schemas and Constants",
            "description": "Establish the data structures and constants required for Stage-2 validation, including prefix ranges and error reporting structures.",
            "dependencies": [],
            "details": "Create or update a module (e.g., `src/models.py` or `src/validator.py`) to define a `ValidationResult` object. This must include fields for 'missing' and 'ambiguous' sets to track prefixes 01 through 20. Define constants for the supported prefix regex and range limits.",
            "status": "done",
            "testStrategy": "Unit tests to verify that the ValidationResult object correctly handles set operations and serialization.",
            "updatedAt": "2026-01-08T01:29:43.273Z",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Action Pack Path Detection Logic",
            "description": "Develop the logic to resolve Action Pack locations based on 'auto', explicit directory, and explicit file modes.",
            "dependencies": [
              1
            ],
            "details": "Implement a resolver that: 1. In 'auto' mode, searches the current and parent directories for valid Action Pack structures. 2. In 'directory' mode, validates the contents of a specific path. 3. In 'file' mode, treats a single file as an Action Pack. Ensure integration with the security module's path validation.",
            "status": "done",
            "testStrategy": "Test with various filesystem layouts: single files, nested directories, and missing paths to ensure correct mode selection.",
            "parentId": "undefined",
            "updatedAt": "2026-01-08T01:29:43.278Z"
          },
          {
            "id": 3,
            "title": "Implement Prefix Collision and Gap Detection",
            "description": "Create the scanner that enforces the 'exactly one file per prefix' rule for prefixes 01..20.",
            "dependencies": [
              2
            ],
            "details": "Implement logic to iterate through files in a detected directory, extract numerical prefixes (01-20), and group them. Return prefixes with multiple file matches in the 'ambiguous' set and prefixes with zero matches in the 'missing' set. Ignore files that do not follow the prefix convention.",
            "status": "done",
            "testStrategy": "Provide directories with duplicate prefixes (e.g., 01-a.md and 01-b.md) and skipped prefixes (e.g., 01, 03) to verify set populations.",
            "parentId": "undefined",
            "updatedAt": "2026-01-08T01:29:43.282Z"
          },
          {
            "id": 4,
            "title": "Implement Markdown Content and Step Header Validation",
            "description": "Develop the parser to enforce the 'single bash fence' and valid header constraints within markdown files.",
            "dependencies": [
              3
            ],
            "details": "For each file in the Action Pack, parse the markdown content to ensure it contains exactly one triple-backtick bash code block. Additionally, verify that the file begins with a valid step header (e.g., '# Step XX: Title'). Return specific errors if multiple fences or invalid headers are found.",
            "status": "done",
            "testStrategy": "Unit tests using raw strings of markdown representing valid, multiple-fence, zero-fence, and malformed header scenarios.",
            "parentId": "undefined",
            "updatedAt": "2026-01-08T01:29:43.286Z"
          },
          {
            "id": 5,
            "title": "Integrate Validation Engine and Execute Comprehensive Test Suite",
            "description": "Combine detection, prefix, and content validation into a single workflow and verify against the requirements.",
            "dependencies": [
              4
            ],
            "details": "Expose a main `validate_action_pack` function that orchestrates the previous steps. Implement a full test suite as described in the parent task, covering all edge cases (missing prefixes, ambiguity, invalid fences) to ensure deterministic results as required by the spec.",
            "status": "done",
            "testStrategy": "Integration tests using temporary directories and files to simulate real-world Action Pack structures and verify the final dictionary output.",
            "parentId": "undefined",
            "updatedAt": "2026-01-08T01:29:43.291Z"
          }
        ],
        "updatedAt": "2026-01-08T01:29:43.291Z"
      },
      {
        "id": 5,
        "title": "Implement MCP Server Core and Transports",
        "description": "Set up the MCP server using the Python SDK, supporting both stdio and streamable-http transports.",
        "details": "Implement `server.py` and `__main__.py` using the `mcp` Python library (Anthropic). Support `--transport stdio` and `--transport streamable-http`. For HTTP, implement Origin validation and localhost binding with basic authentication as specified. Ensure `stdio` mode does not interleave logs on stdout by redirecting application logs to stderr.",
        "testStrategy": "Start the server in both modes. Use an MCP inspector or client to verify connection. Test HTTP origin validation by sending requests with unauthorized Origin headers.",
        "priority": "high",
        "dependencies": [
          "1",
          "2",
          "3"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Initialize MCP Server and Configure Stderr Logging",
            "description": "Create the core MCP server instance and set up logging to stderr.",
            "dependencies": [],
            "details": "Initialize `mcp.server.Server` in `oraclepack/server.py`. Configure the Python logging module to direct all logs to `sys.stderr` to prevent interference with the `stdio` transport protocol on `stdout`. Set the server name and version metadata.",
            "status": "done",
            "testStrategy": "Verify that logging output appears in the terminal's stderr while stdout remains empty or reserved for MCP frames.",
            "updatedAt": "2026-01-08T01:31:52.674Z",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement CLI Argument Parsing in __main__.py",
            "description": "Develop the entry point for the MCP server with support for transport selection.",
            "dependencies": [
              1
            ],
            "details": "Use `argparse` to implement CLI flags: `--transport` (choices: `stdio`, `streamable-http`), `--host`, `--port`, `--auth-token`, and `--allowed-origins`. Ensure the script correctly routes execution to the selected transport handler.",
            "status": "done",
            "testStrategy": "Run the module with various flag combinations and verify correct argument parsing using print/log statements.",
            "parentId": "undefined",
            "updatedAt": "2026-01-08T01:31:52.679Z"
          },
          {
            "id": 3,
            "title": "Implement Stdio Transport Runner",
            "description": "Set up the asynchronous loop for handling MCP communication over standard input/output.",
            "dependencies": [
              1,
              2
            ],
            "details": "Implement the logic to run the server using `mcp.server.stdio.stdio_server`. Ensure that the server handles the initialization handshake and gracefully shuts down on EOF or SIGINT.",
            "status": "done",
            "testStrategy": "Use the MCP Inspector or a simple Python client to connect via stdio and verify the 'initialize' request-response cycle.",
            "parentId": "undefined",
            "updatedAt": "2026-01-08T01:31:52.684Z"
          },
          {
            "id": 4,
            "title": "Implement Streamable-HTTP Transport with Security Controls",
            "description": "Develop the HTTP transport layer with Origin validation and basic authentication.",
            "dependencies": [
              1,
              2
            ],
            "details": "Implement an SSE-based transport (Server-Sent Events) for the MCP server. Add middleware or logic to validate the 'Origin' header against `--allowed-origins`. Enforce localhost binding by default and implement basic token-based authentication for incoming HTTP connections.",
            "status": "done",
            "testStrategy": "Attempt to connect via HTTP with unauthorized Origin headers or missing auth tokens and verify 403/401 responses. Verify successful connection from allowed origins.",
            "parentId": "undefined",
            "updatedAt": "2026-01-08T01:31:52.689Z"
          },
          {
            "id": 5,
            "title": "Register Tools and Integrate Oraclepack Business Logic",
            "description": "Expose the CLI wrapper and validation logic as MCP tools.",
            "dependencies": [
              3,
              4
            ],
            "details": "Register tools on the server instance using `@server.list_tools()` and `@server.call_tool()`. Map MCP tool calls to the `oraclepack_cli` wrapper (Task 3) and validation services (Task 4). Ensure output truncation and security path checks are applied to all tool executions.",
            "status": "done",
            "testStrategy": "Invoke registered tools via an MCP client and verify that the output matches expected results from the oraclepack CLI, including proper error handling for invalid paths.",
            "parentId": "undefined",
            "updatedAt": "2026-01-08T01:31:52.694Z"
          }
        ],
        "updatedAt": "2026-01-08T01:31:52.694Z"
      },
      {
        "id": 6,
        "title": "Expose Read-Only MCP Tools",
        "description": "Register and implement the read-only tools: oraclepack_list_packs, oraclepack_read_file, and oraclepack_validate.",
        "details": "Annotate tools as 'read-only' in MCP metadata. `oraclepack_read_file` must enforce `ORACLEPACK_MAX_READ_BYTES` and `ORACLEPACK_ALLOWED_ROOTS`. `oraclepack_validate` uses logic from Task 4. Implement response formatters that support Markdown and JSON output with truncation indicators.",
        "testStrategy": "Call each read-only tool via an MCP client. Verify that `oraclepack_read_file` rejects unauthorized paths and truncates large files.",
        "priority": "medium",
        "dependencies": [
          "4",
          "5"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-08T01:32:32.688Z"
      },
      {
        "id": 7,
        "title": "Expose Execution and Taskify MCP Tools",
        "description": "Register and implement destructive tools: oraclepack_run_pack and oraclepack_taskify_run_action_pack.",
        "details": "Annotate tools as destructive/open-world. These tools MUST check `ORACLEPACK_ENABLE_EXEC` before proceeding. Implement `taskify.py` helpers to facilitate action pack execution. The tools should map CLI flags like `--no-tui`, `--out-dir`, and `--oracle-bin` from MCP arguments to the subprocess call.",
        "testStrategy": "Verify that execution fails with a clear error when `ORACLEPACK_ENABLE_EXEC=0`. When enabled, verify the subprocess receives the correct flags and returns the expected result payload.",
        "priority": "medium",
        "dependencies": [
          "4",
          "5"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-08T01:32:32.692Z"
      },
      {
        "id": 8,
        "title": "Implement Oraclepack Prompt Generator",
        "description": "Develop a utility to generate prompts for agents based on action pack structures.",
        "details": "Implement a helper within `taskify.py` or a new module that generates a system prompt or instruction block. This should describe how the agent should interact with the validated action pack and what artifacts to expect. Integrate this into the tool outputs where relevant.",
        "testStrategy": "Validate that the generated prompt correctly reflects the action pack's steps and requirements.",
        "priority": "low",
        "dependencies": [
          "4"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Prompt Templates for Action Pack Interaction",
            "description": "Create a collection of string templates that describe how an agent should handle Action Packs.",
            "dependencies": [],
            "details": "Establish constant string templates in a new 'prompts.py' module or within 'taskify.py'. These templates must explain the '01-20' prefix sequencing, the single-bash-fence requirement, and the handling of markdown headers.",
            "status": "done",
            "testStrategy": "Verify that templates contain placeholders for dynamic content like step names and pack descriptions.",
            "updatedAt": "2026-01-08T01:33:40.953Z",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement the Prompt Generation Logic",
            "description": "Develop a function to assemble the final system prompt based on validated Action Pack metadata.",
            "dependencies": [
              1
            ],
            "details": "Implement 'generate_action_pack_prompt(metadata: dict) -> str'. This function will take the output from the Task 4 validator (prefixes, file paths, step titles) and inject them into the templates to create a coherent instruction block.",
            "status": "done",
            "testStrategy": "Unit test with various mock metadata structures to ensure correct string interpolation and formatting.",
            "parentId": "undefined",
            "updatedAt": "2026-01-08T01:33:40.959Z"
          },
          {
            "id": 3,
            "title": "Integrate Prompt Generator into Taskify Workflow",
            "description": "Ensure the prompt is generated immediately after successful Action Pack validation.",
            "dependencies": [
              2
            ],
            "details": "Modify the main execution flow in 'taskify.py' to call the prompt generator once an Action Pack is identified and validated. Store the resulting string in the Action Pack state object.",
            "status": "done",
            "testStrategy": "Trace execution in 'taskify.py' to confirm the prompt is available in the final result object after validation.",
            "parentId": "undefined",
            "updatedAt": "2026-01-08T01:33:40.963Z"
          },
          {
            "id": 4,
            "title": "Expose Generated Prompt via MCP Tool Outputs",
            "description": "Update the MCP server tool responses to include the generated instructions for the calling agent.",
            "dependencies": [
              3
            ],
            "details": "Modify 'server.py' or the tool registration logic so that tools returning Action Pack information include the generated prompt as part of the 'description' or a dedicated 'instructions' field in the JSON output.",
            "status": "done",
            "testStrategy": "Use an MCP inspector or client to call the tool and verify the 'instructions' field is present and correctly populated.",
            "parentId": "undefined",
            "updatedAt": "2026-01-08T01:33:40.967Z"
          },
          {
            "id": 5,
            "title": "End-to-End Validation of Agent Instructions",
            "description": "Perform a full test to ensure the generated prompt effectively guides an agent through a sample Action Pack.",
            "dependencies": [
              4
            ],
            "details": "Create a complex sample Action Pack with 3+ steps. Run the generation logic and manually verify the prompt accurately describes the steps and constraints (bash fences, sequence).",
            "status": "done",
            "testStrategy": "Functional test using a real or mocked LLM agent to see if it follows the generated instructions for the sample pack.",
            "parentId": "undefined",
            "updatedAt": "2026-01-08T01:33:40.970Z"
          }
        ],
        "updatedAt": "2026-01-08T01:33:40.970Z"
      },
      {
        "id": 9,
        "title": "Verbose Payload Rendering and Artifact Summarizer",
        "description": "Implement the Verbose Payload Rendering TUI and the artifact summarizer.",
        "details": "Create a summarizer that reports the presence/absence of expected artifacts (e.g., reports, logs). For the TUI aspect (if consumed via MCP), ensure the JSON response includes a 'verbose_rendering' field containing a Markdown-formatted summary of the execution payload for easier reading by the agent/user.",
        "testStrategy": "Run a pack and check that the artifact summary accurately reflects the files created on disk. Verify the markdown rendering is valid and legible.",
        "priority": "low",
        "dependencies": [
          "7"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Artifact Summarizer Interface",
            "description": "Create a module to define expected artifacts and check their presence on the filesystem.",
            "dependencies": [],
            "details": "Implement an `ArtifactSummarizer` class that accepts a list of expected file patterns or paths. It should provide a `summarize()` method that returns a dictionary mapping file paths to their status (exists, missing) and basic metadata (size).",
            "status": "done",
            "testStrategy": "Unit test with mock filesystem to verify that the summarizer correctly identifies present and absent files based on patterns.",
            "updatedAt": "2026-01-08T01:34:30.654Z",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Markdown Rendering Utility",
            "description": "Develop a utility function to convert execution results into a formatted Markdown string.",
            "dependencies": [
              1
            ],
            "details": "Create a `render_verbose_summary` function that takes execution outputs, artifact status, and pack metadata to produce a structured Markdown document. Use tables for artifacts and code blocks for log snippets.",
            "status": "done",
            "testStrategy": "Verify the generated Markdown string against a schema and ensure it renders correctly in a standard Markdown viewer.",
            "parentId": "undefined",
            "updatedAt": "2026-01-08T01:34:30.658Z"
          },
          {
            "id": 3,
            "title": "Integrate Summarizer into Execution Flow",
            "description": "Update the core execution logic to trigger artifact summarization after a pack run.",
            "dependencies": [
              1
            ],
            "details": "Modify the main execution entry point (e.g., in `taskify.py` or `runner.py`) to call the `ArtifactSummarizer` once the process completes. Capture the results for inclusion in the final response payload.",
            "status": "done",
            "testStrategy": "Integration test: Run a sample pack and verify that the internal result object contains the summarized artifact information.",
            "parentId": "undefined",
            "updatedAt": "2026-01-08T01:34:30.663Z"
          },
          {
            "id": 4,
            "title": "Update MCP Server Response Payload",
            "description": "Modify the MCP server tool handlers to include the 'verbose_rendering' field.",
            "dependencies": [
              2,
              3
            ],
            "details": "Update the tool execution logic in `server.py` to include the 'verbose_rendering' key in the JSON response. This field should contain the Markdown output from the rendering utility.",
            "status": "done",
            "testStrategy": "Use an MCP client to call a tool and verify that the JSON response includes a valid 'verbose_rendering' Markdown string.",
            "parentId": "undefined",
            "updatedAt": "2026-01-08T01:34:30.666Z"
          },
          {
            "id": 5,
            "title": "Implement File-Based Log Summarization",
            "description": "Add logic to extract and summarize tail-end logs or specific report contents into the verbose rendering.",
            "dependencies": [
              2,
              4
            ],
            "details": "Enhance the summarizer to read the last N lines of log files or extract key metrics from report files (like JSON summaries) and inject them into the Markdown rendering for immediate visibility.",
            "status": "done",
            "testStrategy": "Run a pack that generates a log file; verify that the 'verbose_rendering' includes the expected log tail or report summary.",
            "parentId": "undefined",
            "updatedAt": "2026-01-08T01:34:30.671Z"
          }
        ],
        "updatedAt": "2026-01-08T01:34:30.671Z"
      },
      {
        "id": 10,
        "title": "Final Integration and MCP Compliance Testing",
        "description": "Perform end-to-end testing of the MCP server to ensure all derived actions (A01-A26) are satisfied.",
        "details": "Conduct a full pass of the derived actions. Verify deterministic outputs, truncation flags, timeout exit codes, and prefix validation logic. Ensure the repository includes final documentation in README.md explaining how to connect an agent (like Claude Desktop) to the MCP server.",
        "testStrategy": "Automated integration test suite that spins up the server and uses an MCP client to execute a battery of tool calls against a controlled filesystem environment.",
        "priority": "high",
        "dependencies": [
          "1",
          "2",
          "3",
          "4",
          "5",
          "6",
          "7",
          "8",
          "9"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Establish E2E Integration Test Harness",
            "description": "Create a robust integration test suite that spawns the MCP server as a subprocess and interacts with it using a client mock.",
            "dependencies": [],
            "details": "Implement a testing framework in `tests/integration/` using `pytest` and `pytest-asyncio`. Use the `mcp` Python SDK to create a client that connects via stdio to the server. Include utility functions to prepare a temporary 'sandbox' filesystem environment.",
            "status": "done",
            "testStrategy": "Verify the harness can successfully launch the server, call the 'list_tools' method, and receive a valid JSON-RPC response.",
            "updatedAt": "2026-01-08T01:36:03.222Z",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Validate Path Security and Sandbox Constraints (A01-A08)",
            "description": "Verify that all file-based actions strictly adhere to ORACLEPACK_ALLOWED_ROOTS and prevent path traversal.",
            "dependencies": [
              1
            ],
            "details": "Create test cases that attempt to access files outside of the configured `ORACLEPACK_ALLOWED_ROOTS`. Specifically test relative paths (`../`), symlinks (if supported), and absolute paths. Ensure the server returns deterministic error codes and does not leak system information.",
            "status": "done",
            "testStrategy": "Automated tests asserting that calls to tools like 'cat' or 'ls' outside the sandbox return a 'Permission Denied' or similar structured error.",
            "parentId": "undefined",
            "updatedAt": "2026-01-08T01:36:03.227Z"
          },
          {
            "id": 3,
            "title": "Verify Output Truncation and Timeout Logic (A09-A20)",
            "description": "Ensure the server correctly handles large outputs and long-running subprocesses according to configuration.",
            "dependencies": [
              1
            ],
            "details": "Implement tests for `ORACLEPACK_MAX_OUTPUT_CHARS` limits. Verify that stdout is truncated correctly and the 'truncated' flag is set in the response. Test timeout scenarios where a command exceeds the allowed duration, ensuring the exit code 124 is caught and reported correctly.",
            "status": "done",
            "testStrategy": "Execute commands generating large byte streams and commands with 'sleep' to verify truncation thresholds and timeout triggers.",
            "parentId": "undefined",
            "updatedAt": "2026-01-08T01:36:03.231Z"
          },
          {
            "id": 4,
            "title": "Perform MCP Protocol Compliance Audit (A21-A26)",
            "description": "Ensure the server's tool definitions and JSON-RPC messaging fully comply with the Model Context Protocol specification.",
            "dependencies": [
              1
            ],
            "details": "Validate the schema of every tool exposed by the server. Ensure that types, required fields, and descriptions are present. Verify that logs are correctly directed to stderr to avoid corrupting the stdio transport channel.",
            "status": "done",
            "testStrategy": "Use an MCP Inspector tool or a protocol validator script to check the JSON-RPC message formats and schema definitions.",
            "parentId": "undefined",
            "updatedAt": "2026-01-08T01:36:03.235Z"
          },
          {
            "id": 5,
            "title": "Finalize README and Connection Documentation",
            "description": "Update the project documentation to provide clear instructions for agent integration, specifically for Claude Desktop.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Add a 'Deployment' section to `README.md`. Include a sample `claude_desktop_config.json` block showing how to configure the environment variables and the path to the executable. Document the purpose of each environment variable (ROOTS, EXEC, MAX_BYTES).",
            "status": "done",
            "testStrategy": "Manual verification by copying the documented configuration into a local Claude Desktop setup and confirming the server initializes and tools appear.",
            "parentId": "undefined",
            "updatedAt": "2026-01-08T01:36:03.239Z"
          }
        ],
        "updatedAt": "2026-01-08T01:36:03.239Z"
      }
    ],
    "metadata": {
      "version": "1.0.0",
      "lastModified": "2026-01-08T01:36:03.240Z",
      "taskCount": 10,
      "completedCount": 10,
      "tags": [
        "feat-mcp"
      ],
      "created": "2026-01-08T02:22:47.922Z",
      "description": "Tasks for feat-mcp context",
      "updated": "2026-01-08T02:22:47.922Z"
    }
  },
  "prd-oraclepack-2026-01-09": {
    "tasks": [
      {
        "id": 1,
        "title": "Define Pack Data Models and Parser",
        "description": "Establish the core data structures for Packs and Steps in internal/pack and implement the initial Markdown parser.",
        "details": "Create `types.go` with structs `Pack` (Steps [20]Step, Prelude string) and `Step` (ID string, Content string). Implement `ParsePack(path string)` in `parser.go` to extract the contents of the first triple-backtick bash fence from the Markdown file and split it into raw steps.",
        "testStrategy": "Unit tests verifying that a valid Markdown file with a single bash fence is correctly loaded into the internal data structures.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Core Data Structures in types.go",
            "description": "Establish the internal Go data structures used to represent a pack and its individual execution steps.",
            "dependencies": [],
            "details": "Create the file `internal/pack/types.go`. Define a `Step` struct with `ID` (string) and `Content` (string) fields. Define a `Pack` struct containing `Steps [20]Step` (a fixed-size array of 20 steps) and a `Prelude` string.",
            "status": "pending",
            "testStrategy": null
          },
          {
            "id": 2,
            "title": "Implement Regex-Based Markdown Parser",
            "description": "Develop the parser logic to extract bash code blocks from Markdown files and partition them into discrete steps.",
            "dependencies": [
              1
            ],
            "details": "Implement `ParsePack(path string)` in `internal/pack/parser.go`. Use regular expressions to extract the contents of the first triple-backtick bash fence found in the file. Split the fence content into raw steps based on comment markers and populate the `Pack` struct defined in Task 1.",
            "status": "pending",
            "testStrategy": "Unit tests using a sample Markdown file fixture containing a valid bash fence to verify the `Pack` struct is correctly populated with the expected strings."
          }
        ]
      },
      {
        "id": 2,
        "title": "Implement Strict Pack-Shape Validation",
        "description": "Enforce the 20-step invariant and single bash fence constraint on pack validation.",
        "details": "Implement `ValidatePack(pack *Pack)` in `internal/pack/validator.go`. The validator must: 1. Count fences and reject files with > 1 bash fence. 2. Verify exactly 20 steps exist. 3. Ensure steps are numbered sequentially '# 01)' through '# 20)'.",
        "testStrategy": "Table-driven unit tests with fixtures: one fence with 20 steps (pass), two fences (fail), 19 steps (fail), non-sequential IDs (fail).",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Validator Interface and Error Types",
            "description": "Create the basic structure for pack validation including error definitions.",
            "dependencies": [],
            "details": "Create `internal/pack/validator.go`. Define the `ValidatePack(p *Pack) error` function. Include custom error types or constants for `ErrInvalidFenceCount`, `ErrInvalidStepCount`, and `ErrSequenceMismatch` to provide specific feedback.",
            "status": "pending",
            "testStrategy": "Verify that the code compiles and the error types are accessible by other packages."
          },
          {
            "id": 2,
            "title": "Implement Bash Fence Count Validation",
            "description": "Logic to ensure only a single bash code block exists in the pack.",
            "dependencies": [
              1
            ],
            "details": "Update `ValidatePack` to check the raw content or the parsed structure of the `Pack`. It must reject any file that contains zero or more than one '```bash' block. This is critical for maintaining the single-entry point invariant.",
            "status": "pending",
            "testStrategy": "Unit test with sample strings containing 0, 1, and 2 bash code fences."
          },
          {
            "id": 3,
            "title": "Implement Step Count Invariant Check",
            "description": "Verify that the pack contains exactly 20 steps.",
            "dependencies": [
              1
            ],
            "details": "Within `ValidatePack`, add logic to inspect the `Steps` slice. Return an error if `len(p.Steps) != 20`. This enforces the rigid structure required by the oraclepack execution engine.",
            "status": "pending",
            "testStrategy": "Test cases with Pack structs containing 19, 20, and 21 steps respectively."
          },
          {
            "id": 4,
            "title": "Implement Sequential ID Pattern Validation",
            "description": "Validate that steps follow the '# 01)' through '# 20)' naming convention.",
            "dependencies": [
              1,
              3
            ],
            "details": "Iterate through the steps in the `Pack` struct. For each step at index `i`, verify that its ID string matches the format `# %02d)` where the number is `i+1`. Use `fmt.Sprintf` to generate the expected string and compare.",
            "status": "pending",
            "testStrategy": "Test cases with correct sequence, out-of-order IDs, and missing leading zeros (e.g., '# 1)')."
          },
          {
            "id": 5,
            "title": "Create Table-Driven Validation Test Suite",
            "description": "Implement a comprehensive test suite in validator_test.go.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Create `internal/pack/validator_test.go`. Implement a `TestValidatePack` function using a table-driven approach. Include fixtures for: valid 20-step pack, multiple fences, incorrect step count, and non-sequential step IDs.",
            "status": "pending",
            "testStrategy": "Execute 'go test ./internal/pack/...' and ensure coverage includes all branches of the validation logic."
          }
        ]
      },
      {
        "id": 3,
        "title": "Bash Safety Lint for Orphaned Flags",
        "description": "Add a linter to detect lines starting with flags like -p or --prompt that aren't attached to a command.",
        "details": "Implement a custom detector in `internal/pack/lint.go` that scans step content. It should flag lines matching `^\\s*(-p|--prompt|--attachment|-f)\\s+` that appear after a newline or at the start of a step without a preceding command. Integrate `bash -n` for syntax checking.",
        "testStrategy": "Unit tests with step snippets containing intentional orphaned flags and ensuring the linter catches them.",
        "priority": "medium",
        "dependencies": [
          1
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Custom Regex Detector for Orphaned Flags",
            "description": "Develop a regex-based scanner in internal/pack/lint.go to identify lines starting with specific flags that lack a preceding command.",
            "dependencies": [],
            "details": "Define a regex matching ^\\s*(-p|--prompt|--attachment|-f)\\s+ and apply it to each step's content. The detector must identify cases where these flags appear at the start of a string or immediately following a newline without an associated command.",
            "status": "pending",
            "testStrategy": "Unit tests in internal/pack/lint_test.go using string snippets with orphaned flags and valid multi-line commands to ensure low false-positive rates."
          },
          {
            "id": 2,
            "title": "Integrate bash -n Syntax Validation",
            "description": "Incorporate a secondary validation layer using exec.Command to run 'bash -n' against step contents to catch broader syntax errors.",
            "dependencies": [
              1
            ],
            "details": "Invoke 'bash -n' by piping the bash fence content to stdin. Capture stderr output from the command and integrate any detected syntax errors into the linter's reporting output within internal/pack/lint.go.",
            "status": "pending",
            "testStrategy": "Execute the linter against steps containing intentional syntax errors (e.g., unclosed quotes or brackets) and verify that bash -n errors are captured."
          }
        ]
      },
      {
        "id": 4,
        "title": "XDG-Compliant State and Config Management",
        "description": "Relocate run state, reports, and configuration storage outside the repository root using XDG standards.",
        "details": "Implement `internal/state/paths.go` using `os.UserConfigDir()` and `os.UserCacheDir()`. Default state to `~/.cache/oraclepack/runs/`. Add support for `ORACLEPACK_STATE_DIR` env var and `--state-dir` CLI flag to override these defaults.",
        "testStrategy": "Verify path resolution on different OS environments; ensure no state files are written to the current working directory by default.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement core XDG path resolution in internal/state/paths.go",
            "description": "Create the internal/state package and implement basic path resolution using Go's os package.",
            "dependencies": [],
            "details": "Use os.UserConfigDir() and os.UserCacheDir() to determine base paths. Define GetStateDir() which defaults to the 'oraclepack/runs' subdirectory within the user cache directory.",
            "status": "pending",
            "testStrategy": "Unit tests to verify that paths returned match expected XDG directory structures for the target operating systems."
          },
          {
            "id": 2,
            "title": "Add Environment Variable Override for State Directory",
            "description": "Implement support for the ORACLEPACK_STATE_DIR environment variable to override default paths.",
            "dependencies": [
              1
            ],
            "details": "Modify the path resolution logic in internal/state/paths.go to check for the ORACLEPACK_STATE_DIR environment variable before falling back to XDG defaults.",
            "status": "pending",
            "testStrategy": "Set the environment variable in a test environment and verify the resolution logic prioritizes it over XDG defaults."
          },
          {
            "id": 3,
            "title": "Integrate --state-dir CLI flag into the command structure",
            "description": "Add a global or command-specific flag for --state-dir and pass it to the state management logic.",
            "dependencies": [
              2
            ],
            "details": "Update the CLI parsing logic (likely in internal/cli) to accept --state-dir. This flag must take the highest priority in the path resolution hierarchy (Flag > Env > Default).",
            "status": "pending",
            "testStrategy": "Execution of the CLI with the --state-dir flag and verification that it overrides both defaults and environment variables."
          },
          {
            "id": 4,
            "title": "Refactor run state and report storage to use the new path system",
            "description": "Update existing code that writes files to the repository root to use the paths provided by internal/state.",
            "dependencies": [
              1,
              3
            ],
            "details": "Identify and update file writing calls for execution state, reports, and metadata to ensure they are written to the resolved state directory instead of the current working directory.",
            "status": "pending",
            "testStrategy": "Run a full execution cycle and confirm no new files are created in the project repository; verify they appear in the specified state directory."
          },
          {
            "id": 5,
            "title": "Ensure automatic directory creation for state and config paths",
            "description": "Add logic to automatically create the target directories if they do not exist.",
            "dependencies": [
              4
            ],
            "details": "Implement os.MkdirAll logic within the path resolution or file writing helpers to ensure that the application doesn't fail when the state or config directories are missing.",
            "status": "pending",
            "testStrategy": "Delete the state directory and run the tool; verify that the directory structure is automatically re-created without errors."
          }
        ]
      },
      {
        "id": 5,
        "title": "Deterministic Workdir Resolution Logic",
        "description": "Implement the logic to resolve the execution working directory to the repository root if not specified.",
        "details": "In `internal/exec/workdir.go`, implement a search that climbs the directory tree from the pack's location looking for `.git` or `.oraclepack`. This becomes the default WorkDir unless the `--work-dir` flag is provided.",
        "testStrategy": "Integration test: run the tool from a subdirectory and verify the resolved workdir matches the project root.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define ResolveWorkDir function and core structures",
            "description": "Create the internal/exec/workdir.go file and define the public API for directory resolution.",
            "dependencies": [],
            "details": "The function should accept the pack file's path and an optional user-provided work directory string. It should return an absolute path or an error.",
            "status": "pending",
            "testStrategy": "Unit test ensuring that a provided override path is converted to an absolute path correctly."
          },
          {
            "id": 2,
            "title": "Implement directory climbing logic for root markers",
            "description": "Develop the recursive or iterative search for .git or .oraclepack markers in parent directories.",
            "dependencies": [
              1
            ],
            "details": "Using os.Stat and filepath.Dir, climb the tree starting from the pack location. If either .git or .oraclepack is found, return that directory. Default to the pack's directory if no root is found before the filesystem root.",
            "status": "pending",
            "testStrategy": "Unit test using a temporary filesystem structure with nested directories and markers at different levels."
          },
          {
            "id": 3,
            "title": "Integrate resolution into the runner execution path",
            "description": "Update the execution engine to resolve the working directory before running any shell commands.",
            "dependencies": [
              2
            ],
            "details": "Modify the runner logic (likely in internal/exec) to call the new resolution function. Ensure the resolved directory is set as the 'Dir' attribute for os/exec commands or passed to the shell environment.",
            "status": "pending",
            "testStrategy": "Mock the execution environment and verify that the command runner receives the correct working directory path."
          },
          {
            "id": 4,
            "title": "Connect CLI flags to workdir resolution override",
            "description": "Ensure the --work-dir flag value is correctly passed to the resolution function as an override.",
            "dependencies": [
              3
            ],
            "details": "Update the CLI argument parsing logic in the cmd package to capture the --work-dir flag and pass it into the ResolveWorkDir function call within the main execution flow.",
            "status": "pending",
            "testStrategy": "CLI integration test running the tool with the --work-dir flag and verifying the output reflects the forced path."
          },
          {
            "id": 5,
            "title": "Create comprehensive integration tests for workdir resolution",
            "description": "Implement end-to-end tests covering various repository structures and execution contexts.",
            "dependencies": [
              4
            ],
            "details": "Tests should cover: execution from project root, execution from deep subdirectory (with .git at root), and execution with an explicit --work-dir flag overriding a .git root.",
            "status": "pending",
            "testStrategy": "Integration tests using a temporary directory structure mimicking a real repository and executing the oraclepack binary."
          }
        ]
      },
      {
        "id": 6,
        "title": "Attachment Preflight Verification",
        "description": "Verify all file attachments referenced in pack steps exist before execution begins.",
        "details": "In `internal/exec/preflight.go`, parse steps for `-f` or `--attachment` flags. Resolve relative paths against the calculated WorkDir. Return a list of missing files if any pre-run checks fail.",
        "testStrategy": "Run preflight against a pack with a non-existent file reference and verify it returns a descriptive error before executing any steps.",
        "priority": "medium",
        "dependencies": [
          1,
          5
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Attachment Extraction Regex",
            "description": "Create and verify a regular expression to identify -f and --attachment flags in step content.",
            "dependencies": [],
            "details": "In `internal/exec/preflight.go`, implement a regex pattern such as `(?:-f|--attachment)\\s+([^\\s]+)` to extract file paths. The pattern must handle both short and long form flags and ignore surrounding whitespace.",
            "status": "pending",
            "testStrategy": "Unit tests with diverse string inputs representing bash command lines with varying attachment flag positions."
          },
          {
            "id": 2,
            "title": "Implement Path Resolution Logic",
            "description": "Develop a helper function to resolve extracted attachment paths against the execution WorkDir.",
            "dependencies": [
              1
            ],
            "details": "Implement a function that takes a raw path string and a `workDir`. Use `filepath.IsAbs` to check if the path is already absolute; otherwise, use `filepath.Join(workDir, path)` and `filepath.Clean` to resolve the final location.",
            "status": "pending",
            "testStrategy": "Unit tests verifying resolution of '.', '..', absolute paths, and standard relative paths against a mock WorkDir."
          },
          {
            "id": 3,
            "title": "Develop Step Attachment Scanner",
            "description": "Implement the core logic to iterate through all steps and identify missing files.",
            "dependencies": [
              2
            ],
            "details": "Create a function `checkStepAttachments(steps []pack.Step, workDir string) []string`. It should scan each step using the regex from subtask 1, resolve paths using subtask 2, and use `os.Stat` to check for file existence, returning a list of all missing file paths.",
            "status": "pending",
            "testStrategy": "Manual verification with a mock Pack object containing both existing and non-existing file references."
          },
          {
            "id": 4,
            "title": "Integrate Verification into Preflight Workflow",
            "description": "Update the existing preflight execution to fail early if attachments are missing.",
            "dependencies": [
              3
            ],
            "details": "Modify the `Preflight` function in `internal/exec/preflight.go` to invoke the attachment scanner. If the list of missing files is not empty, return a formatted error listing all missing files to prevent the execution engine from starting.",
            "status": "pending",
            "testStrategy": "End-to-end test triggering the preflight check with a manifest referencing a missing file and asserting the error output."
          },
          {
            "id": 5,
            "title": "Add Preflight Attachment Unit Tests",
            "description": "Ensure robustness of the attachment verification with a dedicated test suite.",
            "dependencies": [
              4
            ],
            "details": "Create `internal/exec/preflight_test.go`. Include test cases for multiple attachments in a single step, attachments across different steps, and edge cases like file paths containing spaces or special characters.",
            "status": "pending",
            "testStrategy": "Run `go test -v ./internal/exec` and ensure all attachment-related test cases pass with high code coverage."
          }
        ]
      },
      {
        "id": 7,
        "title": "Core Step Execution Engine",
        "description": "Implement the runner that executes individual bash steps using an interactive login shell context.",
        "details": "In `internal/exec/runner.go`, use `os/exec` to run `bash -lc`. Capture stdout/stderr. Ensure environment variables are propagated. Implement `RunStep(ctx, step)` which returns a `StepResult` including exit code and output duration.",
        "testStrategy": "Execution test with a sample step that outputs a known string and checks environment variables (e.g., 'echo $HOME').",
        "priority": "high",
        "dependencies": [
          1,
          5
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Login Shell Command Construction",
            "description": "Construct the os/exec.Command structure to wrap bash step content within an interactive login shell.",
            "dependencies": [],
            "details": "Update internal/exec/runner.go to build a command using 'bash -lc'. The step script content must be properly escaped or passed via standard input to ensure it executes correctly within the login shell environment.",
            "status": "pending",
            "testStrategy": "Verify command arguments in a unit test to ensure 'bash', '-l', and '-c' are correctly sequenced and the script payload is preserved."
          },
          {
            "id": 2,
            "title": "Stdout and Stderr Streaming and Capture",
            "description": "Implement real-time capture of the process output and calculate execution duration.",
            "dependencies": [
              1
            ],
            "details": "Attach buffers to the process pipes to capture both stdout and stderr. Use a timer to track the start and end of the execution, populating the duration field in the StepResult object returned by RunStep.",
            "status": "pending",
            "testStrategy": "Execute a bash script that writes to both stdout and stderr, then assert that all output is captured in the resulting StepResult."
          },
          {
            "id": 3,
            "title": "Environment Variable Propagation Logic",
            "description": "Develop the logic to merge parent environment variables with task-specific variables.",
            "dependencies": [
              1
            ],
            "details": "Ensure that os.Environ() is combined with any step-specific environment variables. Assign the resulting slice to the Env field of the os/exec.Cmd object to maintain context consistency during execution.",
            "status": "pending",
            "testStrategy": "Run a test step that outputs 'env' and check for the presence of both standard system variables and injected custom variables."
          }
        ]
      },
      {
        "id": 8,
        "title": "Deterministic Run Manifests and Reporting",
        "description": "Generate run.json and steps.json metadata files for every execution run.",
        "details": "Implement `internal/report/generate.go`. `run.json` should include pack hash, git SHA, and start time. `steps.json` should be an array of objects containing step_id, status (success/failed/skipped), and path to captured logs. Write these to the StateDir.",
        "testStrategy": "Execute a 2-step pack and verify that the resulting JSON files are correctly formatted and contain valid pack hashes.",
        "priority": "medium",
        "dependencies": [
          4,
          7
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define JSON Schemas and Data Structures for Manifests",
            "description": "Define the Go structures and JSON tags for run.json and steps.json to ensure consistent metadata reporting.",
            "dependencies": [],
            "details": "Create 'internal/report/models.go'. Define 'RunReport' with PackHash (string), GitSHA (string), and StartTime (time.Time). Define 'StepReport' with StepID (string), Status (string: success/failed/skipped), and LogPath (string). Ensure all fields have proper json tags.",
            "status": "pending",
            "testStrategy": "Unit tests to marshal the structs into JSON and verify the output matches the required schema format."
          },
          {
            "id": 2,
            "title": "Implement Atomic Writing Logic for Report Generation",
            "description": "Develop the file-writing logic in generate.go to save manifest files safely to the StateDir.",
            "dependencies": [
              1
            ],
            "details": "Implement the 'Generate' function in 'internal/report/generate.go'. Use atomic file writing by creating a temporary file in the StateDir and then using os.Rename to replace the target run.json or steps.json. This prevents partial writes or corruption.",
            "status": "pending",
            "testStrategy": "Execution test that creates a mock StateDir, calls the generator, and asserts that the resulting files are valid JSON and persist correctly after writing."
          }
        ]
      },
      {
        "id": 9,
        "title": "Resume and Rerun Semantics",
        "description": "Enable the runner to skip previously successful steps and support targeted reruns.",
        "details": "Modify the orchestrator to load existing `steps.json` from the StateDir. Logic: if `--rerun all` is not set, skip steps marked as 'success'. Support `--rerun 05,06` to force specific step execution.",
        "testStrategy": "Simulate a failed run at step 03, then run again without flags and verify steps 01-02 are skipped.",
        "priority": "medium",
        "dependencies": [
          8
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement State Loading and Previous Result Detection",
            "description": "Develop the logic to read existing execution state from the StateDir to identify completed steps from previous runs.",
            "dependencies": [],
            "details": "Create or update functions in the execution package to load and parse 'steps.json' from the StateDir. The system must map these results to the current execution plan by step ID, enabling the orchestrator to differentiate between 'success', 'failure', and 'not-run' states.",
            "status": "pending",
            "testStrategy": "Unit test using a mocked StateDir containing a 'steps.json' file to verify that the loader accurately identifies successful versus failed steps."
          },
          {
            "id": 2,
            "title": "Implement Conditional Step Execution and Rerun Logic",
            "description": "Integrate logic into the runner orchestrator to skip successful steps or force reruns based on CLI flags.",
            "dependencies": [
              1
            ],
            "details": "Update the main execution loop in the orchestrator to evaluate the loaded state against CLI flags like '--rerun all' or specific step IDs (e.g., '--rerun 05,06'). If a step is marked as 'success' and no override flag applies, the orchestrator should skip execution and proceed to the next step.",
            "status": "pending",
            "testStrategy": "Integration test simulating a failed run at step 03, then performing a second run without flags to verify steps 01-02 are skipped, and a third run with '--rerun 02' to verify step 02 is forced to run."
          }
        ]
      },
      {
        "id": 10,
        "title": "Action Pack Artifact Standardization",
        "description": "Ensure Action Packs produce standardized artifacts in the .oraclepack/ticketify directory.",
        "details": "Modify the output logic specifically for Action Packs to ensure files like `_actions.json` and `_tickets_index.json` are written to a fixed sub-directory `.oraclepack/ticketify/` relative to the WorkDir regardless of the pack's location.",
        "testStrategy": "Run an Action Pack and confirm artifacts are produced in the predictable .oraclepack path.",
        "priority": "high",
        "dependencies": [
          5,
          8
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define standard artifact path constants",
            "description": "Establish the fixed directory and filename constants for Action Pack artifacts.",
            "dependencies": [],
            "details": "In a relevant package (e.g., `internal/pack/constants.go`), define constants for the sub-directory name `.oraclepack/ticketify` and the artifact filenames `_actions.json` and `_tickets_index.json`. This ensures consistency across the codebase.",
            "status": "pending",
            "testStrategy": "Code review to ensure constants follow project naming conventions."
          },
          {
            "id": 2,
            "title": "Implement artifact path resolution helper",
            "description": "Create a utility to resolve absolute paths for Action Pack artifacts relative to the WorkDir.",
            "dependencies": [
              1
            ],
            "details": "Implement a function in `internal/pack` or `internal/exec` that takes the resolved WorkDir (from Task 5) and joins it with the `.oraclepack/ticketify` subdirectory. It should return absolute paths for the specific artifact files required by Action Packs.",
            "status": "pending",
            "testStrategy": "Unit test verifying that given a WorkDir '/repo', the helper returns '/repo/.oraclepack/ticketify/_actions.json'."
          },
          {
            "id": 3,
            "title": "Ensure artifact directory initialization",
            "description": "Ensure the .oraclepack/ticketify directory is created before artifact generation.",
            "dependencies": [
              2
            ],
            "details": "Modify the Action Pack execution setup logic to ensure that `os.MkdirAll` is called on the resolved artifact directory path. This must happen before any JSON writers attempt to create files to prevent path-not-found errors.",
            "status": "pending",
            "testStrategy": "Run a mock execution and verify that the directory is created if it was previously missing."
          },
          {
            "id": 4,
            "title": "Redirect Action Pack JSON output logic",
            "description": "Update the orchestrator to write _actions.json and _tickets_index.json to the standardized directory.",
            "dependencies": [
              3
            ],
            "details": "Update the post-execution or artifact-writing phase of the orchestrator. Specifically for Action Packs, use the helper from subtask 2 to determine the output location instead of writing to the current directory or the Pack's source directory.",
            "status": "pending",
            "testStrategy": "Execute an Action Pack and check for the presence of the JSON files in the standardized path."
          },
          {
            "id": 5,
            "title": "Verify artifact standardization in integration tests",
            "description": "Create an integration test to confirm artifacts are correctly placed regardless of execution context.",
            "dependencies": [
              4
            ],
            "details": "Add an integration test that runs an Action Pack from a subdirectory with a specified --work-dir. Assert that files are created in `.oraclepack/ticketify/` relative to that work directory, not the execution directory or the pack's home.",
            "status": "pending",
            "testStrategy": "Automated integration test using a temporary filesystem to verify file existence at the expected relative path."
          }
        ]
      },
      {
        "id": 11,
        "title": "Executor Dispatcher for Non-Oracle Tools",
        "description": "Implement the dispatcher to route work to codex, gemini, or tm based on the _actions.json manifest.",
        "details": "Create `internal/exec/dispatcher.go`. If a step is identified as an action step (e.g., steps 09-13), read `_actions.json`, check the `executor` field, and run the corresponding binary (e.g., `codex implement`). Guard with `command -v` to skip if the tool is missing.",
        "testStrategy": "Mock 'codex' and 'gemini' binaries and verify the dispatcher correctly calls them when 'executor' is specified in the actions manifest.",
        "priority": "medium",
        "dependencies": [
          7,
          10
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Manifest Parsing for Executor Mapping",
            "description": "Develop the logic to read and parse the _actions.json manifest file to associate steps with specific executors.",
            "dependencies": [],
            "details": "Define the ActionManifest struct in internal/exec/dispatcher.go. Implement a function to load the file from the pack directory and return a map that correlates step identifiers (09-13) with executor strings like 'codex' or 'gemini'.",
            "status": "pending",
            "testStrategy": "Unit tests using various JSON fixtures to ensure the mapping logic correctly identifies executor fields and handles missing manifest files gracefully."
          },
          {
            "id": 2,
            "title": "Dynamic Process Execution with Tool Availability Checks",
            "description": "Develop the core dispatcher function to execute external binaries while verifying tool existence on the system.",
            "dependencies": [
              1
            ],
            "details": "Implement the command execution logic using the os/exec package. Use exec.LookPath to check if the required executor binary exists in the user's PATH. If missing, the step should be logged as skipped; otherwise, run the tool with appropriate subcommands.",
            "status": "pending",
            "testStrategy": "Integration tests using mock binaries in a temporary PATH directory to verify the dispatcher correctly calls the tool or skips execution based on availability."
          }
        ]
      },
      {
        "id": 12,
        "title": "TUI URL Picker and PRD Micro-pack Flow",
        "description": "Add a ChatGPT URL selection entry for PRD generation in the TUI.",
        "details": "Update `internal/tui/url_picker.go` to include a 'PRD Generator' option. When selected, it should trigger a single-step execution (micro-pack) that calls oracle with the `tickets_prd.md` context and produces `final_prd.md`.",
        "testStrategy": "Manual TUI verification to ensure the PRD Generator URL can be selected and saved without affecting the global project URL.",
        "priority": "low",
        "dependencies": [
          1,
          7
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Add PRD Generator Option to TUI URL Picker",
            "description": "Modify the TUI URL selection menu to include a dedicated 'PRD Generator' entry.",
            "dependencies": [],
            "details": "Update `internal/tui/url_picker.go` to append a new item to the list of available URLs. This item should be identified as a special 'PRD' mode rather than a standard URL string when selected.",
            "status": "pending",
            "testStrategy": "Manual verification by launching the TUI and ensuring the 'PRD Generator' option appears and is selectable."
          },
          {
            "id": 2,
            "title": "Define PRD Micro-pack and Context Logic",
            "description": "Implement the logic for creating a single-step execution flow for PRD generation.",
            "dependencies": [
              1
            ],
            "details": "Create a function that constructs a 'Micro-pack' consisting of a single step. This step must be configured to pass the contents of `tickets_prd.md` to the oracle and define `final_prd.md` as the target output file.",
            "status": "pending",
            "testStrategy": "Unit test ensuring the micro-pack generator produces the correct oracle parameters and identifies the correct input/output files."
          },
          {
            "id": 3,
            "title": "Integrate Micro-pack Execution into Core Runner",
            "description": "Connect the TUI selection to the runner to trigger the PRD generation flow immediately upon selection.",
            "dependencies": [
              2
            ],
            "details": "Update the execution entry point to detect the 'PRD' selection from the TUI. When detected, it should bypass the standard pack loading logic and execute the PRD micro-pack through the runner.",
            "status": "pending",
            "testStrategy": "End-to-end test starting from the TUI selection and verifying that the runner initiates a single-step oracle call with the PRD context."
          }
        ]
      },
      {
        "id": 13,
        "title": "MCP Run Tool Flag Passthrough",
        "description": "Update the MCP server to support explicit work_dir, out_dir, and oracle_bin parameters.",
        "details": "Modify `oraclepack-mcp-server/oraclepack_mcp_server/server.py` to accept additional optional arguments in the `oraclepack_run_pack` tool definition. Map these arguments directly to the CLI flags when spawning the process.",
        "testStrategy": "Use the MCP inspector to call the run tool with an explicit `work_dir` and verify the underlying oraclepack command receives the flag.",
        "priority": "medium",
        "dependencies": [
          5,
          7
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Arguments in MCP Tool Schema",
            "description": "Update the oraclepack_run_pack tool definition in the MCP server to include new optional parameters.",
            "dependencies": [],
            "details": "Modify the @mcp.tool() decorator or call in oraclepack_mcp_server/server.py to add work_dir (string), out_dir (string), and oracle_bin (string) as optional arguments with appropriate descriptions.",
            "status": "pending",
            "testStrategy": "Verify the tool schema update by running the MCP server and checking the listed tools via an MCP inspector or client."
          },
          {
            "id": 2,
            "title": "Update CLI Command Construction Logic",
            "description": "Modify the internal logic that constructs the shell command to include the new flags if provided.",
            "dependencies": [
              1
            ],
            "details": "Update the run_pack function in server.py to check for the presence of work_dir, out_dir, and oracle_bin. Append --work-dir, --out-dir, and --oracle-bin flags respectively to the subprocess argument list.",
            "status": "pending",
            "testStrategy": "Unit test the command construction logic by mocking subprocess.run and asserting the command list contains the expected flags."
          },
          {
            "id": 3,
            "title": "Implement Path Validation for MCP Arguments",
            "description": "Add basic validation for the provided directory and binary paths before spawning the process.",
            "dependencies": [
              2
            ],
            "details": "Ensure that if work_dir or oracle_bin are provided, they are checked for basic existence or format to provide better error messages through the MCP interface if the paths are invalid.",
            "status": "pending",
            "testStrategy": "Invoke the tool with non-existent paths via the MCP inspector and verify that appropriate error messages are returned."
          },
          {
            "id": 4,
            "title": "Handle Environment and Binary Pathing",
            "description": "Ensure the oracle_bin parameter correctly overrides the default binary location in the execution environment.",
            "dependencies": [
              2
            ],
            "details": "Update the execution logic to use the value of oracle_bin as the executable path if provided, otherwise defaulting to the standard 'oraclepack' command in the system PATH.",
            "status": "pending",
            "testStrategy": "Run a test where a dummy script is passed as oracle_bin and verify it is the one executed by the MCP server."
          },
          {
            "id": 5,
            "title": "End-to-End Integration Test with MCP Inspector",
            "description": "Perform a full manual test using the MCP Inspector to verify that flags are correctly passed to the Go binary.",
            "dependencies": [
              3,
              4
            ],
            "details": "Use the mcp-inspector tool to call oraclepack_run_pack with a custom work_dir and out_dir. Verify that the files are generated in the specified out_dir and that the Go binary recognizes the context from work_dir.",
            "status": "pending",
            "testStrategy": "Manual verification by inspecting the filesystem output after running the tool via MCP."
          }
        ]
      },
      {
        "id": 14,
        "title": "Headless CLI URL Overrides",
        "description": "Implement CLI flags to provide ChatGPT URL overrides without entering the TUI.",
        "details": "Add `--chatgpt-url` and `--chatgpt-url-name` to `internal/cli/run.go`. When present, these values bypass the saved state/config URLs for the duration of the run.",
        "testStrategy": "Run a pack with `--chatgpt-url-name my-custom-url` and verify the execution metadata reflects the override.",
        "priority": "low",
        "dependencies": [
          4,
          8
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define CLI flags for ChatGPT URL overrides in run command",
            "description": "Add string flags for --chatgpt-url and --chatgpt-url-name to the run command in internal/cli/run.go.",
            "dependencies": [],
            "details": "Modify the init() function or the RunCmd definition in internal/cli/run.go. Use Cobra's PersistentFlags() or Flags() method to register 'chatgpt-url' and 'chatgpt-url-name' and bind them to variables available during execution.",
            "status": "pending",
            "testStrategy": "Execute 'oraclepack run --help' to ensure the new flags are correctly registered and visible in the help menu."
          },
          {
            "id": 2,
            "title": "Implement configuration override logic in state management",
            "description": "Modify the configuration loading logic to accept and prioritize CLI-provided URL overrides.",
            "dependencies": [
              1
            ],
            "details": "Update the state or configuration loading functions (likely within internal/state/paths.go or a similar configuration handler) to check if the new CLI variables are populated. These should take precedence over values stored in the XDG config files.",
            "status": "pending",
            "testStrategy": "Unit test the configuration resolver by providing mock file values and mock CLI values, asserting that CLI values are returned."
          },
          {
            "id": 3,
            "title": "Inject overrides into the execution context",
            "description": "Ensure the Pack execution logic uses the overridden URL and name instead of querying user state.",
            "dependencies": [
              2
            ],
            "details": "In internal/cli/run.go, modify the execution block to pass the CLI flag values into the executor. If the flags are present, bypass the logic that typically fetches the default URL from the saved application state.",
            "status": "pending",
            "testStrategy": "Run a Pack with the --chatgpt-url flag and verify via logs or debug output that the executor receives the correct overridden string."
          },
          {
            "id": 4,
            "title": "Bypass TUI interactions when overrides are provided",
            "description": "Implement logic to skip interactive URL selection or confirmation if headless flags are set.",
            "dependencies": [
              3
            ],
            "details": "Ensure that if --chatgpt-url-name or --chatgpt-url is provided, any calls to the Terminal User Interface (TUI) for URL selection are conditionally skipped. This enables fully automated, headless execution.",
            "status": "pending",
            "testStrategy": "Invoke the run command with overrides in a non-interactive shell environment and verify the command completes without hanging or requiring input."
          },
          {
            "id": 5,
            "title": "Include URL source in execution metadata reporting",
            "description": "Update the run report metadata to reflect that a URL override was used during execution.",
            "dependencies": [
              4
            ],
            "details": "Modify the run reporting logic to include a field indicating the source of the ChatGPT URL (e.g., 'cli-override' vs 'config'). Ensure the specific URL and name used are captured in the final execution report.",
            "status": "pending",
            "testStrategy": "Execute a Pack with overrides and inspect the generated run report or JSON metadata file to confirm the 'Source' field correctly identifies the CLI override."
          }
        ]
      }
    ],
    "metadata": {
      "created": "2026-01-09T02:39:06.401Z",
      "updated": "2026-01-09T02:41:01.537Z",
      "description": "Tasks for prd-oraclepack-2026-01-09 context"
    }
  },
  "feature-action-pack-artifact-gates-headless-steps": {
    "tasks": [
      {
        "id": 1,
        "title": "Establish Foundation Layer and Common Primitives",
        "description": "Implement core utilities for configuration, file system abstraction, and custom error handling to be used across all internal modules.",
        "details": "Create 'internal/foundation' package. Define 'Config' struct to hold run-time settings. Implement 'atomic' file writing using 'os.Rename' to prevent corruption. Define specific error types such as 'ErrMissingBinary' and 'ErrArtifactMissing'. Include a clock interface for testing time-dependent state transitions. Technology: Go standard library (os, path/filepath, errors).",
        "testStrategy": "Unit tests for atomic write semantics (writing to temp file and renaming). Table-driven tests for config parsing from environment variables and JSON files.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Configuration Management System",
            "description": "Create the logic for loading application settings from both JSON files and environment variables.",
            "dependencies": [],
            "details": "Define a Config struct in 'internal/foundation'. Implement a loader function that prioritizes environment variables over JSON file values using the Go standard library.",
            "status": "done",
            "testStrategy": "Table-driven tests to verify override precedence and parsing accuracy for different data types.",
            "updatedAt": "2026-01-09T04:21:13.110Z",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Atomic File System Operations",
            "description": "Develop a utility to write files safely using a temporary file and atomic rename to prevent data corruption.",
            "dependencies": [
              1
            ],
            "details": "Implement a WriteAtomic function in 'internal/foundation' that writes to a .tmp file using os.Create, then calls os.Rename to replace the target destination.",
            "status": "done",
            "testStrategy": "Unit tests ensuring that a file is only updated if the write succeeds, and verifying the existence of the final file.",
            "parentId": "undefined",
            "updatedAt": "2026-01-09T04:21:26.250Z"
          },
          {
            "id": 3,
            "title": "Define Common Error Types and Clock Interface",
            "description": "Create specialized error definitions and a clock interface for reliable time-based state transitions and testing.",
            "dependencies": [
              1
            ],
            "details": "Define ErrMissingBinary and ErrArtifactMissing using errors.New. Implement a Clock interface with a RealClock and MockClock for deterministic testing of time logic.",
            "status": "done",
            "testStrategy": "Mock clock verification tests to ensure state transitions occur at expected intervals. Unit tests for custom error type comparisons.",
            "parentId": "undefined",
            "updatedAt": "2026-01-09T04:21:35.133Z"
          }
        ],
        "complexity": 3,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down the foundation layer into subtasks for: 1) Configuration management using environment variables and JSON. 2) Atomic file system operations with os.Rename. 3) Custom error types and clock interfaces for testing.",
        "updatedAt": "2026-01-09T04:21:35.133Z"
      },
      {
        "id": 2,
        "title": "Implement Shell Runner with Login Shell Semantics",
        "description": "Create a runner that executes commands via 'bash -lc' to ensure environment variables and PATH are correctly loaded as per Action Pack requirements.",
        "details": "Implement 'internal/shell' package. Use 'os/exec' to invoke '/bin/bash' with args ['-lc', cmd]. Capture Stdout, Stderr, and ExitCode. Add a helper 'DetectBinary(name string) (string, bool)' that uses 'exec.LookPath' to check for tool availability on the host system. Ensure login-shell behavior matches standard developer environments.",
        "testStrategy": "Integration test executing 'bash -lc \"echo $PATH\"' to verify login profile loading. Test 'DetectBinary' with known system binaries (e.g., 'ls') and fake binaries.",
        "priority": "high",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Shell Command Execution Wrapper",
            "description": "Develop the core functionality to execute shell commands using 'bash -lc' to ensure login shell semantics and environment loading.",
            "dependencies": [],
            "details": "Implement a runner in the 'internal/shell' package that utilizes 'os/exec' to invoke '/bin/bash' with arguments ['-lc', cmd]. The implementation must properly capture and route Stdout and Stderr, and correctly return the process ExitCode.",
            "status": "done",
            "testStrategy": "Integration test executing 'bash -lc \"echo $PATH\"' to verify that login profiles are loaded and output is captured correctly.",
            "parentId": "undefined",
            "updatedAt": "2026-01-09T04:22:34.736Z"
          },
          {
            "id": 2,
            "title": "Implement Binary Detection Utility",
            "description": "Create a helper function to check for the presence and location of required binaries on the host system's PATH.",
            "dependencies": [],
            "details": "Implement 'DetectBinary(name string) (string, bool)' within 'internal/shell' using 'exec.LookPath'. This function must return the full filesystem path to the binary and a boolean indicating if it was successfully found.",
            "status": "done",
            "testStrategy": "Unit tests verifying detection of known system binaries (e.g., 'ls') and confirming false results for random, non-existent binary names.",
            "parentId": "undefined",
            "updatedAt": "2026-01-09T04:22:42.424Z"
          }
        ],
        "complexity": 4,
        "recommendedSubtasks": 2,
        "expansionPrompt": "Create subtasks for: 1) The 'bash -lc' execution wrapper and output capturing. 2) The binary detection utility using exec.LookPath.",
        "updatedAt": "2026-01-09T04:22:42.424Z"
      },
      {
        "id": 3,
        "title": "Develop Action Pack Document Parser",
        "description": "Implement a parser to convert Markdown-based Action Packs into structured Go models, strictly enforcing the 20-step contract.",
        "details": "Implement 'internal/pack' package. Use a Markdown parser (like 'github.com/yuin/goldmark') or specialized regex to extract the single fenced 'bash' block. Map steps to a 'Step' struct containing ID, Shell command, and Metadata (ROI, impact, etc.). Validate that the pack contains exactly 20 steps.",
        "testStrategy": "Unit tests with golden Markdown fixtures. Test edge cases like empty steps, malformed metadata headers, and multiple code blocks (ensure only the primary 'bash' block is used).",
        "priority": "high",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Markdown Fenced Code Block Extractor",
            "description": "Develop the logic to isolate and extract the content within the primary bash fenced code block from an Action Pack Markdown file.",
            "dependencies": [],
            "details": "Utilize 'github.com/yuin/goldmark' to parse the Markdown document. Navigate the AST to find the first 'bash' language fenced code block. Extract its raw text content for further processing, ensuring handling of cases with multiple blocks by selecting the primary instruction set.",
            "status": "done",
            "testStrategy": "Unit tests with Markdown fixtures containing multiple code blocks, ensuring only the bash-labeled block is extracted.",
            "parentId": "undefined",
            "updatedAt": "2026-01-09T04:40:12.555Z"
          },
          {
            "id": 2,
            "title": "Parse Step Metadata and Shell Commands",
            "description": "Convert extracted bash block content into a slice of structured Go 'Step' models, parsing ROI and Impact metadata.",
            "dependencies": [
              1
            ],
            "details": "Tokenize the bash block by lines. Use regex to extract metadata stored in comments (e.g., '# ROI:', '# Impact:'). Map these values along with the raw shell command to a 'Step' struct. Ensure IDs are correctly parsed from the step headers.",
            "status": "done",
            "testStrategy": "Table-driven tests using mocked step strings to verify that Shell commands and Metadata fields are correctly populated.",
            "parentId": "undefined",
            "updatedAt": "2026-01-09T04:40:22.062Z"
          },
          {
            "id": 3,
            "title": "Implement 20-Step Contract Validation Logic",
            "description": "Create a validator to strictly enforce the requirement that every Action Pack must contain exactly 20 steps.",
            "dependencies": [
              2
            ],
            "details": "Within the 'internal/pack' package, implement a validation function that checks the length of the parsed Step slice. If the count is not exactly 20, or if Step IDs are not sequential (1-20), return a custom error. This ensures the parser adheres to the rigid document contract.",
            "status": "done",
            "testStrategy": "Unit tests using Golden Markdown files with 19, 20, and 21 steps to verify the validation boundaries.",
            "parentId": "undefined",
            "updatedAt": "2026-01-09T04:40:28.585Z"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Detail subtasks for: 1) Markdown fenced block extraction. 2) Step metadata parsing (ROI, impact). 3) 20-step contract validation logic.",
        "updatedAt": "2026-01-09T04:40:28.585Z"
      },
      {
        "id": 4,
        "title": "Implement State and Report Persistence",
        "description": "Create the persistence layer for tracking run progress and generating the final execution report in JSON format.",
        "details": "Implement 'internal/state' package. Define 'RunState' (current step, per-step results) and 'RunReport' (summary, gates, failures). Implement 'WriteState' and 'WriteReport' to output 'ticket-action-pack.state.json' and 'ticket-action-pack.report.json'. Ensure status transitions (Selected -> Running -> Succeeded|Failed|Skipped) are handled atomically.",
        "testStrategy": "Round-trip JSON serialization/deserialization tests. Verify that partial runs can be resumed by reading the state file.",
        "priority": "medium",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define JSON Schemas and Structures for State and Reports",
            "description": "Define the Golang structs and JSON mapping for RunState and RunReport entities in the internal/state package.",
            "dependencies": [],
            "details": "Create the RunState struct to track the current step index and per-step result history. Define the RunReport struct to encapsulate the final summary, artifact gate outcomes, and failure logs. Ensure all structures have appropriate JSON tags for 'ticket-action-pack.state.json' and 'ticket-action-pack.report.json'.",
            "status": "done",
            "testStrategy": "Perform round-trip serialization and deserialization unit tests to ensure no data is lost during JSON conversion.",
            "parentId": "undefined",
            "updatedAt": "2026-01-09T05:09:45.181Z"
          },
          {
            "id": 2,
            "title": "Implement Atomic File I/O and Resumption Logic",
            "description": "Develop the file-handling logic to write state updates safely and allow the engine to resume from existing state files.",
            "dependencies": [
              1
            ],
            "details": "Implement WriteState and WriteReport functions that use a 'write-then-rename' pattern to ensure atomic file updates. Add a LoadState function to parse the existing state file, enabling the execution engine to skip previously completed steps and maintain status transitions.",
            "status": "done",
            "testStrategy": "Integration tests to simulate process interruption and verify that the system can resume execution from the last saved step in the state file.",
            "parentId": "undefined",
            "updatedAt": "2026-01-09T05:09:52.641Z"
          }
        ],
        "complexity": 3,
        "recommendedSubtasks": 2,
        "expansionPrompt": "Divide into subtasks for: 1) JSON schema definition for state and reports. 2) Atomic file writing and resumption logic.",
        "updatedAt": "2026-01-09T05:09:52.641Z"
      },
      {
        "id": 5,
        "title": "Define Tool Registry and Capabilities",
        "description": "Establish a registry of supported tools (oracle, tm, codex, gemini) and their specific invocation requirements.",
        "details": "Implement 'internal/tools' package. Create 'ToolKind' enum. Define metadata for each tool, including non-interactive flag recommendations (e.g., 'codex exec' for Codex). Provide a presence-check interface that wraps shell binary detection.",
        "testStrategy": "Unit tests to ensure 'ToolKind' mappings are correct and that recommended flags for 'codex' match PRD requirements for headless execution.",
        "priority": "high",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define ToolKind Enum and Metadata Registry",
            "description": "Create the internal/tools package to house the ToolKind enum and a registry that maps tools to their specific CLI flags and metadata.",
            "dependencies": [],
            "details": "Implement the ToolKind type using Go iota constants for oracle, tm, codex, and gemini. Create a ToolRegistry structure that maps these constants to metadata, specifically ensuring 'codex' is associated with the 'exec' subcommand for headless operation. Define the interface for binary presence detection.",
            "status": "done",
            "testStrategy": "Unit tests to verify that ToolKind constants map to the correct string identifiers and that the metadata registry returns the expected execution flags for each tool.",
            "parentId": "undefined",
            "updatedAt": "2026-01-09T05:12:10.951Z"
          }
        ],
        "complexity": 2,
        "recommendedSubtasks": 1,
        "expansionPrompt": "Create a subtask to define the ToolKind enum and the tool-specific flag mapping registry.",
        "updatedAt": "2026-01-09T05:12:10.951Z"
      },
      {
        "id": 6,
        "title": "Implement Multi-Tool Command Dispatcher",
        "description": "Expand command detection beyond the 'oracle' prefix to include 'tm', 'task-master', 'codex', and 'gemini'.",
        "details": "Implement 'internal/dispatch' package. Replace existing regex-based detection with a flexible classifier: '^(\\s*)(oracle|tm|task-master|codex|gemini)\\b'. Map detected prefixes to 'ToolKind'. Preserve existing override/validation logic for 'oracle' commands while preparing hooks for other tools.",
        "testStrategy": "Table-driven tests with a variety of command strings (with/without leading whitespace, different tools). Assert that 'oracle' behavior is unchanged.",
        "priority": "high",
        "dependencies": [
          "3",
          "5"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Develop Regex-Based Command Classifier",
            "description": "Implement the core logic to detect tool prefixes and map them to ToolKind enums.",
            "dependencies": [],
            "details": "Create the 'internal/dispatch' package. Implement a classifier using the regex '^(\\s*)(oracle|tm|task-master|codex|gemini)\\b'. Map these to a ToolKind type and ensure the classifier returns both the tool type and the stripped command body.",
            "status": "done",
            "testStrategy": "Table-driven unit tests in 'internal/dispatch' with various strings: with/without spaces, all supported tool names, and unsupported prefixes to ensure correct mapping and error handling.",
            "parentId": "undefined",
            "updatedAt": "2026-01-09T05:12:28.131Z"
          },
          {
            "id": 2,
            "title": "Integrate Dispatcher into Oracle Command Pipeline",
            "description": "Replace the legacy oracle detection with the new dispatcher while maintaining existing validation rules.",
            "dependencies": [
              1
            ],
            "details": "Update the execution engine to call the dispatch classifier. Maintain existing override/validation logic specifically for the 'oracle' ToolKind. Prepare the pipeline architecture to handle diverse tool behaviors based on the detected ToolKind.",
            "status": "done",
            "testStrategy": "Functional testing of the oracle pipeline to confirm no regressions. Verify that 'tm' or 'codex' prefixes are correctly parsed and passed to the next stage of execution.",
            "parentId": "undefined",
            "updatedAt": "2026-01-09T05:12:39.892Z"
          }
        ],
        "complexity": 4,
        "recommendedSubtasks": 2,
        "expansionPrompt": "Split into subtasks for: 1) Implementing the regex-based command classifier. 2) Integrating the classifier into the existing oracle command pipeline.",
        "updatedAt": "2026-01-09T05:12:39.892Z"
      },
      {
        "id": 7,
        "title": "Define Action Pack Artifact Contract",
        "description": "Specify and implement the validation gates for required output artifacts in the '.oraclepack/ticketify/' directory.",
        "details": "Implement 'internal/artifacts' package. Define 'ArtifactContract' as a list of file paths (next.json, codex-implement.md, codex-verify.md, PR.md). Implement 'EvaluateGates' logic to check for file existence and size. Associate specific artifacts with their generating steps (e.g., Step 09 produces 'next.json').",
        "testStrategy": "Unit tests that simulate file existence in a temporary directory and verify that the 'EvaluateGates' function correctly reports pass/fail/skip.",
        "priority": "medium",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Artifact Path Registry and Step Associations",
            "description": "Create the 'internal/artifacts' package and define the registry of required files mapped to specific Action Pack steps.",
            "dependencies": [],
            "details": "Define 'ArtifactContract' as a data structure containing file paths like next.json, codex-implement.md, codex-verify.md, and PR.md relative to the '.oraclepack/ticketify/' directory. Map these artifacts to their respective producing steps (e.g., Step 09 is associated with next.json).",
            "status": "done",
            "testStrategy": "Unit tests verifying that the registry correctly maps step IDs to the expected set of artifact file paths.",
            "parentId": "undefined",
            "updatedAt": "2026-01-09T05:12:58.955Z"
          },
          {
            "id": 2,
            "title": "Implement EvaluateGates Validation Logic",
            "description": "Develop the logic to verify the presence and non-emptiness of artifacts in the local filesystem after step execution.",
            "dependencies": [
              1
            ],
            "details": "Implement the 'EvaluateGates' function in 'internal/artifacts'. This function must check for file existence using 'os.Stat' and ensure file size is greater than zero. It should return 'ErrArtifactMissing' if a required file is absent and handle optional artifact skips based on step metadata.",
            "status": "done",
            "testStrategy": "Table-driven unit tests using temporary directories to simulate missing, empty, and valid files, ensuring the gate logic returns correct pass/fail results.",
            "parentId": "undefined",
            "updatedAt": "2026-01-09T05:13:10.850Z"
          }
        ],
        "complexity": 3,
        "recommendedSubtasks": 2,
        "expansionPrompt": "Break into subtasks for: 1) Defining the artifact path registry. 2) Implementing the EvaluateGates file-check logic.",
        "updatedAt": "2026-01-09T05:13:10.850Z"
      },
      {
        "id": 8,
        "title": "Update Action Pack Markdown Template",
        "description": "Modify the 'ticket-action-pack.md' template to replace placeholder steps with automated headless logic and artifact guards.",
        "details": "Implement 'internal/templates' package. Update steps 09-13 and 16. Use 'command -v' guards (e.g., 'command -v codex >/dev/null && codex exec ... || echo \"Skipped: codex missing\"'). Ensure Codex steps use the 'exec' subcommand for non-interactive execution. Maintain the 20-step single-fence contract.",
        "testStrategy": "Golden-file comparison test for the rendered template. Validate that the rendered Markdown is still parseable by 'internal/pack'.",
        "priority": "medium",
        "dependencies": [
          "7"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Update Markdown Template with Shell Guards and Headless Logic",
            "description": "Modify the 'ticket-action-pack.md' template to update steps 09-13 and 16 with automated logic.",
            "dependencies": [],
            "details": "Update the template file in 'internal/templates'. Implement shell guards using 'command -v' for artifact check and tool presence. Transition Codex steps to use 'exec' for headless execution. Ensure all modifications remain within the single bash code block to respect the 20-step contract.",
            "status": "done",
            "testStrategy": "Manual verification of Markdown structure and syntax within the template source.",
            "parentId": "undefined",
            "updatedAt": "2026-01-09T05:17:42.125Z"
          },
          {
            "id": 2,
            "title": "Implement Template Rendering and Validation Tests",
            "description": "Develop tests to ensure the rendered Markdown template is valid and parseable.",
            "dependencies": [
              1
            ],
            "details": "Create a testing suite that renders the 'ticket-action-pack.md' with mock data. Perform a golden-file comparison to ensure output consistency. Use the 'internal/pack' parser to validate that the rendered output contains exactly 20 steps and follows the expected schema.",
            "status": "done",
            "testStrategy": "Automated unit tests utilizing golden-file comparisons and integration with the internal/pack parser.",
            "parentId": "undefined",
            "updatedAt": "2026-01-09T05:17:50.369Z"
          }
        ],
        "complexity": 3,
        "recommendedSubtasks": 2,
        "expansionPrompt": "Separate subtasks for: 1) Modifying the Markdown template steps. 2) Implementing the template rendering and validation test.",
        "updatedAt": "2026-01-09T05:17:50.369Z"
      },
      {
        "id": 9,
        "title": "Implement Tool Presence Validator",
        "description": "Develop a validator that checks for the availability of required binaries on the system PATH before execution.",
        "details": "Implement 'ToolPresenceValidator' within 'internal/validate'. For each step, check if the classified tool's binary exists using 'internal/shell'. If missing, mark the step as 'Skipped' with a detailed reason in the state and report.",
        "testStrategy": "Integration tests in a controlled environment where specific tools are added or removed from the PATH to verify skip logic.",
        "priority": "medium",
        "dependencies": [
          "2",
          "5",
          "6"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Develop core ToolPresenceValidator logic using internal/shell",
            "description": "Implement the ToolPresenceValidator in the 'internal/validate' package to check for binary availability via system PATH.",
            "dependencies": [],
            "details": "Create the ToolPresenceValidator struct and its validation method. Integrate with the 'internal/shell' package to perform 'which' or equivalent checks for the classified tool binary associated with each execution step.",
            "status": "done",
            "testStrategy": "Unit tests using a mocked shell interface to simulate scenarios where binaries are found or missing in the system PATH.",
            "parentId": "undefined",
            "updatedAt": "2026-01-09T05:18:54.788Z"
          },
          {
            "id": 2,
            "title": "Implement step skipping logic and state reporting for missing tools",
            "description": "Add functionality to mark execution steps as 'Skipped' in the RunState and provide detailed reasons in the RunReport when tools are missing.",
            "dependencies": [
              1
            ],
            "details": "When a tool is detected as missing by the ToolPresenceValidator, update the step status to 'Skipped'. Ensure the RunState transition is recorded and a message like 'Tool [name] not found on PATH' is added to the RunReport.",
            "status": "done",
            "testStrategy": "Integration tests verifying that 'ticket-action-pack.state.json' correctly records the 'Skipped' status and appropriate failure reasons for missing binaries.",
            "parentId": "undefined",
            "updatedAt": "2026-01-09T05:19:04.120Z"
          }
        ],
        "complexity": 3,
        "recommendedSubtasks": 2,
        "expansionPrompt": "Define subtasks for: 1) The validator logic that utilizes internal/shell. 2) The logic for marking steps as 'Skipped' based on tool absence.",
        "updatedAt": "2026-01-09T05:19:04.120Z"
      },
      {
        "id": 10,
        "title": "Implement Artifact Gate Validator",
        "description": "Create logic to verify that required artifacts were produced after specific steps are executed.",
        "details": "Implement 'ArtifactGateValidator' within 'internal/validate'. This validator checks the filesystem against the 'ArtifactContract' defined in 'internal/artifacts'. It should allow 'Skipped' status if the tool was missing, but 'Failed' if the tool ran but didn't produce the file.",
        "testStrategy": "Unit tests providing mock execution results and verifying the gate outcomes for both missing and present files.",
        "priority": "medium",
        "dependencies": [
          "4",
          "7"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement File System Validation Loop for Artifacts",
            "description": "Develop the core logic to iterate through defined artifact contracts and verify their existence on the local file system.",
            "dependencies": [],
            "details": "Create the ArtifactGateValidator in 'internal/validate'. This will use 'os.Stat' to iterate over paths defined in the ArtifactContract from 'internal/artifacts'. It must verify the physical presence of each expected file after a step execution.",
            "status": "done",
            "testStrategy": "Unit tests using temporary directories to verify that the validator correctly identifies existing and missing files on disk.",
            "parentId": "undefined",
            "updatedAt": "2026-01-09T05:19:31.878Z"
          },
          {
            "id": 2,
            "title": "Develop Result Mapping and Status Transition Logic",
            "description": "Implement the logic that determines whether a step should be marked as Failed or Skipped based on artifact presence and tool status.",
            "dependencies": [
              1
            ],
            "details": "Map the results of the file system check to state transitions. If the tool binary was missing (from ToolPresenceValidator), the gate allows a 'Skipped' status. If the tool was present but artifacts are missing, the gate must return a 'Failed' status. Integrate these results with the 'internal/state' report structure.",
            "status": "done",
            "testStrategy": "Table-driven unit tests providing mock execution scenarios: (Tool Missing + File Missing = Skipped), (Tool Present + File Present = Succeeded), (Tool Present + File Missing = Failed).",
            "parentId": "undefined",
            "updatedAt": "2026-01-09T05:19:45.768Z"
          }
        ],
        "complexity": 3,
        "recommendedSubtasks": 2,
        "expansionPrompt": "Divide into: 1) The file system validation loop. 2) Result mapping between artifact presence and step success/failure.",
        "updatedAt": "2026-01-09T05:19:45.768Z"
      },
      {
        "id": 11,
        "title": "Extend Unified Validation Pipeline",
        "description": "Integrate new tool-presence and artifact-gate checks into the existing validation workflow, ensuring non-oracle tools are no longer silently excluded.",
        "details": "In 'internal/validate', compose 'OracleDryRunValidator' (legacy), 'ToolPresenceValidator', and 'ArtifactGateValidator'. Ensure that 'oraclepack validate' command includes all 20 steps in its output, identifying the tool kind for each. Ensure non-oracle steps do not trigger oracle-specific validation logic.",
        "testStrategy": "Integration tests running validation on a 'mixed' pack (oracle + codex). Assert that both types of steps are present in the validation report.",
        "priority": "high",
        "dependencies": [
          "6",
          "9",
          "10"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Composite Validator Pipeline",
            "description": "Create a unified validation coordinator in 'internal/validate' to manage the execution of multiple validators.",
            "dependencies": [],
            "details": "Develop a 'CompositeValidator' struct that iterates through a list of 'StepValidator' interfaces. It must process each of the 20 steps provided by the parser, sequentially applying ToolPresenceValidator, OracleDryRunValidator, and ArtifactGateValidator. The coordinator should aggregate individual check results into a single comprehensive state for each step.",
            "status": "done",
            "testStrategy": "Unit tests using mock validators to ensure the 'CompositeValidator' invokes all registered checks for every step in an Action Pack.",
            "parentId": "undefined",
            "updatedAt": "2026-01-09T05:20:07.884Z"
          },
          {
            "id": 2,
            "title": "Refactor Validation Report and CLI Formatting",
            "description": "Update the validation output schema and CLI rendering to include tool-specific metadata and all 20 contract steps.",
            "dependencies": [
              1
            ],
            "details": "Modify the 'ValidationReport' model to include a 'ToolKind' field for each step. Update the 'oraclepack validate' command logic to ensure it prints all 20 steps defined in the contract, even if they are skipped or represent non-oracle tools. The output must clearly differentiate between legacy oracle tools and standard shell/codex tools.",
            "status": "done",
            "testStrategy": "CLI integration tests using a 'mixed' pack fixture to verify that the terminal output lists 20 entries with correct 'oracle' or 'codex' labels.",
            "parentId": "undefined",
            "updatedAt": "2026-01-09T05:26:18.943Z"
          },
          {
            "id": 3,
            "title": "Implement Tool-Aware Validation Logic",
            "description": "Add conditional logic to the validation pipeline to bypass legacy oracle checks for non-oracle steps.",
            "dependencies": [
              1
            ],
            "details": "Update the validation loop to check the 'Step' metadata for the tool type. If a step is identified as a non-oracle tool, the 'OracleDryRunValidator' should be skipped to avoid triggering legacy oracle-specific validation failures. Ensure that 'ToolPresence' and 'ArtifactGate' checks still run for these steps where applicable.",
            "status": "done",
            "testStrategy": "Integration tests running validation on a pack with non-oracle shell steps, asserting that no 'OracleDryRun' errors are raised for those specific indices.",
            "parentId": "undefined",
            "updatedAt": "2026-01-09T05:20:23.202Z"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Detail subtasks for: 1) Composing multiple validators into a single pipeline. 2) Updating the validation report format. 3) Ensuring non-oracle steps bypass legacy checks.",
        "updatedAt": "2026-01-09T05:26:18.943Z"
      },
      {
        "id": 12,
        "title": "Implement Headless Execution Engine",
        "description": "Develop the core execution logic that iterates through steps, handles skips, and manages state updates in non-interactive mode.",
        "details": "Create the main runner logic in 'internal/shell/engine.go'. It should: 1. Load Pack. 2. Initialize State. 3. For each step: Classify tool, Check Presence, Execute (or Skip), Record Results, Write State. Ensure execution uses 'bash -lc'. Implement timeouts for shell commands.",
        "testStrategy": "End-to-end integration test using a mock 20-step pack. Verify that 'Succeeded' and 'Skipped' statuses are correctly recorded in 'ticket-action-pack.state.json'.",
        "priority": "high",
        "dependencies": [
          "2",
          "4",
          "6"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Core Sequential Step Runner",
            "description": "Build the main iterative loop that processes the list of tasks from the action pack in internal/shell/engine.go.",
            "dependencies": [],
            "details": "Create the main Run function. It must load the Pack, initialize the RunState, and iterate through up to 20 steps, calling the dispatcher to classify tools for each command.",
            "status": "done",
            "testStrategy": "Unit test with a mock action pack containing a list of strings to ensure the loop visits every entry in the correct sequence.",
            "parentId": "undefined",
            "updatedAt": "2026-01-09T05:23:22.737Z"
          },
          {
            "id": 2,
            "title": "Implement Execution Timeouts and Signal Handling",
            "description": "Integrate context-based timeouts and OS signal listeners to ensure commands do not run indefinitely and can be interrupted.",
            "dependencies": [
              1
            ],
            "details": "Wrap command execution in 'bash -lc' using os/exec. Implement context.WithTimeout for each step and trap SIGINT/SIGTERM to ensure the engine shuts down gracefully while saving the current state.",
            "status": "done",
            "testStrategy": "Integration test using a 'sleep' command that exceeds a configured timeout to verify the engine kills the process and marks it as Failed.",
            "parentId": "undefined",
            "updatedAt": "2026-01-09T05:23:34.615Z"
          },
          {
            "id": 3,
            "title": "Integrate Atomic State and Report Updates",
            "description": "Connect the execution loop to the state persistence layer to track run progress in real-time.",
            "dependencies": [
              1
            ],
            "details": "Invoke the 'internal/state' package's WriteState method before and after every step execution. Ensure status transitions to 'Running', then to 'Succeeded', 'Failed', or 'Skipped' are recorded in the JSON state file.",
            "status": "done",
            "testStrategy": "Verify the 'ticket-action-pack.state.json' file content after each step in a multi-step run to ensure the 'current_step' index and status match reality.",
            "parentId": "undefined",
            "updatedAt": "2026-01-09T05:23:48.652Z"
          },
          {
            "id": 4,
            "title": "Implement Step Skip and Error Propagation Logic",
            "description": "Handle tool presence checks and execution errors to determine if a step should be skipped or if the engine should halt.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Incorporate the ToolPresenceValidator. If a tool is missing, mark the step as 'Skipped'. If a shell command returns a non-zero exit code, mark the step as 'Failed' and terminate the execution loop immediately.",
            "status": "done",
            "testStrategy": "Integration test using a pack with one missing tool and one failing command to assert that skip reasons and failure logs are correctly captured.",
            "parentId": "undefined",
            "updatedAt": "2026-01-09T05:24:02.507Z"
          }
        ],
        "complexity": 7,
        "recommendedSubtasks": 4,
        "expansionPrompt": "Break down into: 1) The core execution loop for 20 steps. 2) Timeout and signal handling logic. 3) State persistence integration within the loop. 4) Error handling and skip logic.",
        "updatedAt": "2026-01-09T05:24:02.507Z"
      },
      {
        "id": 13,
        "title": "Wire CLI Commands and Journey States",
        "description": "Update the 'cmd/oraclepack' entry point to expose the new 'validate' and 'run' capabilities with '--no-tui' support.",
        "details": "Update 'cmd/oraclepack/main.go'. Add 'run --no-tui' flag. Implement journey state rendering (Selected -> Ready -> Running -> Completed). Ensure exit codes are appropriate for CI (0 for success/skip, non-zero for failure). Hook up state/report writing to the CLI execution loop.",
        "testStrategy": "CLI integration tests using a test harness to invoke the binary and check stdout/stderr and exit codes. Verify the creation of '.state.json' and '.report.json' in the current directory.",
        "priority": "high",
        "dependencies": [
          "11",
          "12"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Configure Cobra Commands and --no-tui Flag",
            "description": "Register the 'validate' and 'run' subcommands in the CLI entry point and implement the '--no-tui' flag logic.",
            "dependencies": [],
            "details": "Update 'cmd/oraclepack/main.go' to include 'validate' and 'run' commands using the Cobra library. Add a persistent or local boolean flag '--no-tui' to the 'run' command to bypass the interactive Bubble Tea interface.",
            "status": "done",
            "testStrategy": "Execute the binary with '--help' to confirm new commands/flags exist. Run 'run --no-tui' to ensure it bypasses TUI initialization code.",
            "parentId": "undefined",
            "updatedAt": "2026-01-09T05:26:47.746Z"
          },
          {
            "id": 2,
            "title": "Implement Headless Journey State Rendering",
            "description": "Create a terminal output formatter that prints execution progress (Selected, Ready, Running, Completed) to stdout when TUI is disabled.",
            "dependencies": [
              1
            ],
            "details": "Develop a lightweight logging mechanism that monitors the 'RunState'. As the execution loop progresses through steps, print structured text updates (e.g., '[Running] Step 5: Validate Tooling') to provide real-time feedback without full-screen terminal manipulation.",
            "status": "done",
            "testStrategy": "Integration tests capturing stdout during a mock run to verify that each state transition is printed in the correct sequence.",
            "parentId": "undefined",
            "updatedAt": "2026-01-09T05:27:02.318Z"
          },
          {
            "id": 3,
            "title": "Wire Exit Codes and Persistence Hookup",
            "description": "Ensure the CLI execution loop handles system exit codes correctly and triggers state/report file writing before termination.",
            "dependencies": [
              2
            ],
            "details": "Integrate the 'internal/state' package into the 'cmd/oraclepack' execution loop. Ensure that 'os.Exit(0)' is called on success/skip and 'os.Exit(1)' (or other non-zero codes) on failure. Guarantee that 'ticket-action-pack.state.json' and 'ticket-action-pack.report.json' are written even on early failure.",
            "status": "done",
            "testStrategy": "Automated CLI harness tests that invoke the binary and assert the exit code value ($?) and the existence/content of the generated JSON files.",
            "parentId": "undefined",
            "updatedAt": "2026-01-09T05:27:16.615Z"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Split into: 1) Cobra command and flag setup for '--no-tui'. 2) Implementation of the journey state UI (Selected, Ready, etc.). 3) CI-compliant exit code management.",
        "updatedAt": "2026-01-09T05:27:16.615Z"
      },
      {
        "id": 14,
        "title": "Publish Documentation for Execution Semantics",
        "description": "Create user-facing documentation detailing how Action Packs are executed and how to ensure tool compatibility.",
        "details": "Update README or 'docs/' folder. Explicitly document 'bash -lc' behavior. List supported command prefixes (oracle, tm, task-master, codex, gemini). Provide guidance on 'codex exec' usage for automation and explain how the artifact gate validation prevents broken rollouts.",
        "testStrategy": "Manual review of documentation for clarity and accuracy against the implemented code. Verify 'help' output in CLI matches docs.",
        "priority": "low",
        "dependencies": [
          "13"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Author README and Execution Semantics Documentation",
            "description": "Develop comprehensive user-facing documentation in the README or docs/ directory that details the 'bash -lc' execution environment, supported command prefixes, and artifact gate logic.",
            "dependencies": [],
            "details": "Explain the necessity of 'bash -lc' for environment variable loading. Document the list of supported prefixes: oracle, tm, task-master, codex, and gemini. Provide examples of 'codex exec' for automation and describe how artifact gates ensure rollout integrity.",
            "status": "done",
            "testStrategy": "Manual review for clarity and technical accuracy. Verify that the documented prefixes and execution flags align with the 'internal/shell' and 'internal/validate' implementations.",
            "parentId": "undefined",
            "updatedAt": "2026-01-09T05:28:01.687Z"
          }
        ],
        "complexity": 2,
        "recommendedSubtasks": 1,
        "expansionPrompt": "Create a subtask for writing the README and internal docs covering tool prefixes and 'bash -lc' behavior.",
        "updatedAt": "2026-01-09T05:28:01.687Z"
      }
    ],
    "metadata": {
      "version": "1.0.0",
      "lastModified": "2026-01-09T05:28:01.692Z",
      "taskCount": 14,
      "completedCount": 14,
      "tags": [
        "feature-action-pack-artifact-gates-headless-steps"
      ],
      "created": "2026-01-09T22:12:45.274Z",
      "description": "Tasks for feature-action-pack-artifact-gates-headless-steps context",
      "updated": "2026-01-09T22:12:45.274Z"
    }
  },
  "chore-overview": {
    "tasks": [
      {
        "id": 1,
        "title": "Establish Core Types and Domain Models",
        "description": "Define the foundational data structures for Packs, Steps, and Verification results in a central internal package.",
        "details": "Create `internal/types/pack.go` and `internal/types/verification.go`. Define `Pack` and `Step` structs. Introduce `OutputContract` enum (e.g., `AllSections`, `DirectAnswerOnly`, `ChunkedBySuffix`). Define `OutputFailure` and `SyntaxFinding` structs to represent validation errors across the system. This centralizes types to prevent circular dependencies in subsequent phases.",
        "testStrategy": "Unit tests verifying that the structs can be instantiated and serialized/deserialized correctly. Ensure zero-dependency status for the types package.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Core Pack and Step Structs",
            "description": "Establish the primary domain models for the application by creating the Pack and Step data structures.",
            "dependencies": [],
            "details": "Create the file internal/types/pack.go. Define the Pack and Step Go structs, ensuring they include fields for identification, titles, and step sequences. This file must remain free of external dependencies to prevent circular imports.",
            "status": "pending",
            "testStrategy": "Unit tests to verify struct instantiation and field accessibility.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Define OutputContract and Validation Structs",
            "description": "Implement the enum for output contracts and the structures used for reporting validation failures.",
            "dependencies": [
              1
            ],
            "details": "Create internal/types/verification.go. Define the OutputContract enum (e.g., AllSections, DirectAnswerOnly) and the OutputFailure and SyntaxFinding structs to standardize how validation errors are passed through the system.",
            "status": "pending",
            "testStrategy": "Verify enum values and ensure validation structs correctly capture error metadata.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement Serialization Tags and Tests",
            "description": "Add metadata tags to the core structs and implement unit tests to ensure correct serialization behavior.",
            "dependencies": [
              1,
              2
            ],
            "details": "Decorate all structs in internal/types with standard JSON and YAML tags. Implement table-driven unit tests that perform Round-Trip serialization (Marshal then Unmarshal) to ensure data integrity across formats.",
            "status": "pending",
            "testStrategy": "Table-driven unit tests using json.Marshal and yaml.Marshal for all core types.",
            "parentId": "undefined"
          }
        ],
        "complexity": 3,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down the implementation into: 1. Definition of core Pack/Step structs in internal/types/pack.go. 2. Definition of OutputContract enum and validation structs in internal/types/verification.go. 3. Implementation of basic JSON/YAML tags and serialization tests.",
        "updatedAt": "2026-01-09T23:08:18.473Z"
      },
      {
        "id": 2,
        "title": "Implement Configuration Defaults and Env-Var Constants",
        "description": "Define the environment variable mapping and default values for oraclepack's operation.",
        "details": "Create `internal/config/defaults.go`. Define constants for `ORACLEPACK_OUTPUT_VERIFY` (default: true) and `ORACLEPACK_OUTPUT_RETRIES` (default: 0). Use a standard approach like `os.LookupEnv` or prepare for `spf13/viper` integration. Ensure these defaults are accessible to both execution and verification commands.",
        "testStrategy": "Unit tests ensuring default values are returned when no environment variables are set and that names match the PRD specification.",
        "priority": "high",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Environment Variable Key Constants",
            "description": "Create the constant string definitions for environment variables in the configuration package.",
            "dependencies": [],
            "details": "In internal/config/defaults.go, define exported constants for all configuration keys, specifically ORACLEPACK_OUTPUT_VERIFY and ORACLEPACK_OUTPUT_RETRIES, to prevent magic strings throughout the codebase.",
            "status": "pending",
            "testStrategy": "Unit test to verify that the constant values match the naming conventions defined in the PRD.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Default Configuration Provider",
            "description": "Develop a function that returns a base configuration object with defaults and environment overrides.",
            "dependencies": [
              1
            ],
            "details": "Implement a 'GetDefaultConfig' function in internal/config/defaults.go. Use os.LookupEnv to populate a Config struct, applying defaults of 'true' for verification and '0' for retries if the environment variables are unset.",
            "status": "pending",
            "testStrategy": "Table-driven unit tests ensuring defaults are applied when environment variables are missing, and correctly overridden when they are present.",
            "parentId": "undefined"
          }
        ],
        "complexity": 2,
        "recommendedSubtasks": 2,
        "expansionPrompt": "Separate into: 1. Constant definitions for environment variable keys. 2. A default provider function that returns a base configuration object.",
        "updatedAt": "2026-01-09T23:08:18.483Z"
      },
      {
        "id": 3,
        "title": "Implement Contract-Aware Output Expectation Inference",
        "description": "Develop logic to detect when a step requires a full 4-token response versus a single 'Direct answer' token.",
        "details": "Create `internal/pack/output_expectations.go`. Implement `DetectOutputContract(step Step) OutputContract`. Use regex or string matching to find 'Answer format' AND 'Return only: Direct answer' (or common variants). If both are present, the contract is `DirectAnswerOnly`. Otherwise, if 'Answer format' is present, default to `AllSections`. Return an array of required tokens (e.g., [\"### Direct answer\"] or the full set).",
        "testStrategy": "Table-driven unit tests with various Step Markdown snippets (e.g., varying indentation, casing, and phrasing of 'Direct answer only') to verify correct contract detection.",
        "priority": "high",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Develop Regex for 'Answer format' Detection",
            "description": "Create robust regular expressions to identify and extract 'Answer format' sections within Markdown step descriptions.",
            "dependencies": [],
            "details": "Use the Go `regexp` package to handle variations in Markdown header levels (e.g., ## or ###) and case-insensitive matching for the 'Answer format' string to ensure reliable extraction of the instruction block.",
            "status": "pending",
            "testStrategy": "Unit tests with various header styles and whitespace configurations to verify successful block extraction.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Contract Variant Detection Logic",
            "description": "Develop the internal logic to distinguish between standard multi-section responses and direct-answer-only responses.",
            "dependencies": [
              1
            ],
            "details": "Implement the logic in `internal/pack/output_expectations.go` to scan extracted blocks for phrases like 'Return only: Direct answer' or 'Direct answer only'. Return a `DirectAnswerOnly` contract if found, otherwise default to `AllSections`.",
            "status": "pending",
            "testStrategy": "Tests using specific instruction strings to ensure 'Direct answer' variants are correctly classified into the appropriate contract type.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Create Token Array Mapping Logic",
            "description": "Map the identified output contracts to their corresponding required token arrays used for validation.",
            "dependencies": [
              2
            ],
            "details": "Define a mapping function that returns `[]string{\"### Direct answer\"}` for the `DirectAnswerOnly` contract and a full list (e.g., Research, Steps, Code, Direct Answer) for the `AllSections` contract.",
            "status": "pending",
            "testStrategy": "Verification of the returned string slice contents for each specific OutputContract enum value.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Build Comprehensive Table-Driven Test Suite",
            "description": "Develop a table-driven test suite in `internal/pack/output_expectations_test.go` to handle edge cases in Markdown parsing.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Create a test suite covering scenarios such as missing 'Answer format' sections, mixed casing, extra indentation, Windows vs Linux line endings, and malformed Markdown snippets to ensure the inference engine never panics.",
            "status": "pending",
            "testStrategy": "A suite of at least 15 distinct Markdown input cases compared against expected contract types and token lists.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 4,
        "expansionPrompt": "Divide into: 1. Regex development for identifying 'Answer format' blocks. 2. Logic for variant detection (e.g., 'Direct answer' vs 'Direct answer only'). 3. Mapping logic from contract to token arrays. 4. Table-driven test suite for edge-case Markdown snippets.",
        "updatedAt": "2026-01-09T23:08:18.490Z"
      },
      {
        "id": 4,
        "title": "Refactor Suffix-Based Chunked Output Mapping",
        "description": "Map specific file suffixes to literal token requirements, removing placeholder logic.",
        "details": "Update `internal/pack/output_expectations.go` to handle multiple `--write-output` paths. Map suffixes: `-direct-answer` -> '### Direct answer', `-risks-unknowns` -> '### Risks and unknowns', `-next-experiment` -> '### Next experiment', and `-missing-evidence` -> '### Missing evidence' (literal token). Ensure the logic prioritizes suffix matching for multiple files.",
        "testStrategy": "Unit tests passing arrays of file paths to the inference engine and asserting the resulting token map matches the suffix-to-literal-token rules exactly.",
        "priority": "medium",
        "dependencies": [
          "3"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Suffix-to-Token Mapping Table",
            "description": "Create a lookup structure to map specific filename suffixes to their required Markdown header tokens.",
            "dependencies": [],
            "details": "In `internal/pack/output_expectations.go`, implement a map or constant list linking suffixes like '-direct-answer' to '### Direct answer', '-risks-unknowns' to '### Risks and unknowns', '-next-experiment' to '### Next experiment', and '-missing-evidence' to '### Missing evidence'.",
            "status": "pending",
            "testStrategy": "Unit tests ensuring the mapping function returns the correct literal token string for each defined suffix.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Multi-Path Suffix Inference Logic",
            "description": "Extend the expectation logic to handle multiple file paths provided in the '--write-output' flags.",
            "dependencies": [
              1
            ],
            "details": "Modify the inference engine to iterate through all output file paths for a Step. For each path, check against the suffix map defined in Task 1. Ensure the system can handle an array of paths and return a mapping of filename to its specific token expectation.",
            "status": "pending",
            "testStrategy": "Unit tests passing arrays of varied file paths and asserting that the resulting expectation map assigns the correct token to each file.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Develop Suffix Conflict Resolution and Prioritization",
            "description": "Handle edge cases where multiple suffixes might conflict or where no suffix is found in a chunked output context.",
            "dependencies": [
              1,
              2
            ],
            "details": "Implement logic to prioritize the most specific suffix match if multiple exist. Define behavior for files without recognized suffixes when other files in the same step have them (e.g., falling back to Step-level contract or flagging as invalid).",
            "status": "pending",
            "testStrategy": "Table-driven unit tests covering edge cases like overlapping suffix strings and file paths that do not match any known suffix pattern.",
            "parentId": "undefined"
          }
        ],
        "complexity": 4,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Split into: 1. Mapping table definition for suffixes to literal tokens. 2. Logic to handle multiple output file paths in a single step. 3. Conflict resolution logic when multiple suffixes are present.",
        "updatedAt": "2026-01-09T23:08:18.496Z"
      },
      {
        "id": 5,
        "title": "Develop Config Precedence Resolver",
        "description": "Create a resolver that merges CLI flags, environment variables, and defaults.",
        "details": "Implement `internal/config/resolve.go`. The precedence logic must be: CLI Flag > Environment Variable > Default. This resolver will be used by `run.go` and the new `verify_outputs.go`. Ensure boolean flags handle 'false' correctly from environment variables (e.g., '0', 'false', 'off').",
        "testStrategy": "Integration tests mocking CLI flags and environment variables to ensure the resolved runtime config follows the defined hierarchy.",
        "priority": "medium",
        "dependencies": [
          "2"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Configuration Merging Logic",
            "description": "Develop the core logic in internal/config/resolve.go to merge configuration settings from multiple sources.",
            "dependencies": [],
            "details": "Create a resolver function that applies overrides in the specific order: CLI Flag > Environment Variable > Default. The implementation must ensure that a value provided at a higher precedence level completely overrides values at lower levels.",
            "status": "pending",
            "testStrategy": "Unit tests with mock input structures representing flags, environment variables, and defaults to verify the resulting merged object.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Develop Boolean String Parsing Utility",
            "description": "Create a robust parser to convert various string representations of booleans into actual boolean types.",
            "dependencies": [],
            "details": "Implement a helper function to correctly interpret environment variables like '1'/'0', 'true'/'false', and 'on'/'off'. This is critical for ensuring ORACLEPACK_OUTPUT_VERIFY and similar flags behave as expected when set via shell environments.",
            "status": "pending",
            "testStrategy": "Table-driven unit tests covering all truthy and falsy variations, including case-insensitivity and error handling for invalid strings.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Integration Testing for Config Precedence",
            "description": "Validate the full end-to-end precedence logic (Flag > Env > Default) through integration tests.",
            "dependencies": [
              1,
              2
            ],
            "details": "Set up integration tests that simulate a runtime environment by mocking OS environment variables and CLI argument structures. Assert that the resolver returns the correct configuration state when values conflict across sources.",
            "status": "pending",
            "testStrategy": "Integration tests using a controlled environment to verify that CLI flags successfully override environment variables and defaults.",
            "parentId": "undefined"
          }
        ],
        "complexity": 4,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Decompose into: 1. Logic for merging nested configuration objects. 2. Boolean string parser (handling 1/0, true/false, on/off). 3. Integration tests for the resolution hierarchy (Flag > Env > Default).",
        "updatedAt": "2026-01-09T23:08:18.502Z"
      },
      {
        "id": 6,
        "title": "Implement 'verify-outputs' CLI Command",
        "description": "Create a new command to validate existing output files without executing bash commands.",
        "details": "Create `cmd/oraclepack/verify_outputs.go`. Logic: 1. Parse Pack. 2. Resolve config (verify enabled?). 3. For each Step, infer expectations. 4. Check if files exist on disk. 5. Scan files for tokens using `internal/pack/output_validator.go`. 6. Exit non-zero and print a report using `internal/pack/verify_report.go` if any tokens are missing.",
        "testStrategy": "Integration test using a sample Pack and pre-generated output files (some valid, some invalid). Assert the command returns non-zero for missing tokens and zero for success.",
        "priority": "high",
        "dependencies": [
          "4",
          "5"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement CLI command scaffolding and flag parsing",
            "description": "Create the entry point for the 'verify-outputs' command and define necessary CLI flags.",
            "dependencies": [],
            "details": "Create `cmd/oraclepack/verify_outputs.go` using spf13/cobra. Implement flags for the Pack file path and the target output directory. Ensure the command is correctly registered in the main CLI entry point.",
            "status": "pending",
            "testStrategy": "Manual verification of command availability via 'oraclepack verify-outputs --help' and unit tests for flag binding.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Develop file system scanning logic for step output discovery",
            "description": "Implement logic to iterate through Pack steps, identify expected output file paths, and check their existence on disk.",
            "dependencies": [
              1
            ],
            "details": "Integrate with the Pack parser to iterate through defined steps. For each step, determine the expected file path in the output directory. Implement logic to check if these files exist and record missing files as initial validation failures.",
            "status": "pending",
            "testStrategy": "Unit tests using temporary directories and mock Pack files to verify correct file path resolution and existence checks.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement token-based content validation logic",
            "description": "Create the core validation engine to scan file contents for required tokens based on step contracts.",
            "dependencies": [
              2
            ],
            "details": "Develop `internal/pack/output_validator.go`. This logic must take an OutputContract (from Task 3) and a file handle, then scan for required headers/tokens like '### Direct answer'. It should return a list of missing tokens for each file.",
            "status": "pending",
            "testStrategy": "Table-driven unit tests with various string contents and expected OutputContract requirements to ensure precise token detection.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement detailed error reporting and report generator",
            "description": "Develop the reporting module to aggregate validation failures and format them for the console.",
            "dependencies": [
              3
            ],
            "details": "Create `internal/pack/verify_report.go`. Implement logic to aggregate 'OutputFailure' objects into a structured report. The report should highlight exactly which steps failed and which tokens were missing. Ensure the CLI exits with status code 1 if any failures are detected.",
            "status": "pending",
            "testStrategy": "Visual verification of the report output format and automated tests checking for non-zero exit codes on validation failure.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Create integration tests for 'verify-outputs' command",
            "description": "Establish end-to-end tests that simulate a full verification run against mock data.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Set up a test suite that uses a sample Pack markdown and a directory of pre-generated output files. Include scenarios for perfect compliance, missing files, and files missing specific required tokens. Assert that the CLI output and exit codes match expectations.",
            "status": "pending",
            "testStrategy": "Integration tests using 'os/exec' or internal CLI invocation to verify the complete command workflow.",
            "parentId": "undefined"
          }
        ],
        "complexity": 7,
        "recommendedSubtasks": 5,
        "expansionPrompt": "Break down into: 1. CLI command scaffolding and flag parsing. 2. File system scanning logic for step output paths. 3. Token-based content validation. 4. Detailed error reporting/TUI integration. 5. Integration tests with mock output files.",
        "updatedAt": "2026-01-09T23:08:18.507Z"
      },
      {
        "id": 7,
        "title": "Implement Bash Syntax and Orphaned-Flag Validator",
        "description": "Detect unsafe structural issues in the generated bash blocks of a pack.",
        "details": "Create `internal/pack/bash_syntax_validator.go`. Use regex to detect lines containing ONLY flags like `-p` or `--prompt` without a preceding command or a line continuation (`\\`). Also implement `internal/pack/bash_tooling_checks.go` to wrap `bash -n`. If `bash` is not found in $PATH, log a warning and skip the check instead of failing.",
        "testStrategy": "Unit tests with bash strings containing orphaned flags and malformed syntax. Ensure the validator returns the correct line numbers and clear error messages.",
        "priority": "medium",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Regex-Based Orphaned Flag Detection",
            "description": "Develop the core logic to identify lines in bash blocks that contain isolated flags without valid preceding commands.",
            "dependencies": [],
            "details": "Create 'internal/pack/bash_syntax_validator.go'. Implement a regex such as '^\\s*(-[a-zA-Z0-9]+|--[a-z-]+)\\s*$' to detect lines containing ONLY flags. Ensure it ignores lines ending in a backslash continuation or lines where flags are preceded by non-whitespace characters.",
            "status": "pending",
            "testStrategy": "Unit tests using strings with valid command-flag pairs vs strings with isolated flags like ' -p' on a new line.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Develop Bash Syntax Execution Wrapper",
            "description": "Create a service to wrap the 'bash -n' command for external syntax validation with environmental safety checks.",
            "dependencies": [],
            "details": "Implement 'internal/pack/bash_tooling_checks.go'. Use 'os/exec' to run 'bash -n'. Add logic to check 'os.LookupEnv' or 'exec.LookPath' for bash; if not found, log a warning using the internal logger and skip the check to maintain portability.",
            "status": "pending",
            "testStrategy": "Execution tests that verify the validator gracefully handles environments where the bash binary is missing.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Create Validation Result Aggregator",
            "description": "Build a component to consolidate errors from both regex checks and external bash output into a unified report.",
            "dependencies": [
              1,
              2
            ],
            "details": "Implement a 'Validator' struct that calls both the regex detector and the bash -n wrapper. Implement parsing logic for bash stderr output (e.g., 'line 5: ...') to extract line numbers and map them to a standardized error structure alongside regex-detected orphaned flags.",
            "status": "pending",
            "testStrategy": "Integration testing of the aggregator to ensure it returns a combined slice of errors with correct line numbers.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement Comprehensive Validation Test Suite",
            "description": "Write a suite of unit and integration tests using broken and malformed bash scripts to ensure detection accuracy.",
            "dependencies": [
              3
            ],
            "details": "Create 'internal/pack/bash_syntax_validator_test.go'. Test cases must include: unclosed quotes, missing 'done' keywords (syntax errors), and isolated flags like '--prompt' followed by empty lines. Assert that the returned error messages point to the exact expected line numbers.",
            "status": "pending",
            "testStrategy": "Table-driven unit tests comparing actual error objects against expected line numbers and error types.",
            "parentId": "undefined"
          }
        ],
        "complexity": 6,
        "recommendedSubtasks": 4,
        "expansionPrompt": "Segment into: 1. Regex logic for structural 'orphaned flag' detection. 2. Exec wrapper for 'bash -n' with environment PATH checks. 3. Result aggregator that maps errors back to specific line numbers. 4. Unit tests with intentionally broken shell scripts.",
        "updatedAt": "2026-01-09T23:08:18.512Z"
      },
      {
        "id": 8,
        "title": "Integrate Syntax Checks into 'validate' and 'run' Commands",
        "description": "Ensure bash safety checks run automatically before execution or during manual validation.",
        "details": "Modify `cmd/oraclepack/validate.go` to include the new structural and `bash -n` checks. Update `cmd/oraclepack/run.go` to optionally (or by default) run these checks before starting step execution to prevent 'command not found' errors mid-run.",
        "testStrategy": "End-to-end test running `oraclepack validate` on a pack with an orphaned `-p` line. Confirm it fails with a non-zero exit code.",
        "priority": "medium",
        "dependencies": [
          "7"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Integrate Syntax Validator into 'validate' Command",
            "description": "Incorporate structural and bash syntax checks into the existing manual validation command flow.",
            "dependencies": [
              7
            ],
            "details": "Modify 'cmd/oraclepack/validate.go' to import and invoke the validators defined in Task 7. Update the validation loop to aggregate syntax errors alongside schema errors and ensure the command returns a non-zero exit code if fatal syntax errors are detected.",
            "status": "pending",
            "testStrategy": "Execute 'oraclepack validate' on a pack file containing intentional bash syntax errors (e.g., unclosed quotes) and verify the command reports the error and exits with code 1.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Pre-run Syntax Guards for 'run' Command",
            "description": "Add logic to the 'run' command to verify step scripts before they are executed to prevent mid-run failures.",
            "dependencies": [
              7
            ],
            "details": "Update 'cmd/oraclepack/run.go' to trigger the bash syntax validator before initiating the step execution loop. If syntax errors are detected, the system should halt execution or prompt the user for confirmation, depending on the configuration for bash safety.",
            "status": "pending",
            "testStrategy": "Run 'oraclepack run' on a pack with an orphaned '-p' flag. Confirm that the execution is blocked before any shell commands are executed or external API calls are made.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Standardize Syntax Error Reporting in CLI/TUI",
            "description": "Enhance the user interface to clearly display syntax warnings and line-specific error messages to the user.",
            "dependencies": [
              1
            ],
            "details": "Refactor error reporting within the TUI components to handle multi-line syntax error details. Implement color-coding (e.g., red for fatal structural errors, yellow for 'bash -n' warnings) to provide clear developer feedback during the validation and execution cycles.",
            "status": "pending",
            "testStrategy": "Visual inspection of CLI output when running validation on a malformed pack; verify that line numbers and error descriptions are correctly aligned and color-coded.",
            "parentId": "undefined"
          }
        ],
        "complexity": 4,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Divide into: 1. Hooking the validator into the 'validate' command pipeline. 2. Pre-execution check logic for the 'run' command. 3. User feedback/warning formatting for the TUI.",
        "updatedAt": "2026-01-09T23:08:18.517Z"
      },
      {
        "id": 9,
        "title": "Update Pack Authoring Documentation and Templates",
        "description": "Revise documentation to align with the new verification contracts and browser-mode mitigations.",
        "details": "Update the 'Gold Stage' pack templates. Document the literal token requirements and the suffix scheme. Add a section on 'Browser Mode Reliability' recommending adjustments to `--browser-timeout` and `--browser-input-timeout` (e.g., using `30s`, `1m`). Mention the `MSYS_NO_PATHCONV=1` caveat for Windows/Git Bash environments.",
        "testStrategy": "Manual review of documentation for accuracy. Verify that example packs provided in docs pass the new `verify-outputs` command.",
        "priority": "low",
        "dependencies": [
          "6"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Update Example Markdown Templates in examples/ Directory",
            "description": "Revise the 'Gold Stage' and other pack templates in the examples folder to reflect new suffix schemes and literal token requirements.",
            "dependencies": [],
            "details": "Modify the example files to include the correct suffixes (e.g., -direct-answer) and ensure the Markdown content contains the required literal tokens like '### Direct answer'. This ensures users have functional starting points that align with the new verification logic.",
            "status": "pending",
            "testStrategy": "Run oraclepack verify-outputs against the updated example files to ensure they conform to the new verification contracts.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Update Documentation for Browser-Mode and Verification Contracts",
            "description": "Update the project README and main documentation with details on verification contracts, browser timeouts, and Windows environment caveats.",
            "dependencies": [
              1
            ],
            "details": "Add sections explaining the literal token suffix scheme. Include a 'Browser Mode Reliability' guide recommending --browser-timeout and --browser-input-timeout values (30s, 1m). Document the MSYS_NO_PATHCONV=1 fix for Git Bash on Windows environments.",
            "status": "pending",
            "testStrategy": "Manual proofreading of Markdown files and verification that all technical instructions (timeouts and env vars) are accurate and easy to follow.",
            "parentId": "undefined"
          }
        ],
        "complexity": 2,
        "recommendedSubtasks": 2,
        "expansionPrompt": "Split into: 1. Updating Markdown templates in the examples/ directory. 2. Updating README and documentation sections for browser-mode and verification contracts.",
        "updatedAt": "2026-01-09T23:08:18.522Z"
      },
      {
        "id": 10,
        "title": "Final Integration and CI Verification",
        "description": "Ensure all components work together and provide guidance for CI implementation.",
        "details": "Perform a full pass over the TUI and CLI output formatting for consistency. Create a sample GitHub Action or GitLab CI snippet in the README demonstrating the use of `oraclepack validate` and `oraclepack verify-outputs` in a pipeline.",
        "testStrategy": "E2E smoke test: Create a pack, run it with fake outputs, then run `verify-outputs` to ensure a green pipeline.",
        "priority": "low",
        "dependencies": [
          "8",
          "9"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Develop E2E Smoke Test Script",
            "description": "Create a comprehensive shell script that simulates a full user lifecycle including pack creation, validation, and output verification.",
            "dependencies": [],
            "details": "The script should be located in `scripts/smoke_test.sh`. It will initialize a test environment, run `oraclepack validate` on a sample pack, simulate step execution to generate files, and finally execute `oraclepack verify-outputs` to confirm the pipeline's integrity.",
            "status": "pending",
            "testStrategy": "Run the script in a clean Docker container or temporary directory to ensure it exits with code 0 under success conditions and catches injected errors.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Create CI/CD Configuration Templates",
            "description": "Author reusable configuration snippets for GitHub Actions and GitLab CI to provide users with copy-pasteable pipeline integrations.",
            "dependencies": [
              1
            ],
            "details": "Update the README.md or a new `docs/ci.md` file with YAML examples. These examples must demonstrate how to install the binary and use the `validate` and `verify-outputs` commands as gates in a standard CI workflow.",
            "status": "pending",
            "testStrategy": "Validate the generated YAML snippets using online linter tools and perform a test run in a private GitHub repository.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Final CLI/TUI Output Polish Pass",
            "description": "Review and standardize all CLI outputs, error messages, and TUI components for consistent branding and user clarity.",
            "dependencies": [
              1,
              2
            ],
            "details": "Audit the `cmd/` and `internal/ui/` packages. Ensure consistent color schemes (e.g., using `lipgloss` or `fatih/color`), align table headers in the verification report, and ensure all error messages provide actionable feedback.",
            "status": "pending",
            "testStrategy": "Manual visual inspection of all CLI command outputs (validate, run, verify-outputs) across different terminal sizes and themes.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Decompose into: 1. End-to-end smoke test script encompassing the full lifecycle. 2. Development of reusable CI configuration snippets. 3. Final UX/CLI output polish pass.",
        "updatedAt": "2026-01-09T23:08:18.528Z"
      }
    ],
    "metadata": {
      "version": "1.0.0",
      "lastModified": "2026-01-09T23:08:18.529Z",
      "taskCount": 10,
      "completedCount": 10,
      "tags": [
        "chore-overview"
      ],
      "created": "2026-01-09T23:12:42.124Z",
      "description": "Tasks for chore-overview context",
      "updated": "2026-01-09T23:12:42.124Z"
    }
  }
}